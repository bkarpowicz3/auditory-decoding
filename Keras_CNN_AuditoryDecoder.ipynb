{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/miniconda2/envs/py34/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from lfp_extracters import *\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, SimpleRNN, Conv1D, MaxPooling1D, Flatten, Conv2D, MaxPooling2D, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and Load Data\n",
    "#### 80% train data, of which 20% is withheld for validation\n",
    "#### 20% test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inds(): \n",
    "    \n",
    "    numIter = 5\n",
    "    rs = ShuffleSplit(1200, n_iter=numIter, test_size=.2, random_state=0)\n",
    "    \n",
    "    A_files = glob.glob('data/data/Stimulus_A/LFP_15_300/*.csv')\n",
    "    B_files = glob.glob('data/data/Stimulus_B/LFP_15_300/*.csv')\n",
    "    files = np.array(A_files + B_files)\n",
    "    \n",
    "    train_inds = []\n",
    "    test_inds = []\n",
    "    for train, test in rs: \n",
    "        train_inds.append(train)\n",
    "        test_inds.append(test)\n",
    "        \n",
    "    train2 = []\n",
    "    val_inds = []\n",
    "    for t in train_inds: \n",
    "        s = ShuffleSplit(len(t), n_iter = 1, test_size = .2, random_state = 0)\n",
    "        for train, val in s: \n",
    "            train2.append(train)\n",
    "            val_inds.append(val)\n",
    "        \n",
    "    train_inds = train2\n",
    "        \n",
    "    return numIter, train_inds, val_inds, test_inds, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_test_val(trial_num, train_inds, test_inds, val_inds, files):\n",
    "    train_files = files[train_inds[trial_num]]\n",
    "    test_files = files[test_inds[trial_num]]\n",
    "    val_files = files[val_inds[trial_num]]\n",
    "    return train_files, test_files, val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_test_df(test_files, with_time, bin_function, threshold, bin_width):\n",
    "    channel_data = ['Ch1', 'Ch2', 'Ch3', 'Ch4', 'Ch5', 'Ch6', 'Ch7', 'Ch8', 'Ch9', 'Ch10', \\\n",
    "                   'Ch11', 'Ch12', 'Ch13', 'Ch14', 'Ch15', 'Ch16']\n",
    "    l = []\n",
    "    y_test = []\n",
    "    bin_width = int(bin_width)\n",
    "    \n",
    "    for i in range(len(test_files)):\n",
    "        n1 = np.genfromtxt(test_files[i], delimiter = ',')\n",
    "        if not with_time: \n",
    "            n1 = n1[0:16]\n",
    "        n_new = bin_function(n1, threshold, bin_width)\n",
    "        l.append(n_new) \n",
    "        if 'A' in test_files[i]: \n",
    "            y_test.append([0,1])\n",
    "        else: \n",
    "            y_test.append([1,0])\n",
    "\n",
    "    return np.array(l), y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_train_df(train_files, with_time, bin_function, threshold, bin_width):\n",
    "    channel_data = ['Ch1', 'Ch2', 'Ch3', 'Ch4', 'Ch5', 'Ch6', 'Ch7', 'Ch8', 'Ch9', 'Ch10', \\\n",
    "                   'Ch11', 'Ch12', 'Ch13', 'Ch14', 'Ch15', 'Ch16']\n",
    "    \n",
    "    list_ = []\n",
    "    y_train = []\n",
    "    bin_width = int(bin_width)\n",
    "    \n",
    "    for i in range(len(train_files)):\n",
    "        n1 = np.genfromtxt(train_files[i], delimiter = ',')\n",
    "        if not with_time: \n",
    "            n1 = n1[0:16]\n",
    "        n_new = bin_function(n1, threshold, bin_width)\n",
    "        list_.append(n_new)\n",
    "        if 'A' in train_files[i]: \n",
    "            y_train.append([0,1])\n",
    "        else: \n",
    "            y_train.append([1,0])\n",
    "\n",
    "    return np.array(list_), y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_val_df(val_files, with_time, bin_function, threshold, bin_width):\n",
    "    channel_data = ['Ch1', 'Ch2', 'Ch3', 'Ch4', 'Ch5', 'Ch6', 'Ch7', 'Ch8', 'Ch9', 'Ch10', \\\n",
    "                   'Ch11', 'Ch12', 'Ch13', 'Ch14', 'Ch15', 'Ch16']\n",
    "    \n",
    "    list_ = []\n",
    "    y_val = []\n",
    "    bin_width = int(bin_width)\n",
    "    \n",
    "    for i in range(len(val_files)):\n",
    "        n1 = np.genfromtxt(val_files[i], delimiter = ',')\n",
    "        if not with_time: \n",
    "            n1 = n1[0:16]\n",
    "        n_new = bin_function(n1, threshold, bin_width)\n",
    "        list_.append(n_new)\n",
    "        if 'A' in val_files[i]: \n",
    "            y_val.append([0,1])\n",
    "        else: \n",
    "            y_val.append([1,0])\n",
    "\n",
    "    return np.array(list_), y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Crossing Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def threshold_crossing_rate(x, threshold, bin_width): \n",
    "    inds = range(0, 6001, int(bin_width))\n",
    "    hastime = len(x) == 17\n",
    "    newx = []\n",
    "    if hastime: \n",
    "        t = x[16]\n",
    "    for i in range(0, len(inds) - 1): \n",
    "        sub = x[0:16, inds[i]:inds[i+1]]\n",
    "        count_above = np.sum(abs(np.diff(sub > threshold)), axis = 1)/float(bin_width)\n",
    "        if hastime:\n",
    "            t_seg = np.reshape(t[inds[i]], (1,)) #uses time stamp at beginning of each bin\n",
    "            count_above = np.concatenate((count_above, t_seg))\n",
    "        newx.append(count_above)\n",
    "    return np.array(newx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Mean Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def root_mean_square(x, threshold, bin_width): \n",
    "    inds = range(0, 6001, bin_width)\n",
    "    hastime = len(x) == 17\n",
    "    newx = []\n",
    "    if hastime: \n",
    "        t = x[16]\n",
    "    for i in range(0, len(inds) - 1): \n",
    "        sub = x[0:16, inds[i]:inds[i+1]]\n",
    "        rms = np.sqrt(np.mean(sub**2, axis = 1)) # , np.reshape((16,))\n",
    "        \n",
    "        if hastime: \n",
    "            t_seg = np.reshape(t[inds[i]], (1,))\n",
    "            rms = np.concatenate((rms, t_seg))\n",
    "        newx.append(rms)\n",
    "    return np.array(newx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma Band Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def power_gamma_band(x, threshold, bin_width): \n",
    "    inds = range(0, 6001, bin_width)\n",
    "    hastime = len(x) == 17\n",
    "    newx = [] \n",
    "    #make bandpass filter for 30-100Hz \n",
    "    fs = 6000\n",
    "    nyq = 0.5 * fs\n",
    "    cutOff_L = 30\n",
    "    cutOff_H = 100\n",
    "    low = cutOff_L / nyq\n",
    "    high = cutOff_H / nyq\n",
    "    order = 2\n",
    "    b, a = signal.butter(order, [low, high], btype='band')\n",
    "    \n",
    "    if hastime: \n",
    "        t = x[16]\n",
    "    for i in range(0, len(inds) - 1): \n",
    "        sub = x[0:16, inds[i]:inds[i+1]]\n",
    "        sub_f = signal.lfilter(b, a, sub)\n",
    "        rms = np.sqrt(np.mean(sub**2, axis = 1))\n",
    "        if hastime: \n",
    "            t_seg = np.reshape(t[inds[i]], (1,))\n",
    "            rms = np.concatenate((rms, t_seg))\n",
    "        newx.append(rms)\n",
    "    return np.array(newx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Change \n",
    "#### Placeholder binning metric function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def no_change(x, threshold, bin_width): \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_cnn(with_time, bin_width):\n",
    "    # a linear stack of layers\n",
    "    model_cnn = Sequential()\n",
    "    \n",
    "    if with_time: \n",
    "        x = 17\n",
    "    else: \n",
    "        x = 16\n",
    "    \n",
    "#     model_cnn.add(Dropout(0.3, input_shape = (6000//bin_width, x)))\n",
    "    model_cnn.add(Conv1D(2, kernel_size = (10), input_shape = (6000//bin_width, x), activation = 'relu', kernel_initializer='he_normal'))\n",
    "#     model_cnn.add(Conv1D(2, kernel_size = (10), activation = 'relu', kernel_initializer='he_normal'))\n",
    "    model_cnn.add(Flatten())\n",
    "#     model_cnn.add(Dropout(0.2))\n",
    "    model_cnn.add(Dense(32, activation = 'relu'))\n",
    "#     model_cnn.add(Dropout(0.2))\n",
    "    model_cnn.add(Dense(2, activation = 'softmax'))\n",
    "    \n",
    "    return model_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit, Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makePredictions(model, train_df, train_l, test_df, test_l, val_df, val_l, with_time):\n",
    "    a = keras.optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=a,\n",
    "                  metrics=['accuracy'])\n",
    "    print('Fitting...')\n",
    "    History = model.fit(train_df, train_l, epochs = 200, validation_data = (val_df, val_l)) #batch_size = 1200*.8*.8\n",
    "    print('Predicting...')\n",
    "    pred = model.predict(test_df)\n",
    "    return pred, History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(): \n",
    "    \n",
    "    numIter, train_inds, val_inds, test_inds, files = get_inds()\n",
    "    \n",
    "    threshold = 0.010 \n",
    "    bin_width = 265 \n",
    "    bin_function = power_gamma_band\n",
    "    with_time = False\n",
    "\n",
    "    \n",
    "    actual = []\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(0, numIter): \n",
    "        print('Cross val iteration ' + str(i+1) +  ' of ' + str(numIter))\n",
    "        \n",
    "        model_cnn = make_cnn(with_time, bin_width)\n",
    "        model = model_cnn\n",
    "    \n",
    "        train_files, test_files, val_files = get_train_test_val(i, train_inds, test_inds, val_inds, files)\n",
    "        \n",
    "        test_df, y_test = make_test_df(test_files, with_time, bin_function, threshold, bin_width)\n",
    "        train_df, y_train = make_train_df(train_files, with_time, bin_function, threshold, bin_width)\n",
    "        val_df, y_val = make_val_df(val_files, with_time, bin_function, threshold, bin_width)\n",
    "        \n",
    "        if with_time: \n",
    "            all_times = np.concatenate((train_df[:,:,-1], test_df[:,:,-1], val_df[:,:,-1]))\n",
    "            overall_time_mean = all_times.mean()\n",
    "            overall_time_std = all_times.std() * 100\n",
    "\n",
    "            new_times_train = (train_df[:,:, -1]-overall_time_mean)/overall_time_std\n",
    "            new_times_test = (test_df[:,:,-1]-overall_time_mean)/overall_time_std\n",
    "            new_times_val = (val_df[:,:, -1]-overall_time_mean)/overall_time_std\n",
    "            train_df[:,:,-1] = new_times_train\n",
    "            test_df[:,:,-1] = new_times_test\n",
    "            val_df[:,:,-1] = new_times_val\n",
    "\n",
    "        actual.append(y_test)\n",
    "        \n",
    "        pred, History = makePredictions(model, train_df, np.array(y_train), test_df, \\\n",
    "                                        np.array(y_test), val_df, np.array(y_val), with_time)\n",
    "\n",
    "        preds.append(pred)\n",
    "        \n",
    "    return preds, actual, History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val iteration 1 of 5\n",
      "Fitting...\n",
      "Train on 768 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.6858 - acc: 0.6237 - val_loss: 0.6843 - val_acc: 0.5833\n",
      "Epoch 2/200\n",
      "768/768 [==============================] - 0s 66us/step - loss: 0.6708 - acc: 0.6354 - val_loss: 0.6785 - val_acc: 0.5833\n",
      "Epoch 3/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6604 - acc: 0.6354 - val_loss: 0.6810 - val_acc: 0.5833\n",
      "Epoch 4/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6559 - acc: 0.6354 - val_loss: 0.6828 - val_acc: 0.5833\n",
      "Epoch 5/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6594 - acc: 0.6354 - val_loss: 0.6816 - val_acc: 0.5833\n",
      "Epoch 6/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.6559 - acc: 0.6354 - val_loss: 0.6823 - val_acc: 0.5833\n",
      "Epoch 7/200\n",
      "768/768 [==============================] - 0s 80us/step - loss: 0.6592 - acc: 0.6354 - val_loss: 0.6798 - val_acc: 0.5833\n",
      "Epoch 8/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6556 - acc: 0.6354 - val_loss: 0.6814 - val_acc: 0.5833\n",
      "Epoch 9/200\n",
      "768/768 [==============================] - 0s 75us/step - loss: 0.6534 - acc: 0.6354 - val_loss: 0.6816 - val_acc: 0.5833\n",
      "Epoch 10/200\n",
      "768/768 [==============================] - 0s 80us/step - loss: 0.6545 - acc: 0.6354 - val_loss: 0.6829 - val_acc: 0.5833\n",
      "Epoch 11/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6507 - acc: 0.6354 - val_loss: 0.6808 - val_acc: 0.5833\n",
      "Epoch 12/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.6507 - acc: 0.6354 - val_loss: 0.6819 - val_acc: 0.5833\n",
      "Epoch 13/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6544 - acc: 0.6354 - val_loss: 0.6807 - val_acc: 0.5833\n",
      "Epoch 14/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6534 - acc: 0.6354 - val_loss: 0.6798 - val_acc: 0.5833\n",
      "Epoch 15/200\n",
      "768/768 [==============================] - 0s 79us/step - loss: 0.6519 - acc: 0.6354 - val_loss: 0.6798 - val_acc: 0.5833\n",
      "Epoch 16/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.6524 - acc: 0.6354 - val_loss: 0.6782 - val_acc: 0.5833\n",
      "Epoch 17/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6541 - acc: 0.6354 - val_loss: 0.6772 - val_acc: 0.5833\n",
      "Epoch 18/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6475 - acc: 0.6354 - val_loss: 0.6776 - val_acc: 0.5833\n",
      "Epoch 19/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.6498 - acc: 0.6354 - val_loss: 0.6786 - val_acc: 0.5833\n",
      "Epoch 20/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6476 - acc: 0.6354 - val_loss: 0.6761 - val_acc: 0.5833\n",
      "Epoch 21/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6474 - acc: 0.6354 - val_loss: 0.6747 - val_acc: 0.5833\n",
      "Epoch 22/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.6461 - acc: 0.6354 - val_loss: 0.6723 - val_acc: 0.5833\n",
      "Epoch 23/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6498 - acc: 0.6354 - val_loss: 0.6687 - val_acc: 0.5833\n",
      "Epoch 24/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6458 - acc: 0.6354 - val_loss: 0.6707 - val_acc: 0.5833\n",
      "Epoch 25/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6434 - acc: 0.6354 - val_loss: 0.6665 - val_acc: 0.5833\n",
      "Epoch 26/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.6385 - acc: 0.639 - 0s 83us/step - loss: 0.6405 - acc: 0.6354 - val_loss: 0.6653 - val_acc: 0.5833\n",
      "Epoch 27/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6383 - acc: 0.6354 - val_loss: 0.6632 - val_acc: 0.5833\n",
      "Epoch 28/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6362 - acc: 0.6354 - val_loss: 0.6592 - val_acc: 0.5833\n",
      "Epoch 29/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6326 - acc: 0.6354 - val_loss: 0.6537 - val_acc: 0.5833\n",
      "Epoch 30/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6274 - acc: 0.6354 - val_loss: 0.6471 - val_acc: 0.5833\n",
      "Epoch 31/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.6215 - acc: 0.6354 - val_loss: 0.6473 - val_acc: 0.5833\n",
      "Epoch 32/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.6208 - acc: 0.6354 - val_loss: 0.6399 - val_acc: 0.5833\n",
      "Epoch 33/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6122 - acc: 0.6354 - val_loss: 0.6286 - val_acc: 0.5833\n",
      "Epoch 34/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.6068 - acc: 0.6354 - val_loss: 0.6251 - val_acc: 0.5833\n",
      "Epoch 35/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.6047 - acc: 0.6354 - val_loss: 0.6341 - val_acc: 0.5833\n",
      "Epoch 36/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6019 - acc: 0.6354 - val_loss: 0.6274 - val_acc: 0.5833\n",
      "Epoch 37/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5905 - acc: 0.6354 - val_loss: 0.6055 - val_acc: 0.5833\n",
      "Epoch 38/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5862 - acc: 0.6354 - val_loss: 0.6009 - val_acc: 0.5833\n",
      "Epoch 39/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5798 - acc: 0.6354 - val_loss: 0.5929 - val_acc: 0.5833\n",
      "Epoch 40/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5757 - acc: 0.6354 - val_loss: 0.5921 - val_acc: 0.5833\n",
      "Epoch 41/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5668 - acc: 0.6354 - val_loss: 0.5869 - val_acc: 0.5833\n",
      "Epoch 42/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.5673 - acc: 0.6354 - val_loss: 0.5817 - val_acc: 0.5833\n",
      "Epoch 43/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5654 - acc: 0.6354 - val_loss: 0.5813 - val_acc: 0.5833\n",
      "Epoch 44/200\n",
      "768/768 [==============================] - 0s 80us/step - loss: 0.5642 - acc: 0.6354 - val_loss: 0.5733 - val_acc: 0.5833\n",
      "Epoch 45/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5608 - acc: 0.6354 - val_loss: 0.5758 - val_acc: 0.5833\n",
      "Epoch 46/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.5596 - acc: 0.6641 - val_loss: 0.5793 - val_acc: 0.5833\n",
      "Epoch 47/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5634 - acc: 0.6706 - val_loss: 0.5836 - val_acc: 0.5833\n",
      "Epoch 48/200\n",
      "768/768 [==============================] - 0s 167us/step - loss: 0.5560 - acc: 0.6966 - val_loss: 0.5649 - val_acc: 0.7188\n",
      "Epoch 49/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.5602 - acc: 0.6693 - val_loss: 0.5678 - val_acc: 0.6979\n",
      "Epoch 50/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.5557 - acc: 0.7161 - val_loss: 0.5614 - val_acc: 0.7188\n",
      "Epoch 51/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5479 - acc: 0.7148 - val_loss: 0.5633 - val_acc: 0.6562\n",
      "Epoch 52/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.5445 - acc: 0.7096 - val_loss: 0.5674 - val_acc: 0.6198\n",
      "Epoch 53/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5529 - acc: 0.7044 - val_loss: 0.5569 - val_acc: 0.7240\n",
      "Epoch 54/200\n",
      "768/768 [==============================] - 0s 80us/step - loss: 0.5430 - acc: 0.6979 - val_loss: 0.5572 - val_acc: 0.6875\n",
      "Epoch 55/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.5438 - acc: 0.7148 - val_loss: 0.5544 - val_acc: 0.7344\n",
      "Epoch 56/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5486 - acc: 0.7214 - val_loss: 0.5578 - val_acc: 0.7135\n",
      "Epoch 57/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.5397 - acc: 0.708 - 0s 85us/step - loss: 0.5435 - acc: 0.7057 - val_loss: 0.5548 - val_acc: 0.7031\n",
      "Epoch 58/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.5501 - acc: 0.7005 - val_loss: 0.5531 - val_acc: 0.7135\n",
      "Epoch 59/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5436 - acc: 0.7370 - val_loss: 0.5528 - val_acc: 0.7031\n",
      "Epoch 60/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.5439 - acc: 0.7214 - val_loss: 0.5499 - val_acc: 0.7188\n",
      "Epoch 61/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5399 - acc: 0.7292 - val_loss: 0.5563 - val_acc: 0.6927\n",
      "Epoch 62/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.5426 - acc: 0.7279 - val_loss: 0.5514 - val_acc: 0.7083\n",
      "Epoch 63/200\n",
      "768/768 [==============================] - 0s 75us/step - loss: 0.5393 - acc: 0.7279 - val_loss: 0.5464 - val_acc: 0.7292\n",
      "Epoch 64/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5405 - acc: 0.7383 - val_loss: 0.5523 - val_acc: 0.6771\n",
      "Epoch 65/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5416 - acc: 0.7331 - val_loss: 0.5464 - val_acc: 0.7135\n",
      "Epoch 66/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.5365 - acc: 0.7448 - val_loss: 0.5505 - val_acc: 0.6979\n",
      "Epoch 67/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5393 - acc: 0.7396 - val_loss: 0.5464 - val_acc: 0.7135\n",
      "Epoch 68/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.5444 - acc: 0.7214 - val_loss: 0.5553 - val_acc: 0.6927\n",
      "Epoch 69/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5315 - acc: 0.7318 - val_loss: 0.5475 - val_acc: 0.7188\n",
      "Epoch 70/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5353 - acc: 0.7318 - val_loss: 0.5410 - val_acc: 0.7344\n",
      "Epoch 71/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5270 - acc: 0.7331 - val_loss: 0.5410 - val_acc: 0.7292\n",
      "Epoch 72/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5343 - acc: 0.7214 - val_loss: 0.5403 - val_acc: 0.7240\n",
      "Epoch 73/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5334 - acc: 0.7370 - val_loss: 0.5396 - val_acc: 0.7344\n",
      "Epoch 74/200\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.5271 - acc: 0.7370 - val_loss: 0.5383 - val_acc: 0.7448\n",
      "Epoch 75/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5282 - acc: 0.7409 - val_loss: 0.5564 - val_acc: 0.6667\n",
      "Epoch 76/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5327 - acc: 0.7161 - val_loss: 0.5666 - val_acc: 0.6302\n",
      "Epoch 77/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5301 - acc: 0.7240 - val_loss: 0.5520 - val_acc: 0.6719\n",
      "Epoch 78/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5329 - acc: 0.7109 - val_loss: 0.5381 - val_acc: 0.7083\n",
      "Epoch 79/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5316 - acc: 0.7292 - val_loss: 0.5353 - val_acc: 0.7448\n",
      "Epoch 80/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5157 - acc: 0.7227 - val_loss: 0.5607 - val_acc: 0.6510\n",
      "Epoch 81/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.5323 - acc: 0.7214 - val_loss: 0.5349 - val_acc: 0.7240\n",
      "Epoch 82/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5273 - acc: 0.7526 - val_loss: 0.5371 - val_acc: 0.7448\n",
      "Epoch 83/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5177 - acc: 0.7331 - val_loss: 0.5327 - val_acc: 0.7448\n",
      "Epoch 84/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5201 - acc: 0.7474 - val_loss: 0.5337 - val_acc: 0.7448\n",
      "Epoch 85/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5284 - acc: 0.7409 - val_loss: 0.5365 - val_acc: 0.7448\n",
      "Epoch 86/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5276 - acc: 0.7461 - val_loss: 0.5310 - val_acc: 0.7448\n",
      "Epoch 87/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5244 - acc: 0.7435 - val_loss: 0.5321 - val_acc: 0.7135\n",
      "Epoch 88/200\n",
      "768/768 [==============================] - 0s 75us/step - loss: 0.5183 - acc: 0.7344 - val_loss: 0.5415 - val_acc: 0.7240\n",
      "Epoch 89/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.5233 - acc: 0.7422 - val_loss: 0.5304 - val_acc: 0.7604\n",
      "Epoch 90/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5165 - acc: 0.7448 - val_loss: 0.5306 - val_acc: 0.7552\n",
      "Epoch 91/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.5207 - acc: 0.7500 - val_loss: 0.5288 - val_acc: 0.7604\n",
      "Epoch 92/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.5173 - acc: 0.7409 - val_loss: 0.5284 - val_acc: 0.7604\n",
      "Epoch 93/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.5175 - acc: 0.7318 - val_loss: 0.5315 - val_acc: 0.7396\n",
      "Epoch 94/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.5271 - acc: 0.738 - 0s 90us/step - loss: 0.5199 - acc: 0.7396 - val_loss: 0.5271 - val_acc: 0.7552\n",
      "Epoch 95/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.5004 - acc: 0.7604 - val_loss: 0.5273 - val_acc: 0.7292\n",
      "Epoch 96/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5198 - acc: 0.7487 - val_loss: 0.5264 - val_acc: 0.7344\n",
      "Epoch 97/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5128 - acc: 0.7422 - val_loss: 0.5272 - val_acc: 0.7500\n",
      "Epoch 98/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5202 - acc: 0.7604 - val_loss: 0.5253 - val_acc: 0.7396\n",
      "Epoch 99/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5108 - acc: 0.7565 - val_loss: 0.5265 - val_acc: 0.7500\n",
      "Epoch 100/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.5215 - acc: 0.7500 - val_loss: 0.5372 - val_acc: 0.6875\n",
      "Epoch 101/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5191 - acc: 0.7370 - val_loss: 0.5250 - val_acc: 0.7240\n",
      "Epoch 102/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5181 - acc: 0.7344 - val_loss: 0.5264 - val_acc: 0.6823\n",
      "Epoch 103/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5166 - acc: 0.7435 - val_loss: 0.5306 - val_acc: 0.6927\n",
      "Epoch 104/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5100 - acc: 0.7526 - val_loss: 0.5267 - val_acc: 0.7552\n",
      "Epoch 105/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5236 - acc: 0.7578 - val_loss: 0.5240 - val_acc: 0.7031\n",
      "Epoch 106/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5057 - acc: 0.7630 - val_loss: 0.5223 - val_acc: 0.7656\n",
      "Epoch 107/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5079 - acc: 0.7708 - val_loss: 0.5349 - val_acc: 0.6927\n",
      "Epoch 108/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5118 - acc: 0.7448 - val_loss: 0.5267 - val_acc: 0.7552\n",
      "Epoch 109/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5103 - acc: 0.7474 - val_loss: 0.5210 - val_acc: 0.7656\n",
      "Epoch 110/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5127 - acc: 0.7591 - val_loss: 0.5269 - val_acc: 0.7552\n",
      "Epoch 111/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5149 - acc: 0.7474 - val_loss: 0.5200 - val_acc: 0.7656\n",
      "Epoch 112/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.5169 - acc: 0.7591 - val_loss: 0.5249 - val_acc: 0.6823\n",
      "Epoch 113/200\n",
      "768/768 [==============================] - 0s 80us/step - loss: 0.5048 - acc: 0.7539 - val_loss: 0.5293 - val_acc: 0.7604\n",
      "Epoch 114/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5219 - acc: 0.7266 - val_loss: 0.5350 - val_acc: 0.7448\n",
      "Epoch 115/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5162 - acc: 0.7487 - val_loss: 0.5279 - val_acc: 0.7708\n",
      "Epoch 116/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.5001 - acc: 0.7630 - val_loss: 0.5199 - val_acc: 0.7552\n",
      "Epoch 117/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4976 - acc: 0.7565 - val_loss: 0.5246 - val_acc: 0.7604\n",
      "Epoch 118/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5147 - acc: 0.7487 - val_loss: 0.5195 - val_acc: 0.7500\n",
      "Epoch 119/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5088 - acc: 0.7721 - val_loss: 0.5168 - val_acc: 0.7448\n",
      "Epoch 120/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 88us/step - loss: 0.5106 - acc: 0.7630 - val_loss: 0.5166 - val_acc: 0.7396\n",
      "Epoch 121/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.5045 - acc: 0.7643 - val_loss: 0.5192 - val_acc: 0.6979\n",
      "Epoch 122/200\n",
      "768/768 [==============================] - 0s 72us/step - loss: 0.5048 - acc: 0.7604 - val_loss: 0.5159 - val_acc: 0.7448\n",
      "Epoch 123/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5055 - acc: 0.7565 - val_loss: 0.5170 - val_acc: 0.7552\n",
      "Epoch 124/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5086 - acc: 0.7513 - val_loss: 0.5169 - val_acc: 0.7552\n",
      "Epoch 125/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.5050 - acc: 0.7500 - val_loss: 0.5151 - val_acc: 0.7344\n",
      "Epoch 126/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.5087 - acc: 0.7370 - val_loss: 0.5150 - val_acc: 0.7292\n",
      "Epoch 127/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5086 - acc: 0.7526 - val_loss: 0.5147 - val_acc: 0.7396\n",
      "Epoch 128/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4971 - acc: 0.7578 - val_loss: 0.5163 - val_acc: 0.7500\n",
      "Epoch 129/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.5062 - acc: 0.756 - 0s 83us/step - loss: 0.5036 - acc: 0.7591 - val_loss: 0.5153 - val_acc: 0.7500\n",
      "Epoch 130/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5076 - acc: 0.7591 - val_loss: 0.5161 - val_acc: 0.6979\n",
      "Epoch 131/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5037 - acc: 0.7474 - val_loss: 0.5198 - val_acc: 0.7083\n",
      "Epoch 132/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.5015 - acc: 0.7513 - val_loss: 0.5146 - val_acc: 0.7552\n",
      "Epoch 133/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5002 - acc: 0.7721 - val_loss: 0.5209 - val_acc: 0.7083\n",
      "Epoch 134/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4958 - acc: 0.7669 - val_loss: 0.5210 - val_acc: 0.7031\n",
      "Epoch 135/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5047 - acc: 0.7591 - val_loss: 0.5121 - val_acc: 0.7292\n",
      "Epoch 136/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5040 - acc: 0.7591 - val_loss: 0.5134 - val_acc: 0.7552\n",
      "Epoch 137/200\n",
      "768/768 [==============================] - 0s 80us/step - loss: 0.4897 - acc: 0.7630 - val_loss: 0.5133 - val_acc: 0.7552\n",
      "Epoch 138/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4993 - acc: 0.7656 - val_loss: 0.5114 - val_acc: 0.7344\n",
      "Epoch 139/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4978 - acc: 0.7591 - val_loss: 0.5125 - val_acc: 0.7083\n",
      "Epoch 140/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5035 - acc: 0.7500 - val_loss: 0.5245 - val_acc: 0.6927\n",
      "Epoch 141/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.4939 - acc: 0.7565 - val_loss: 0.5211 - val_acc: 0.6979\n",
      "Epoch 142/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4918 - acc: 0.7630 - val_loss: 0.5105 - val_acc: 0.7292\n",
      "Epoch 143/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.4924 - acc: 0.7630 - val_loss: 0.5093 - val_acc: 0.7396\n",
      "Epoch 144/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4892 - acc: 0.7669 - val_loss: 0.5112 - val_acc: 0.7135\n",
      "Epoch 145/200\n",
      "768/768 [==============================] - 0s 74us/step - loss: 0.5020 - acc: 0.7565 - val_loss: 0.5260 - val_acc: 0.6771\n",
      "Epoch 146/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.4961 - acc: 0.7630 - val_loss: 0.5086 - val_acc: 0.7396\n",
      "Epoch 147/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4985 - acc: 0.7630 - val_loss: 0.5093 - val_acc: 0.7240\n",
      "Epoch 148/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4887 - acc: 0.7643 - val_loss: 0.5082 - val_acc: 0.7448\n",
      "Epoch 149/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.4952 - acc: 0.7721 - val_loss: 0.5099 - val_acc: 0.7240\n",
      "Epoch 150/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.4996 - acc: 0.7734 - val_loss: 0.5074 - val_acc: 0.7396\n",
      "Epoch 151/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4931 - acc: 0.7747 - val_loss: 0.5242 - val_acc: 0.7604\n",
      "Epoch 152/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4857 - acc: 0.7786 - val_loss: 0.5072 - val_acc: 0.7396\n",
      "Epoch 153/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4911 - acc: 0.7708 - val_loss: 0.5068 - val_acc: 0.7344\n",
      "Epoch 154/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4913 - acc: 0.7708 - val_loss: 0.5077 - val_acc: 0.7396\n",
      "Epoch 155/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.4903 - acc: 0.7591 - val_loss: 0.5077 - val_acc: 0.7240\n",
      "Epoch 156/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4882 - acc: 0.7721 - val_loss: 0.5058 - val_acc: 0.7500\n",
      "Epoch 157/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4811 - acc: 0.7812 - val_loss: 0.5067 - val_acc: 0.7344\n",
      "Epoch 158/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4852 - acc: 0.7656 - val_loss: 0.5070 - val_acc: 0.7344\n",
      "Epoch 159/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4795 - acc: 0.7812 - val_loss: 0.5088 - val_acc: 0.7500\n",
      "Epoch 160/200\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.4880 - acc: 0.7760 - val_loss: 0.5135 - val_acc: 0.7708\n",
      "Epoch 161/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4903 - acc: 0.7734 - val_loss: 0.5082 - val_acc: 0.7500\n",
      "Epoch 162/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4890 - acc: 0.7682 - val_loss: 0.5049 - val_acc: 0.7344\n",
      "Epoch 163/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4851 - acc: 0.7669 - val_loss: 0.5097 - val_acc: 0.7604\n",
      "Epoch 164/200\n",
      "768/768 [==============================] - 0s 75us/step - loss: 0.4901 - acc: 0.7721 - val_loss: 0.5049 - val_acc: 0.7344\n",
      "Epoch 165/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.4862 - acc: 0.7604 - val_loss: 0.5036 - val_acc: 0.7604\n",
      "Epoch 166/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.4868 - acc: 0.7682 - val_loss: 0.5042 - val_acc: 0.7344\n",
      "Epoch 167/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4868 - acc: 0.7721 - val_loss: 0.5036 - val_acc: 0.7344\n",
      "Epoch 168/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4835 - acc: 0.7812 - val_loss: 0.5062 - val_acc: 0.7344\n",
      "Epoch 169/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4872 - acc: 0.7799 - val_loss: 0.5038 - val_acc: 0.7396\n",
      "Epoch 170/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4798 - acc: 0.7812 - val_loss: 0.5027 - val_acc: 0.7604\n",
      "Epoch 171/200\n",
      "768/768 [==============================] - 0s 75us/step - loss: 0.4814 - acc: 0.7656 - val_loss: 0.5023 - val_acc: 0.7604\n",
      "Epoch 172/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.4849 - acc: 0.7604 - val_loss: 0.5030 - val_acc: 0.7344\n",
      "Epoch 173/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4801 - acc: 0.7656 - val_loss: 0.5125 - val_acc: 0.7083\n",
      "Epoch 174/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4862 - acc: 0.7747 - val_loss: 0.5202 - val_acc: 0.6979\n",
      "Epoch 175/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4768 - acc: 0.7878 - val_loss: 0.5018 - val_acc: 0.7604\n",
      "Epoch 176/200\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.4786 - acc: 0.7617 - val_loss: 0.5097 - val_acc: 0.7240\n",
      "Epoch 177/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4870 - acc: 0.7526 - val_loss: 0.5077 - val_acc: 0.7552\n",
      "Epoch 178/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.4798 - acc: 0.7565 - val_loss: 0.5011 - val_acc: 0.7604\n",
      "Epoch 179/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4913 - acc: 0.7643 - val_loss: 0.5087 - val_acc: 0.7604\n",
      "Epoch 180/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 87us/step - loss: 0.4754 - acc: 0.7734 - val_loss: 0.5021 - val_acc: 0.7500\n",
      "Epoch 181/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4783 - acc: 0.7656 - val_loss: 0.5158 - val_acc: 0.7396\n",
      "Epoch 182/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.4824 - acc: 0.7604 - val_loss: 0.5072 - val_acc: 0.7552\n",
      "Epoch 183/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.4993 - acc: 0.7474 - val_loss: 0.5037 - val_acc: 0.7500\n",
      "Epoch 184/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4763 - acc: 0.7852 - val_loss: 0.5036 - val_acc: 0.7240\n",
      "Epoch 185/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4754 - acc: 0.7682 - val_loss: 0.5004 - val_acc: 0.7604\n",
      "Epoch 186/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.4741 - acc: 0.7747 - val_loss: 0.4997 - val_acc: 0.7656\n",
      "Epoch 187/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4789 - acc: 0.7734 - val_loss: 0.5028 - val_acc: 0.7240\n",
      "Epoch 188/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4767 - acc: 0.7773 - val_loss: 0.5071 - val_acc: 0.7552\n",
      "Epoch 189/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4768 - acc: 0.7839 - val_loss: 0.4992 - val_acc: 0.7604\n",
      "Epoch 190/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4794 - acc: 0.7773 - val_loss: 0.5009 - val_acc: 0.7500\n",
      "Epoch 191/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4715 - acc: 0.7656 - val_loss: 0.5044 - val_acc: 0.7188\n",
      "Epoch 192/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.4705 - acc: 0.7734 - val_loss: 0.5077 - val_acc: 0.7552\n",
      "Epoch 193/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.4760 - acc: 0.7669 - val_loss: 0.5060 - val_acc: 0.7552\n",
      "Epoch 194/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4857 - acc: 0.7721 - val_loss: 0.5020 - val_acc: 0.7552\n",
      "Epoch 195/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.4750 - acc: 0.7786 - val_loss: 0.4989 - val_acc: 0.7500\n",
      "Epoch 196/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4734 - acc: 0.7773 - val_loss: 0.4994 - val_acc: 0.7448\n",
      "Epoch 197/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4804 - acc: 0.7630 - val_loss: 0.4988 - val_acc: 0.7500\n",
      "Epoch 198/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4654 - acc: 0.7891 - val_loss: 0.5118 - val_acc: 0.7448\n",
      "Epoch 199/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4781 - acc: 0.7591 - val_loss: 0.4994 - val_acc: 0.7604\n",
      "Epoch 200/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4912 - acc: 0.7656 - val_loss: 0.5067 - val_acc: 0.7500\n",
      "Predicting...\n",
      "Cross val iteration 2 of 5\n",
      "Fitting...\n",
      "Train on 768 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "768/768 [==============================] - 0s 386us/step - loss: 0.6857 - acc: 0.6185 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 2/200\n",
      "768/768 [==============================] - 0s 73us/step - loss: 0.6725 - acc: 0.6354 - val_loss: 0.6798 - val_acc: 0.5833\n",
      "Epoch 3/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.6641 - acc: 0.6354 - val_loss: 0.6794 - val_acc: 0.5833\n",
      "Epoch 4/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.6584 - acc: 0.6354 - val_loss: 0.6814 - val_acc: 0.5833\n",
      "Epoch 5/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6566 - acc: 0.6354 - val_loss: 0.6834 - val_acc: 0.5833\n",
      "Epoch 6/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6560 - acc: 0.6354 - val_loss: 0.6847 - val_acc: 0.5833\n",
      "Epoch 7/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6540 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 8/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6561 - acc: 0.6354 - val_loss: 0.6851 - val_acc: 0.5833\n",
      "Epoch 9/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6549 - acc: 0.6354 - val_loss: 0.6851 - val_acc: 0.5833\n",
      "Epoch 10/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6603 - acc: 0.6354 - val_loss: 0.6833 - val_acc: 0.5833\n",
      "Epoch 11/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.6560 - acc: 0.6354 - val_loss: 0.6850 - val_acc: 0.5833\n",
      "Epoch 12/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6580 - acc: 0.6354 - val_loss: 0.6853 - val_acc: 0.5833\n",
      "Epoch 13/200\n",
      "768/768 [==============================] - 0s 183us/step - loss: 0.6567 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 14/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6569 - acc: 0.6354 - val_loss: 0.6846 - val_acc: 0.5833\n",
      "Epoch 15/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.6564 - acc: 0.6354 - val_loss: 0.6848 - val_acc: 0.5833\n",
      "Epoch 16/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.6555 - acc: 0.6354 - val_loss: 0.6854 - val_acc: 0.5833\n",
      "Epoch 17/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6545 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 18/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6559 - acc: 0.6354 - val_loss: 0.6852 - val_acc: 0.5833\n",
      "Epoch 19/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6558 - acc: 0.6354 - val_loss: 0.6851 - val_acc: 0.5833\n",
      "Epoch 20/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6562 - acc: 0.6354 - val_loss: 0.6848 - val_acc: 0.5833\n",
      "Epoch 21/200\n",
      "768/768 [==============================] - 0s 74us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6850 - val_acc: 0.5833\n",
      "Epoch 22/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.6579 - acc: 0.6354 - val_loss: 0.6847 - val_acc: 0.5833\n",
      "Epoch 23/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6572 - acc: 0.6354 - val_loss: 0.6838 - val_acc: 0.5833\n",
      "Epoch 24/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6571 - acc: 0.6354 - val_loss: 0.6855 - val_acc: 0.5833\n",
      "Epoch 25/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6556 - acc: 0.6354 - val_loss: 0.6847 - val_acc: 0.5833\n",
      "Epoch 26/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.6578 - acc: 0.6354 - val_loss: 0.6835 - val_acc: 0.5833\n",
      "Epoch 27/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6570 - acc: 0.6354 - val_loss: 0.6858 - val_acc: 0.5833\n",
      "Epoch 28/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6578 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 29/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.6573 - acc: 0.6354 - val_loss: 0.6842 - val_acc: 0.5833\n",
      "Epoch 30/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6571 - acc: 0.6354 - val_loss: 0.6841 - val_acc: 0.5833\n",
      "Epoch 31/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.6575 - acc: 0.6354 - val_loss: 0.6843 - val_acc: 0.5833\n",
      "Epoch 32/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6836 - val_acc: 0.5833\n",
      "Epoch 33/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6566 - acc: 0.6354 - val_loss: 0.6845 - val_acc: 0.5833\n",
      "Epoch 34/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6584 - acc: 0.6354 - val_loss: 0.6845 - val_acc: 0.5833\n",
      "Epoch 35/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.6566 - acc: 0.6354 - val_loss: 0.6854 - val_acc: 0.5833\n",
      "Epoch 36/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.6577 - acc: 0.6354 - val_loss: 0.6845 - val_acc: 0.5833\n",
      "Epoch 37/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6575 - acc: 0.6354 - val_loss: 0.6848 - val_acc: 0.5833\n",
      "Epoch 38/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6564 - acc: 0.6354 - val_loss: 0.6841 - val_acc: 0.5833\n",
      "Epoch 39/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.6576 - acc: 0.6354 - val_loss: 0.6839 - val_acc: 0.5833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6559 - acc: 0.6354 - val_loss: 0.6843 - val_acc: 0.5833\n",
      "Epoch 41/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6554 - acc: 0.6354 - val_loss: 0.6847 - val_acc: 0.5833\n",
      "Epoch 42/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6568 - acc: 0.6354 - val_loss: 0.6854 - val_acc: 0.5833\n",
      "Epoch 43/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6567 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 44/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.6571 - acc: 0.6354 - val_loss: 0.6842 - val_acc: 0.5833\n",
      "Epoch 45/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6851 - val_acc: 0.5833\n",
      "Epoch 46/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6568 - acc: 0.6354 - val_loss: 0.6845 - val_acc: 0.5833\n",
      "Epoch 47/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6575 - acc: 0.6354 - val_loss: 0.6857 - val_acc: 0.5833\n",
      "Epoch 48/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6851 - val_acc: 0.5833\n",
      "Epoch 49/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.6568 - acc: 0.6354 - val_loss: 0.6838 - val_acc: 0.5833\n",
      "Epoch 50/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6569 - acc: 0.6354 - val_loss: 0.6835 - val_acc: 0.5833\n",
      "Epoch 51/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.6573 - acc: 0.6354 - val_loss: 0.6838 - val_acc: 0.5833\n",
      "Epoch 52/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6567 - acc: 0.6354 - val_loss: 0.6848 - val_acc: 0.5833\n",
      "Epoch 53/200\n",
      "768/768 [==============================] - 0s 80us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6846 - val_acc: 0.5833\n",
      "Epoch 54/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6572 - acc: 0.6354 - val_loss: 0.6845 - val_acc: 0.5833\n",
      "Epoch 55/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6568 - acc: 0.6354 - val_loss: 0.6857 - val_acc: 0.5833\n",
      "Epoch 56/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6578 - acc: 0.6354 - val_loss: 0.6839 - val_acc: 0.5833\n",
      "Epoch 57/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 58/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.6570 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 59/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6565 - acc: 0.6354 - val_loss: 0.6850 - val_acc: 0.5833\n",
      "Epoch 60/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6562 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 61/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6571 - acc: 0.6354 - val_loss: 0.6848 - val_acc: 0.5833\n",
      "Epoch 62/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6560 - acc: 0.6354 - val_loss: 0.6845 - val_acc: 0.5833\n",
      "Epoch 63/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6570 - acc: 0.6354 - val_loss: 0.6845 - val_acc: 0.5833\n",
      "Epoch 64/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6593 - acc: 0.6354 - val_loss: 0.6840 - val_acc: 0.5833\n",
      "Epoch 65/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6565 - acc: 0.6354 - val_loss: 0.6841 - val_acc: 0.5833\n",
      "Epoch 66/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6553 - acc: 0.6354 - val_loss: 0.6843 - val_acc: 0.5833\n",
      "Epoch 67/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6573 - acc: 0.6354 - val_loss: 0.6840 - val_acc: 0.5833\n",
      "Epoch 68/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6559 - acc: 0.6354 - val_loss: 0.6850 - val_acc: 0.5833\n",
      "Epoch 69/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6558 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 70/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6570 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 71/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.6552 - acc: 0.637 - 0s 85us/step - loss: 0.6564 - acc: 0.6354 - val_loss: 0.6852 - val_acc: 0.5833\n",
      "Epoch 72/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6560 - acc: 0.6354 - val_loss: 0.6840 - val_acc: 0.5833\n",
      "Epoch 73/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.6564 - acc: 0.6354 - val_loss: 0.6857 - val_acc: 0.5833\n",
      "Epoch 74/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6567 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 75/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6566 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 76/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6577 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 77/200\n",
      "768/768 [==============================] - 0s 72us/step - loss: 0.6562 - acc: 0.6354 - val_loss: 0.6852 - val_acc: 0.5833\n",
      "Epoch 78/200\n",
      "768/768 [==============================] - 0s 74us/step - loss: 0.6574 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 79/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6565 - acc: 0.6354 - val_loss: 0.6843 - val_acc: 0.5833\n",
      "Epoch 80/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6853 - val_acc: 0.5833\n",
      "Epoch 81/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6567 - acc: 0.6354 - val_loss: 0.6845 - val_acc: 0.5833\n",
      "Epoch 82/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.6569 - acc: 0.6354 - val_loss: 0.6837 - val_acc: 0.5833\n",
      "Epoch 83/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6569 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 84/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6846 - val_acc: 0.5833\n",
      "Epoch 85/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6558 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 86/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6560 - acc: 0.6354 - val_loss: 0.6848 - val_acc: 0.5833\n",
      "Epoch 87/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6579 - acc: 0.6354 - val_loss: 0.6847 - val_acc: 0.5833\n",
      "Epoch 88/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.6557 - acc: 0.6354 - val_loss: 0.6842 - val_acc: 0.5833\n",
      "Epoch 89/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.6571 - acc: 0.6354 - val_loss: 0.6843 - val_acc: 0.5833\n",
      "Epoch 90/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6566 - acc: 0.6354 - val_loss: 0.6838 - val_acc: 0.5833\n",
      "Epoch 91/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6582 - acc: 0.6354 - val_loss: 0.6857 - val_acc: 0.5833\n",
      "Epoch 92/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6552 - acc: 0.6354 - val_loss: 0.6845 - val_acc: 0.5833\n",
      "Epoch 93/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.6557 - acc: 0.6354 - val_loss: 0.6851 - val_acc: 0.5833\n",
      "Epoch 94/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6560 - acc: 0.6354 - val_loss: 0.6853 - val_acc: 0.5833\n",
      "Epoch 95/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6581 - acc: 0.6354 - val_loss: 0.6841 - val_acc: 0.5833\n",
      "Epoch 96/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6558 - acc: 0.6354 - val_loss: 0.6853 - val_acc: 0.5833\n",
      "Epoch 97/200\n",
      "768/768 [==============================] - 0s 73us/step - loss: 0.6577 - acc: 0.6354 - val_loss: 0.6839 - val_acc: 0.5833\n",
      "Epoch 98/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6553 - acc: 0.6354 - val_loss: 0.6841 - val_acc: 0.5833\n",
      "Epoch 99/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6557 - acc: 0.6354 - val_loss: 0.6848 - val_acc: 0.5833\n",
      "Epoch 100/200\n",
      "768/768 [==============================] - 0s 80us/step - loss: 0.6564 - acc: 0.6354 - val_loss: 0.6856 - val_acc: 0.5833\n",
      "Epoch 101/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.6555 - acc: 0.6354 - val_loss: 0.6854 - val_acc: 0.5833\n",
      "Epoch 102/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6577 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 103/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.6556 - acc: 0.6354 - val_loss: 0.6847 - val_acc: 0.5833\n",
      "Epoch 104/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6573 - acc: 0.6354 - val_loss: 0.6838 - val_acc: 0.5833\n",
      "Epoch 105/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.6567 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 106/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6595 - acc: 0.6354 - val_loss: 0.6839 - val_acc: 0.5833\n",
      "Epoch 107/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6562 - acc: 0.6354 - val_loss: 0.6839 - val_acc: 0.5833\n",
      "Epoch 108/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6567 - acc: 0.6354 - val_loss: 0.6848 - val_acc: 0.5833\n",
      "Epoch 109/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6577 - acc: 0.6354 - val_loss: 0.6837 - val_acc: 0.5833\n",
      "Epoch 110/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6562 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 111/200\n",
      "768/768 [==============================] - 0s 75us/step - loss: 0.6559 - acc: 0.6354 - val_loss: 0.6851 - val_acc: 0.5833\n",
      "Epoch 112/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6561 - acc: 0.6354 - val_loss: 0.6843 - val_acc: 0.5833\n",
      "Epoch 113/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6573 - acc: 0.6354 - val_loss: 0.6859 - val_acc: 0.5833\n",
      "Epoch 114/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6568 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 115/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6558 - acc: 0.6354 - val_loss: 0.6847 - val_acc: 0.5833\n",
      "Epoch 116/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.6566 - acc: 0.6354 - val_loss: 0.6840 - val_acc: 0.5833\n",
      "Epoch 117/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.6553 - acc: 0.637 - 0s 88us/step - loss: 0.6562 - acc: 0.6354 - val_loss: 0.6845 - val_acc: 0.5833\n",
      "Epoch 118/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6560 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 119/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6551 - acc: 0.6354 - val_loss: 0.6851 - val_acc: 0.5833\n",
      "Epoch 120/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6578 - acc: 0.6354 - val_loss: 0.6845 - val_acc: 0.5833\n",
      "Epoch 121/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.6572 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 122/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6841 - val_acc: 0.5833\n",
      "Epoch 123/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.6564 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 124/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6577 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 125/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.6545 - acc: 0.638 - 0s 84us/step - loss: 0.6561 - acc: 0.6354 - val_loss: 0.6848 - val_acc: 0.5833\n",
      "Epoch 126/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6557 - acc: 0.6354 - val_loss: 0.6856 - val_acc: 0.5833\n",
      "Epoch 127/200\n",
      "768/768 [==============================] - 0s 73us/step - loss: 0.6560 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 128/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6566 - acc: 0.6354 - val_loss: 0.6842 - val_acc: 0.5833\n",
      "Epoch 129/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.6580 - acc: 0.6354 - val_loss: 0.6846 - val_acc: 0.5833\n",
      "Epoch 130/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6553 - acc: 0.6354 - val_loss: 0.6850 - val_acc: 0.5833\n",
      "Epoch 131/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.6568 - acc: 0.6354 - val_loss: 0.6839 - val_acc: 0.5833\n",
      "Epoch 132/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6557 - acc: 0.6354 - val_loss: 0.6856 - val_acc: 0.5833\n",
      "Epoch 133/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6579 - acc: 0.6354 - val_loss: 0.6845 - val_acc: 0.5833\n",
      "Epoch 134/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6571 - acc: 0.6354 - val_loss: 0.6839 - val_acc: 0.5833\n",
      "Epoch 135/200\n",
      "768/768 [==============================] - 0s 73us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6851 - val_acc: 0.5833\n",
      "Epoch 136/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6566 - acc: 0.6354 - val_loss: 0.6846 - val_acc: 0.5833\n",
      "Epoch 137/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6556 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 138/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6580 - acc: 0.6354 - val_loss: 0.6843 - val_acc: 0.5833\n",
      "Epoch 139/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6559 - acc: 0.6354 - val_loss: 0.6846 - val_acc: 0.5833\n",
      "Epoch 140/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6584 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 141/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6568 - acc: 0.6354 - val_loss: 0.6845 - val_acc: 0.5833\n",
      "Epoch 142/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6558 - acc: 0.6354 - val_loss: 0.6848 - val_acc: 0.5833\n",
      "Epoch 143/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6564 - acc: 0.6354 - val_loss: 0.6854 - val_acc: 0.5833\n",
      "Epoch 144/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.6558 - acc: 0.6354 - val_loss: 0.6847 - val_acc: 0.5833\n",
      "Epoch 145/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6577 - acc: 0.6354 - val_loss: 0.6845 - val_acc: 0.5833\n",
      "Epoch 146/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.6562 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 147/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6574 - acc: 0.6354 - val_loss: 0.6847 - val_acc: 0.5833\n",
      "Epoch 148/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6573 - acc: 0.6354 - val_loss: 0.6842 - val_acc: 0.5833\n",
      "Epoch 149/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.6564 - acc: 0.6354 - val_loss: 0.6848 - val_acc: 0.5833\n",
      "Epoch 150/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6557 - acc: 0.6354 - val_loss: 0.6855 - val_acc: 0.5833\n",
      "Epoch 151/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6564 - acc: 0.6354 - val_loss: 0.6855 - val_acc: 0.5833\n",
      "Epoch 152/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6552 - acc: 0.6354 - val_loss: 0.6850 - val_acc: 0.5833\n",
      "Epoch 153/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 154/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6566 - acc: 0.6354 - val_loss: 0.6856 - val_acc: 0.5833\n",
      "Epoch 155/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6555 - acc: 0.6354 - val_loss: 0.6842 - val_acc: 0.5833\n",
      "Epoch 156/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6570 - acc: 0.6354 - val_loss: 0.6837 - val_acc: 0.5833\n",
      "Epoch 157/200\n",
      "768/768 [==============================] - 0s 72us/step - loss: 0.6560 - acc: 0.6354 - val_loss: 0.6851 - val_acc: 0.5833\n",
      "Epoch 158/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6566 - acc: 0.6354 - val_loss: 0.6848 - val_acc: 0.5833\n",
      "Epoch 159/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6564 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6564 - acc: 0.6354 - val_loss: 0.6848 - val_acc: 0.5833\n",
      "Epoch 161/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.6561 - acc: 0.6354 - val_loss: 0.6854 - val_acc: 0.5833\n",
      "Epoch 162/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6569 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 163/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6561 - acc: 0.6354 - val_loss: 0.6850 - val_acc: 0.5833\n",
      "Epoch 164/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.6561 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 165/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6576 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 166/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.6557 - acc: 0.6354 - val_loss: 0.6851 - val_acc: 0.5833\n",
      "Epoch 167/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6555 - acc: 0.6354 - val_loss: 0.6850 - val_acc: 0.5833\n",
      "Epoch 168/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.6567 - acc: 0.6354 - val_loss: 0.6842 - val_acc: 0.5833\n",
      "Epoch 169/200\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.6566 - acc: 0.6354 - val_loss: 0.6853 - val_acc: 0.5833\n",
      "Epoch 170/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.6570 - acc: 0.6354 - val_loss: 0.6850 - val_acc: 0.5833\n",
      "Epoch 171/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 172/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6572 - acc: 0.6354 - val_loss: 0.6846 - val_acc: 0.5833\n",
      "Epoch 173/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6554 - acc: 0.6354 - val_loss: 0.6842 - val_acc: 0.5833\n",
      "Epoch 174/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6570 - acc: 0.6354 - val_loss: 0.6850 - val_acc: 0.5833\n",
      "Epoch 175/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6852 - val_acc: 0.5833\n",
      "Epoch 176/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 177/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6553 - acc: 0.6354 - val_loss: 0.6847 - val_acc: 0.5833\n",
      "Epoch 178/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6567 - acc: 0.6354 - val_loss: 0.6842 - val_acc: 0.5833\n",
      "Epoch 179/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6566 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 180/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6569 - acc: 0.6354 - val_loss: 0.6843 - val_acc: 0.5833\n",
      "Epoch 181/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6565 - acc: 0.6354 - val_loss: 0.6859 - val_acc: 0.5833\n",
      "Epoch 182/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6848 - val_acc: 0.5833\n",
      "Epoch 183/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6557 - acc: 0.6354 - val_loss: 0.6847 - val_acc: 0.5833\n",
      "Epoch 184/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6856 - val_acc: 0.5833\n",
      "Epoch 185/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6570 - acc: 0.6354 - val_loss: 0.6840 - val_acc: 0.5833\n",
      "Epoch 186/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6557 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 187/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6562 - acc: 0.6354 - val_loss: 0.6848 - val_acc: 0.5833\n",
      "Epoch 188/200\n",
      "768/768 [==============================] - 0s 74us/step - loss: 0.6573 - acc: 0.6354 - val_loss: 0.6856 - val_acc: 0.5833\n",
      "Epoch 189/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6572 - acc: 0.6354 - val_loss: 0.6851 - val_acc: 0.5833\n",
      "Epoch 190/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6556 - acc: 0.6354 - val_loss: 0.6847 - val_acc: 0.5833\n",
      "Epoch 191/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6562 - acc: 0.6354 - val_loss: 0.6841 - val_acc: 0.5833\n",
      "Epoch 192/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6554 - acc: 0.6354 - val_loss: 0.6843 - val_acc: 0.5833\n",
      "Epoch 193/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.6556 - acc: 0.6354 - val_loss: 0.6851 - val_acc: 0.5833\n",
      "Epoch 194/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6554 - acc: 0.6354 - val_loss: 0.6861 - val_acc: 0.5833\n",
      "Epoch 195/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6558 - acc: 0.6354 - val_loss: 0.6855 - val_acc: 0.5833\n",
      "Epoch 196/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6557 - acc: 0.6354 - val_loss: 0.6852 - val_acc: 0.5833\n",
      "Epoch 197/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6567 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 198/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6562 - acc: 0.6354 - val_loss: 0.6840 - val_acc: 0.5833\n",
      "Epoch 199/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6576 - acc: 0.6354 - val_loss: 0.6840 - val_acc: 0.5833\n",
      "Epoch 200/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6572 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Predicting...\n",
      "Cross val iteration 3 of 5\n",
      "Fitting...\n",
      "Train on 768 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "768/768 [==============================] - 0s 408us/step - loss: 0.6851 - acc: 0.6315 - val_loss: 0.6832 - val_acc: 0.5833\n",
      "Epoch 2/200\n",
      "768/768 [==============================] - 0s 74us/step - loss: 0.6679 - acc: 0.6354 - val_loss: 0.6789 - val_acc: 0.5833\n",
      "Epoch 3/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.6589 - acc: 0.6354 - val_loss: 0.6819 - val_acc: 0.5833\n",
      "Epoch 4/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6564 - acc: 0.6354 - val_loss: 0.6820 - val_acc: 0.5833\n",
      "Epoch 5/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6552 - acc: 0.6354 - val_loss: 0.6824 - val_acc: 0.5833\n",
      "Epoch 6/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6554 - acc: 0.6354 - val_loss: 0.6821 - val_acc: 0.5833\n",
      "Epoch 7/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.6570 - acc: 0.6354 - val_loss: 0.6828 - val_acc: 0.5833\n",
      "Epoch 8/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.6517 - acc: 0.6354 - val_loss: 0.6820 - val_acc: 0.5833\n",
      "Epoch 9/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.6538 - acc: 0.6354 - val_loss: 0.6815 - val_acc: 0.5833\n",
      "Epoch 10/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6557 - acc: 0.6354 - val_loss: 0.6812 - val_acc: 0.5833\n",
      "Epoch 11/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6568 - acc: 0.6354 - val_loss: 0.6795 - val_acc: 0.5833\n",
      "Epoch 12/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6545 - acc: 0.6354 - val_loss: 0.6776 - val_acc: 0.5833\n",
      "Epoch 13/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6535 - acc: 0.6354 - val_loss: 0.6786 - val_acc: 0.5833\n",
      "Epoch 14/200\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.6527 - acc: 0.6354 - val_loss: 0.6813 - val_acc: 0.5833\n",
      "Epoch 15/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6532 - acc: 0.6354 - val_loss: 0.6777 - val_acc: 0.5833\n",
      "Epoch 16/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6548 - acc: 0.6354 - val_loss: 0.6764 - val_acc: 0.5833\n",
      "Epoch 17/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6486 - acc: 0.6354 - val_loss: 0.6779 - val_acc: 0.5833\n",
      "Epoch 18/200\n",
      "768/768 [==============================] - 0s 62us/step - loss: 0.6495 - acc: 0.6354 - val_loss: 0.6770 - val_acc: 0.5833\n",
      "Epoch 19/200\n",
      "768/768 [==============================] - 0s 65us/step - loss: 0.6507 - acc: 0.6354 - val_loss: 0.6752 - val_acc: 0.5833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200\n",
      "768/768 [==============================] - 0s 66us/step - loss: 0.6496 - acc: 0.6354 - val_loss: 0.6736 - val_acc: 0.5833\n",
      "Epoch 21/200\n",
      "768/768 [==============================] - 0s 74us/step - loss: 0.6481 - acc: 0.6354 - val_loss: 0.6722 - val_acc: 0.5833\n",
      "Epoch 22/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6453 - acc: 0.6354 - val_loss: 0.6736 - val_acc: 0.5833\n",
      "Epoch 23/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6473 - acc: 0.6354 - val_loss: 0.6687 - val_acc: 0.5833\n",
      "Epoch 24/200\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.6441 - acc: 0.6354 - val_loss: 0.6709 - val_acc: 0.5833\n",
      "Epoch 25/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6422 - acc: 0.6354 - val_loss: 0.6655 - val_acc: 0.5833\n",
      "Epoch 26/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.6386 - acc: 0.6354 - val_loss: 0.6665 - val_acc: 0.5833\n",
      "Epoch 27/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6395 - acc: 0.6354 - val_loss: 0.6604 - val_acc: 0.5833\n",
      "Epoch 28/200\n",
      "768/768 [==============================] - 0s 79us/step - loss: 0.6351 - acc: 0.6354 - val_loss: 0.6585 - val_acc: 0.5833\n",
      "Epoch 29/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.6325 - acc: 0.6354 - val_loss: 0.6553 - val_acc: 0.5833\n",
      "Epoch 30/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.6274 - acc: 0.6354 - val_loss: 0.6476 - val_acc: 0.5833\n",
      "Epoch 31/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6258 - acc: 0.6354 - val_loss: 0.6466 - val_acc: 0.5833\n",
      "Epoch 32/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.6165 - acc: 0.6354 - val_loss: 0.6373 - val_acc: 0.5833\n",
      "Epoch 33/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.6155 - acc: 0.6354 - val_loss: 0.6359 - val_acc: 0.5833\n",
      "Epoch 34/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6099 - acc: 0.6354 - val_loss: 0.6256 - val_acc: 0.5833\n",
      "Epoch 35/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5999 - acc: 0.6354 - val_loss: 0.6198 - val_acc: 0.5833\n",
      "Epoch 36/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5958 - acc: 0.6354 - val_loss: 0.6126 - val_acc: 0.5833\n",
      "Epoch 37/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5918 - acc: 0.6354 - val_loss: 0.6179 - val_acc: 0.5833\n",
      "Epoch 38/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5843 - acc: 0.6354 - val_loss: 0.6026 - val_acc: 0.5833\n",
      "Epoch 39/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.5809 - acc: 0.6354 - val_loss: 0.6202 - val_acc: 0.5833\n",
      "Epoch 40/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5802 - acc: 0.6354 - val_loss: 0.5910 - val_acc: 0.5833\n",
      "Epoch 41/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5719 - acc: 0.6354 - val_loss: 0.5924 - val_acc: 0.5833\n",
      "Epoch 42/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5744 - acc: 0.6354 - val_loss: 0.5825 - val_acc: 0.5833\n",
      "Epoch 43/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.5730 - acc: 0.6354 - val_loss: 0.5833 - val_acc: 0.5833\n",
      "Epoch 44/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.5646 - acc: 0.6354 - val_loss: 0.5747 - val_acc: 0.5833\n",
      "Epoch 45/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.5590 - acc: 0.6354 - val_loss: 0.5740 - val_acc: 0.5885\n",
      "Epoch 46/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5537 - acc: 0.6836 - val_loss: 0.5690 - val_acc: 0.6510\n",
      "Epoch 47/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5515 - acc: 0.6953 - val_loss: 0.5697 - val_acc: 0.6198\n",
      "Epoch 48/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5535 - acc: 0.6992 - val_loss: 0.5639 - val_acc: 0.7083\n",
      "Epoch 49/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5501 - acc: 0.7227 - val_loss: 0.5622 - val_acc: 0.7083\n",
      "Epoch 50/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.5531 - acc: 0.705 - 0s 84us/step - loss: 0.5537 - acc: 0.7070 - val_loss: 0.5603 - val_acc: 0.7135\n",
      "Epoch 51/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5462 - acc: 0.7070 - val_loss: 0.5589 - val_acc: 0.7083\n",
      "Epoch 52/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5443 - acc: 0.7266 - val_loss: 0.5585 - val_acc: 0.6823\n",
      "Epoch 53/200\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.5424 - acc: 0.7305 - val_loss: 0.5559 - val_acc: 0.7083\n",
      "Epoch 54/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5457 - acc: 0.6784 - val_loss: 0.5589 - val_acc: 0.7135\n",
      "Epoch 55/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.5435 - acc: 0.7109 - val_loss: 0.5603 - val_acc: 0.7031\n",
      "Epoch 56/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.5420 - acc: 0.7161 - val_loss: 0.5531 - val_acc: 0.7083\n",
      "Epoch 57/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5385 - acc: 0.7435 - val_loss: 0.5502 - val_acc: 0.7344\n",
      "Epoch 58/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.5390 - acc: 0.7318 - val_loss: 0.5544 - val_acc: 0.7083\n",
      "Epoch 59/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5380 - acc: 0.7357 - val_loss: 0.5475 - val_acc: 0.7396\n",
      "Epoch 60/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.5407 - acc: 0.7227 - val_loss: 0.5464 - val_acc: 0.7396\n",
      "Epoch 61/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.5435 - acc: 0.720 - 0s 86us/step - loss: 0.5428 - acc: 0.7174 - val_loss: 0.5587 - val_acc: 0.6667\n",
      "Epoch 62/200\n",
      "768/768 [==============================] - 0s 80us/step - loss: 0.5285 - acc: 0.7318 - val_loss: 0.5474 - val_acc: 0.7031\n",
      "Epoch 63/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5449 - acc: 0.7318 - val_loss: 0.5436 - val_acc: 0.7448\n",
      "Epoch 64/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.5313 - acc: 0.7422 - val_loss: 0.5431 - val_acc: 0.7344\n",
      "Epoch 65/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5394 - acc: 0.7122 - val_loss: 0.5523 - val_acc: 0.6927\n",
      "Epoch 66/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5372 - acc: 0.7305 - val_loss: 0.5425 - val_acc: 0.7396\n",
      "Epoch 67/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5258 - acc: 0.7487 - val_loss: 0.5420 - val_acc: 0.7396\n",
      "Epoch 68/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.5312 - acc: 0.7435 - val_loss: 0.5395 - val_acc: 0.7344\n",
      "Epoch 69/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.5262 - acc: 0.7396 - val_loss: 0.5396 - val_acc: 0.7188\n",
      "Epoch 70/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5260 - acc: 0.7083 - val_loss: 0.5462 - val_acc: 0.6927\n",
      "Epoch 71/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5269 - acc: 0.7331 - val_loss: 0.5381 - val_acc: 0.7188\n",
      "Epoch 72/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5250 - acc: 0.7422 - val_loss: 0.5379 - val_acc: 0.7396\n",
      "Epoch 73/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5344 - acc: 0.7292 - val_loss: 0.5422 - val_acc: 0.7292\n",
      "Epoch 74/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5350 - acc: 0.7253 - val_loss: 0.5365 - val_acc: 0.7448\n",
      "Epoch 75/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5232 - acc: 0.7422 - val_loss: 0.5351 - val_acc: 0.7292\n",
      "Epoch 76/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5295 - acc: 0.7500 - val_loss: 0.5343 - val_acc: 0.7292\n",
      "Epoch 77/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.5181 - acc: 0.7396 - val_loss: 0.5333 - val_acc: 0.7448\n",
      "Epoch 78/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5297 - acc: 0.7318 - val_loss: 0.5477 - val_acc: 0.6927\n",
      "Epoch 79/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.5187 - acc: 0.7422 - val_loss: 0.5328 - val_acc: 0.7552\n",
      "Epoch 80/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.5272 - acc: 0.743 - 0s 82us/step - loss: 0.5288 - acc: 0.7448 - val_loss: 0.5314 - val_acc: 0.7448\n",
      "Epoch 81/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5263 - acc: 0.7331 - val_loss: 0.5308 - val_acc: 0.7396\n",
      "Epoch 82/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5162 - acc: 0.7500 - val_loss: 0.5340 - val_acc: 0.7500\n",
      "Epoch 83/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5180 - acc: 0.7526 - val_loss: 0.5298 - val_acc: 0.7500\n",
      "Epoch 84/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.5173 - acc: 0.7474 - val_loss: 0.5302 - val_acc: 0.7552\n",
      "Epoch 85/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.5172 - acc: 0.7500 - val_loss: 0.5284 - val_acc: 0.7396\n",
      "Epoch 86/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5199 - acc: 0.7435 - val_loss: 0.5387 - val_acc: 0.7396\n",
      "Epoch 87/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5275 - acc: 0.7435 - val_loss: 0.5346 - val_acc: 0.7448\n",
      "Epoch 88/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5160 - acc: 0.7422 - val_loss: 0.5288 - val_acc: 0.7552\n",
      "Epoch 89/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5134 - acc: 0.7422 - val_loss: 0.5295 - val_acc: 0.7552\n",
      "Epoch 90/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5141 - acc: 0.7565 - val_loss: 0.5270 - val_acc: 0.7500\n",
      "Epoch 91/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.5288 - acc: 0.7266 - val_loss: 0.5306 - val_acc: 0.7604\n",
      "Epoch 92/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.5183 - acc: 0.7500 - val_loss: 0.5266 - val_acc: 0.7500\n",
      "Epoch 93/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.5245 - acc: 0.7214 - val_loss: 0.5285 - val_acc: 0.6927\n",
      "Epoch 94/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5229 - acc: 0.7435 - val_loss: 0.5250 - val_acc: 0.7500\n",
      "Epoch 95/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5116 - acc: 0.7617 - val_loss: 0.5309 - val_acc: 0.7552\n",
      "Epoch 96/200\n",
      "768/768 [==============================] - 0s 80us/step - loss: 0.5193 - acc: 0.7396 - val_loss: 0.5288 - val_acc: 0.6771\n",
      "Epoch 97/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5345 - acc: 0.7318 - val_loss: 0.5242 - val_acc: 0.7292\n",
      "Epoch 98/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.4973 - acc: 0.757 - 0s 89us/step - loss: 0.5015 - acc: 0.7539 - val_loss: 0.5234 - val_acc: 0.7292\n",
      "Epoch 99/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.5114 - acc: 0.7630 - val_loss: 0.5248 - val_acc: 0.7500\n",
      "Epoch 100/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5191 - acc: 0.7513 - val_loss: 0.5213 - val_acc: 0.7448\n",
      "Epoch 101/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.5032 - acc: 0.7578 - val_loss: 0.5246 - val_acc: 0.6823\n",
      "Epoch 102/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5103 - acc: 0.7539 - val_loss: 0.5271 - val_acc: 0.6875\n",
      "Epoch 103/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5156 - acc: 0.7448 - val_loss: 0.5217 - val_acc: 0.7135\n",
      "Epoch 104/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5059 - acc: 0.7435 - val_loss: 0.5201 - val_acc: 0.7240\n",
      "Epoch 105/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5103 - acc: 0.7552 - val_loss: 0.5187 - val_acc: 0.7500\n",
      "Epoch 106/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5059 - acc: 0.7643 - val_loss: 0.5196 - val_acc: 0.7604\n",
      "Epoch 107/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5045 - acc: 0.7630 - val_loss: 0.5179 - val_acc: 0.7604\n",
      "Epoch 108/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5012 - acc: 0.7695 - val_loss: 0.5215 - val_acc: 0.7552\n",
      "Epoch 109/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.5113 - acc: 0.7578 - val_loss: 0.5209 - val_acc: 0.7552\n",
      "Epoch 110/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5028 - acc: 0.7630 - val_loss: 0.5204 - val_acc: 0.7552\n",
      "Epoch 111/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5048 - acc: 0.7539 - val_loss: 0.5179 - val_acc: 0.7083\n",
      "Epoch 112/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5080 - acc: 0.7461 - val_loss: 0.5158 - val_acc: 0.7240\n",
      "Epoch 113/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5058 - acc: 0.7539 - val_loss: 0.5175 - val_acc: 0.7604\n",
      "Epoch 114/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4863 - acc: 0.7617 - val_loss: 0.5247 - val_acc: 0.6979\n",
      "Epoch 115/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4971 - acc: 0.7565 - val_loss: 0.5139 - val_acc: 0.7500\n",
      "Epoch 116/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5013 - acc: 0.7604 - val_loss: 0.5141 - val_acc: 0.7083\n",
      "Epoch 117/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5125 - acc: 0.7604 - val_loss: 0.5163 - val_acc: 0.6979\n",
      "Epoch 118/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5085 - acc: 0.7565 - val_loss: 0.5140 - val_acc: 0.7500\n",
      "Epoch 119/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4919 - acc: 0.7630 - val_loss: 0.5335 - val_acc: 0.6771\n",
      "Epoch 120/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5179 - acc: 0.7357 - val_loss: 0.5241 - val_acc: 0.6979\n",
      "Epoch 121/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.5030 - acc: 0.7500 - val_loss: 0.5150 - val_acc: 0.6979\n",
      "Epoch 122/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5159 - acc: 0.7383 - val_loss: 0.5242 - val_acc: 0.6927\n",
      "Epoch 123/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5181 - acc: 0.7513 - val_loss: 0.5122 - val_acc: 0.7448\n",
      "Epoch 124/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5010 - acc: 0.7552 - val_loss: 0.5161 - val_acc: 0.7656\n",
      "Epoch 125/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.4991 - acc: 0.7656 - val_loss: 0.5229 - val_acc: 0.7656\n",
      "Epoch 126/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.4991 - acc: 0.7578 - val_loss: 0.5242 - val_acc: 0.6927\n",
      "Epoch 127/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4926 - acc: 0.7539 - val_loss: 0.5122 - val_acc: 0.7135\n",
      "Epoch 128/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4965 - acc: 0.7643 - val_loss: 0.5102 - val_acc: 0.7448\n",
      "Epoch 129/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4903 - acc: 0.7721 - val_loss: 0.5227 - val_acc: 0.7708\n",
      "Epoch 130/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4934 - acc: 0.7487 - val_loss: 0.5145 - val_acc: 0.7604\n",
      "Epoch 131/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4967 - acc: 0.7578 - val_loss: 0.5107 - val_acc: 0.7500\n",
      "Epoch 132/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4958 - acc: 0.7448 - val_loss: 0.5252 - val_acc: 0.7708\n",
      "Epoch 133/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4988 - acc: 0.7604 - val_loss: 0.5135 - val_acc: 0.7604\n",
      "Epoch 134/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4922 - acc: 0.7708 - val_loss: 0.5084 - val_acc: 0.7448\n",
      "Epoch 135/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.4933 - acc: 0.7630 - val_loss: 0.5087 - val_acc: 0.7500\n",
      "Epoch 136/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4913 - acc: 0.7591 - val_loss: 0.5099 - val_acc: 0.7552\n",
      "Epoch 137/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4828 - acc: 0.7760 - val_loss: 0.5090 - val_acc: 0.7083\n",
      "Epoch 138/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4966 - acc: 0.7565 - val_loss: 0.5069 - val_acc: 0.7240\n",
      "Epoch 139/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.4899 - acc: 0.759 - 0s 84us/step - loss: 0.4924 - acc: 0.7643 - val_loss: 0.5073 - val_acc: 0.7135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4950 - acc: 0.7552 - val_loss: 0.5083 - val_acc: 0.7031\n",
      "Epoch 141/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4906 - acc: 0.7617 - val_loss: 0.5249 - val_acc: 0.6875\n",
      "Epoch 142/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.4917 - acc: 0.774 - 0s 86us/step - loss: 0.4903 - acc: 0.7734 - val_loss: 0.5068 - val_acc: 0.7135\n",
      "Epoch 143/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4916 - acc: 0.7578 - val_loss: 0.5076 - val_acc: 0.7031\n",
      "Epoch 144/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4845 - acc: 0.7695 - val_loss: 0.5052 - val_acc: 0.7240\n",
      "Epoch 145/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4880 - acc: 0.7682 - val_loss: 0.5048 - val_acc: 0.7292\n",
      "Epoch 146/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4877 - acc: 0.7786 - val_loss: 0.5067 - val_acc: 0.7031\n",
      "Epoch 147/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4951 - acc: 0.7826 - val_loss: 0.5040 - val_acc: 0.7396\n",
      "Epoch 148/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4858 - acc: 0.7604 - val_loss: 0.5039 - val_acc: 0.7448\n",
      "Epoch 149/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4955 - acc: 0.7695 - val_loss: 0.5316 - val_acc: 0.6927\n",
      "Epoch 150/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.4987 - acc: 0.7461 - val_loss: 0.5483 - val_acc: 0.6927\n",
      "Epoch 151/200\n",
      "768/768 [==============================] - 0s 80us/step - loss: 0.4933 - acc: 0.7604 - val_loss: 0.5099 - val_acc: 0.7240\n",
      "Epoch 152/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5021 - acc: 0.7396 - val_loss: 0.5042 - val_acc: 0.7552\n",
      "Epoch 153/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.4796 - acc: 0.7695 - val_loss: 0.5058 - val_acc: 0.7135\n",
      "Epoch 154/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.4814 - acc: 0.7591 - val_loss: 0.5038 - val_acc: 0.7552\n",
      "Epoch 155/200\n",
      "768/768 [==============================] - 0s 80us/step - loss: 0.4801 - acc: 0.7695 - val_loss: 0.5030 - val_acc: 0.7292\n",
      "Epoch 156/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.4750 - acc: 0.7891 - val_loss: 0.5182 - val_acc: 0.6875\n",
      "Epoch 157/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.4882 - acc: 0.7435 - val_loss: 0.5017 - val_acc: 0.7500\n",
      "Epoch 158/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4840 - acc: 0.7865 - val_loss: 0.5034 - val_acc: 0.7604\n",
      "Epoch 159/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4805 - acc: 0.7721 - val_loss: 0.5042 - val_acc: 0.7188\n",
      "Epoch 160/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4843 - acc: 0.7812 - val_loss: 0.5013 - val_acc: 0.7500\n",
      "Epoch 161/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4705 - acc: 0.7839 - val_loss: 0.5021 - val_acc: 0.7552\n",
      "Epoch 162/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4750 - acc: 0.7786 - val_loss: 0.5006 - val_acc: 0.7500\n",
      "Epoch 163/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4912 - acc: 0.7500 - val_loss: 0.5007 - val_acc: 0.7500\n",
      "Epoch 164/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.4771 - acc: 0.7773 - val_loss: 0.5142 - val_acc: 0.6927\n",
      "Epoch 165/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.4781 - acc: 0.7799 - val_loss: 0.5070 - val_acc: 0.7135\n",
      "Epoch 166/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.4790 - acc: 0.7656 - val_loss: 0.4996 - val_acc: 0.7552\n",
      "Epoch 167/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4718 - acc: 0.7682 - val_loss: 0.5118 - val_acc: 0.7552\n",
      "Epoch 168/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4701 - acc: 0.7799 - val_loss: 0.5109 - val_acc: 0.7552\n",
      "Epoch 169/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4844 - acc: 0.7773 - val_loss: 0.5043 - val_acc: 0.7552\n",
      "Epoch 170/200\n",
      "768/768 [==============================] - 0s 79us/step - loss: 0.4748 - acc: 0.7708 - val_loss: 0.5027 - val_acc: 0.7604\n",
      "Epoch 171/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4797 - acc: 0.7760 - val_loss: 0.4987 - val_acc: 0.7500\n",
      "Epoch 172/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.4748 - acc: 0.7734 - val_loss: 0.5014 - val_acc: 0.7604\n",
      "Epoch 173/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4653 - acc: 0.7865 - val_loss: 0.4983 - val_acc: 0.7552\n",
      "Epoch 174/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4770 - acc: 0.7930 - val_loss: 0.4981 - val_acc: 0.7552\n",
      "Epoch 175/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.4700 - acc: 0.7878 - val_loss: 0.4980 - val_acc: 0.7448\n",
      "Epoch 176/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4576 - acc: 0.7721 - val_loss: 0.4984 - val_acc: 0.7500\n",
      "Epoch 177/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4706 - acc: 0.7773 - val_loss: 0.4973 - val_acc: 0.7656\n",
      "Epoch 178/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4749 - acc: 0.7747 - val_loss: 0.5501 - val_acc: 0.7240\n",
      "Epoch 179/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4744 - acc: 0.7656 - val_loss: 0.4997 - val_acc: 0.7604\n",
      "Epoch 180/200\n",
      "768/768 [==============================] - 0s 79us/step - loss: 0.4777 - acc: 0.7695 - val_loss: 0.4970 - val_acc: 0.7656\n",
      "Epoch 181/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4771 - acc: 0.7630 - val_loss: 0.4969 - val_acc: 0.7656\n",
      "Epoch 182/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.4686 - acc: 0.7826 - val_loss: 0.4981 - val_acc: 0.7552\n",
      "Epoch 183/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4826 - acc: 0.7552 - val_loss: 0.4978 - val_acc: 0.7604\n",
      "Epoch 184/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.4727 - acc: 0.7760 - val_loss: 0.4968 - val_acc: 0.7500\n",
      "Epoch 185/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.4757 - acc: 0.7721 - val_loss: 0.4970 - val_acc: 0.7604\n",
      "Epoch 186/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4667 - acc: 0.7891 - val_loss: 0.4959 - val_acc: 0.7708\n",
      "Epoch 187/200\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.4697 - acc: 0.7760 - val_loss: 0.4984 - val_acc: 0.7188\n",
      "Epoch 188/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4736 - acc: 0.7760 - val_loss: 0.4957 - val_acc: 0.7656\n",
      "Epoch 189/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.4807 - acc: 0.7760 - val_loss: 0.4961 - val_acc: 0.7604\n",
      "Epoch 190/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4602 - acc: 0.7786 - val_loss: 0.4970 - val_acc: 0.7604\n",
      "Epoch 191/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4618 - acc: 0.7734 - val_loss: 0.4958 - val_acc: 0.7656\n",
      "Epoch 192/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.4701 - acc: 0.7956 - val_loss: 0.4969 - val_acc: 0.7344\n",
      "Epoch 193/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.4676 - acc: 0.788 - 0s 83us/step - loss: 0.4674 - acc: 0.7878 - val_loss: 0.5001 - val_acc: 0.7604\n",
      "Epoch 194/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4941 - acc: 0.7695 - val_loss: 0.4953 - val_acc: 0.7604\n",
      "Epoch 195/200\n",
      "768/768 [==============================] - 0s 79us/step - loss: 0.4873 - acc: 0.7591 - val_loss: 0.4951 - val_acc: 0.7708\n",
      "Epoch 196/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4585 - acc: 0.7773 - val_loss: 0.5041 - val_acc: 0.7760\n",
      "Epoch 197/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4689 - acc: 0.7969 - val_loss: 0.4960 - val_acc: 0.7656\n",
      "Epoch 198/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4663 - acc: 0.7852 - val_loss: 0.4962 - val_acc: 0.7656\n",
      "Epoch 199/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4681 - acc: 0.7682 - val_loss: 0.4958 - val_acc: 0.7656\n",
      "Epoch 200/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4770 - acc: 0.7839 - val_loss: 0.4944 - val_acc: 0.7708\n",
      "Predicting...\n",
      "Cross val iteration 4 of 5\n",
      "Fitting...\n",
      "Train on 768 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.6816 - acc: 0.6250 - val_loss: 0.6801 - val_acc: 0.5833\n",
      "Epoch 2/200\n",
      "768/768 [==============================] - 0s 74us/step - loss: 0.6621 - acc: 0.6354 - val_loss: 0.6801 - val_acc: 0.5833\n",
      "Epoch 3/200\n",
      "768/768 [==============================] - 0s 74us/step - loss: 0.6557 - acc: 0.6354 - val_loss: 0.6841 - val_acc: 0.5833\n",
      "Epoch 4/200\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.6569 - acc: 0.6354 - val_loss: 0.6833 - val_acc: 0.5833\n",
      "Epoch 5/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.6577 - acc: 0.6354 - val_loss: 0.6804 - val_acc: 0.5833\n",
      "Epoch 6/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6562 - acc: 0.6354 - val_loss: 0.6818 - val_acc: 0.5833\n",
      "Epoch 7/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6551 - acc: 0.6354 - val_loss: 0.6809 - val_acc: 0.5833\n",
      "Epoch 8/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.6528 - acc: 0.6354 - val_loss: 0.6826 - val_acc: 0.5833\n",
      "Epoch 9/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.6564 - acc: 0.6354 - val_loss: 0.6789 - val_acc: 0.5833\n",
      "Epoch 10/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6567 - acc: 0.6354 - val_loss: 0.6800 - val_acc: 0.5833\n",
      "Epoch 11/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6533 - acc: 0.6354 - val_loss: 0.6779 - val_acc: 0.5833\n",
      "Epoch 12/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6510 - acc: 0.6354 - val_loss: 0.6787 - val_acc: 0.5833\n",
      "Epoch 13/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6546 - acc: 0.6354 - val_loss: 0.6767 - val_acc: 0.5833\n",
      "Epoch 14/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6526 - acc: 0.6354 - val_loss: 0.6765 - val_acc: 0.5833\n",
      "Epoch 15/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6496 - acc: 0.6354 - val_loss: 0.6765 - val_acc: 0.5833\n",
      "Epoch 16/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6454 - acc: 0.6354 - val_loss: 0.6749 - val_acc: 0.5833\n",
      "Epoch 17/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.6445 - acc: 0.637 - 0s 85us/step - loss: 0.6457 - acc: 0.6354 - val_loss: 0.6734 - val_acc: 0.5833\n",
      "Epoch 18/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6460 - acc: 0.6354 - val_loss: 0.6709 - val_acc: 0.5833\n",
      "Epoch 19/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6476 - acc: 0.6354 - val_loss: 0.6702 - val_acc: 0.5833\n",
      "Epoch 20/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.6405 - acc: 0.6354 - val_loss: 0.6689 - val_acc: 0.5833\n",
      "Epoch 21/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6396 - acc: 0.6354 - val_loss: 0.6644 - val_acc: 0.5833\n",
      "Epoch 22/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6366 - acc: 0.6354 - val_loss: 0.6635 - val_acc: 0.5833\n",
      "Epoch 23/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6363 - acc: 0.6354 - val_loss: 0.6575 - val_acc: 0.5833\n",
      "Epoch 24/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6323 - acc: 0.6354 - val_loss: 0.6566 - val_acc: 0.5833\n",
      "Epoch 25/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.6286 - acc: 0.6354 - val_loss: 0.6489 - val_acc: 0.5833\n",
      "Epoch 26/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.6260 - acc: 0.6354 - val_loss: 0.6444 - val_acc: 0.5833\n",
      "Epoch 27/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.6224 - acc: 0.6354 - val_loss: 0.6462 - val_acc: 0.5833\n",
      "Epoch 28/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6195 - acc: 0.6354 - val_loss: 0.6372 - val_acc: 0.5833\n",
      "Epoch 29/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6060 - acc: 0.6354 - val_loss: 0.6218 - val_acc: 0.5833\n",
      "Epoch 30/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5978 - acc: 0.6354 - val_loss: 0.6135 - val_acc: 0.5833\n",
      "Epoch 31/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5917 - acc: 0.6354 - val_loss: 0.6053 - val_acc: 0.5833\n",
      "Epoch 32/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5829 - acc: 0.6341 - val_loss: 0.6038 - val_acc: 0.5833\n",
      "Epoch 33/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5780 - acc: 0.6549 - val_loss: 0.5991 - val_acc: 0.6927\n",
      "Epoch 34/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5742 - acc: 0.6536 - val_loss: 0.5878 - val_acc: 0.7083\n",
      "Epoch 35/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5642 - acc: 0.6810 - val_loss: 0.5756 - val_acc: 0.7031\n",
      "Epoch 36/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5606 - acc: 0.6810 - val_loss: 0.5725 - val_acc: 0.7135\n",
      "Epoch 37/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5574 - acc: 0.6862 - val_loss: 0.5653 - val_acc: 0.7031\n",
      "Epoch 38/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5570 - acc: 0.6849 - val_loss: 0.5658 - val_acc: 0.6510\n",
      "Epoch 39/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.5549 - acc: 0.6940 - val_loss: 0.5645 - val_acc: 0.6562\n",
      "Epoch 40/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5524 - acc: 0.6745 - val_loss: 0.5593 - val_acc: 0.6719\n",
      "Epoch 41/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5457 - acc: 0.6979 - val_loss: 0.5585 - val_acc: 0.6979\n",
      "Epoch 42/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5546 - acc: 0.6849 - val_loss: 0.5687 - val_acc: 0.6719\n",
      "Epoch 43/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5401 - acc: 0.7188 - val_loss: 0.5836 - val_acc: 0.5885\n",
      "Epoch 44/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5491 - acc: 0.7005 - val_loss: 0.5648 - val_acc: 0.6458\n",
      "Epoch 45/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5499 - acc: 0.6901 - val_loss: 0.5635 - val_acc: 0.6510\n",
      "Epoch 46/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5440 - acc: 0.6927 - val_loss: 0.5523 - val_acc: 0.7135\n",
      "Epoch 47/200\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.5359 - acc: 0.7266 - val_loss: 0.5434 - val_acc: 0.7240\n",
      "Epoch 48/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5393 - acc: 0.7292 - val_loss: 0.5417 - val_acc: 0.7240\n",
      "Epoch 49/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5339 - acc: 0.7201 - val_loss: 0.5406 - val_acc: 0.7396\n",
      "Epoch 50/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5319 - acc: 0.7201 - val_loss: 0.5409 - val_acc: 0.7083\n",
      "Epoch 51/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5325 - acc: 0.7266 - val_loss: 0.5611 - val_acc: 0.6562\n",
      "Epoch 52/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.5397 - acc: 0.7057 - val_loss: 0.5390 - val_acc: 0.7083\n",
      "Epoch 53/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5374 - acc: 0.7174 - val_loss: 0.5386 - val_acc: 0.7083\n",
      "Epoch 54/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5299 - acc: 0.7240 - val_loss: 0.5360 - val_acc: 0.7083\n",
      "Epoch 55/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.5280 - acc: 0.7253 - val_loss: 0.5347 - val_acc: 0.7240\n",
      "Epoch 56/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5270 - acc: 0.7357 - val_loss: 0.5371 - val_acc: 0.7396\n",
      "Epoch 57/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5285 - acc: 0.7487 - val_loss: 0.5536 - val_acc: 0.6823\n",
      "Epoch 58/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5328 - acc: 0.7227 - val_loss: 0.5760 - val_acc: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5386 - acc: 0.7070 - val_loss: 0.5670 - val_acc: 0.6354\n",
      "Epoch 60/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.5506 - acc: 0.7057 - val_loss: 0.5517 - val_acc: 0.6979\n",
      "Epoch 61/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5306 - acc: 0.7161 - val_loss: 0.5347 - val_acc: 0.7500\n",
      "Epoch 62/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5310 - acc: 0.7396 - val_loss: 0.5310 - val_acc: 0.7031\n",
      "Epoch 63/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5361 - acc: 0.7227 - val_loss: 0.5494 - val_acc: 0.6875\n",
      "Epoch 64/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.5422 - acc: 0.6992 - val_loss: 0.5407 - val_acc: 0.6875\n",
      "Epoch 65/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5368 - acc: 0.7109 - val_loss: 0.5327 - val_acc: 0.7500\n",
      "Epoch 66/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5224 - acc: 0.7513 - val_loss: 0.5333 - val_acc: 0.6979\n",
      "Epoch 67/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5241 - acc: 0.7253 - val_loss: 0.5284 - val_acc: 0.7344\n",
      "Epoch 68/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5206 - acc: 0.7174 - val_loss: 0.5298 - val_acc: 0.6979\n",
      "Epoch 69/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5230 - acc: 0.7461 - val_loss: 0.5309 - val_acc: 0.6927\n",
      "Epoch 70/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5130 - acc: 0.7305 - val_loss: 0.5264 - val_acc: 0.7135\n",
      "Epoch 71/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5168 - acc: 0.7396 - val_loss: 0.5306 - val_acc: 0.6771\n",
      "Epoch 72/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5182 - acc: 0.7422 - val_loss: 0.5320 - val_acc: 0.7500\n",
      "Epoch 73/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5140 - acc: 0.7396 - val_loss: 0.5238 - val_acc: 0.7396\n",
      "Epoch 74/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5128 - acc: 0.7370 - val_loss: 0.5260 - val_acc: 0.6823\n",
      "Epoch 75/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5107 - acc: 0.7487 - val_loss: 0.5264 - val_acc: 0.6823\n",
      "Epoch 76/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5140 - acc: 0.7214 - val_loss: 0.5231 - val_acc: 0.6927\n",
      "Epoch 77/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5197 - acc: 0.7526 - val_loss: 0.5243 - val_acc: 0.7552\n",
      "Epoch 78/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5061 - acc: 0.7474 - val_loss: 0.5222 - val_acc: 0.6823\n",
      "Epoch 79/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5111 - acc: 0.7487 - val_loss: 0.5301 - val_acc: 0.6875\n",
      "Epoch 80/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5006 - acc: 0.7565 - val_loss: 0.5193 - val_acc: 0.7448\n",
      "Epoch 81/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.5131 - acc: 0.7370 - val_loss: 0.5310 - val_acc: 0.6771\n",
      "Epoch 82/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5070 - acc: 0.7539 - val_loss: 0.5237 - val_acc: 0.7448\n",
      "Epoch 83/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5127 - acc: 0.7396 - val_loss: 0.5360 - val_acc: 0.7448\n",
      "Epoch 84/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5163 - acc: 0.7396 - val_loss: 0.5276 - val_acc: 0.6927\n",
      "Epoch 85/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.5080 - acc: 0.7513 - val_loss: 0.5169 - val_acc: 0.7552\n",
      "Epoch 86/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4968 - acc: 0.7669 - val_loss: 0.5182 - val_acc: 0.7552\n",
      "Epoch 87/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5047 - acc: 0.7526 - val_loss: 0.5172 - val_acc: 0.7552\n",
      "Epoch 88/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4958 - acc: 0.7448 - val_loss: 0.5157 - val_acc: 0.6979\n",
      "Epoch 89/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5054 - acc: 0.7565 - val_loss: 0.5178 - val_acc: 0.7604\n",
      "Epoch 90/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5064 - acc: 0.7513 - val_loss: 0.5142 - val_acc: 0.7500\n",
      "Epoch 91/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4944 - acc: 0.7617 - val_loss: 0.5160 - val_acc: 0.6875\n",
      "Epoch 92/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5015 - acc: 0.7682 - val_loss: 0.5175 - val_acc: 0.6875\n",
      "Epoch 93/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5000 - acc: 0.7695 - val_loss: 0.5168 - val_acc: 0.7552\n",
      "Epoch 94/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5044 - acc: 0.7435 - val_loss: 0.5210 - val_acc: 0.7604\n",
      "Epoch 95/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4969 - acc: 0.7630 - val_loss: 0.5142 - val_acc: 0.7604\n",
      "Epoch 96/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5023 - acc: 0.7513 - val_loss: 0.5113 - val_acc: 0.7500\n",
      "Epoch 97/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4965 - acc: 0.7643 - val_loss: 0.5122 - val_acc: 0.7500\n",
      "Epoch 98/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4992 - acc: 0.7591 - val_loss: 0.5294 - val_acc: 0.6719\n",
      "Epoch 99/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4951 - acc: 0.7656 - val_loss: 0.5245 - val_acc: 0.6927\n",
      "Epoch 100/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4919 - acc: 0.7721 - val_loss: 0.5103 - val_acc: 0.7188\n",
      "Epoch 101/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4961 - acc: 0.7695 - val_loss: 0.5101 - val_acc: 0.7500\n",
      "Epoch 102/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5009 - acc: 0.7552 - val_loss: 0.5257 - val_acc: 0.7656\n",
      "Epoch 103/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4988 - acc: 0.7513 - val_loss: 0.5090 - val_acc: 0.7448\n",
      "Epoch 104/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4959 - acc: 0.7591 - val_loss: 0.5087 - val_acc: 0.7396\n",
      "Epoch 105/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4911 - acc: 0.7734 - val_loss: 0.5124 - val_acc: 0.7760\n",
      "Epoch 106/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4951 - acc: 0.7565 - val_loss: 0.5264 - val_acc: 0.7656\n",
      "Epoch 107/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5042 - acc: 0.7435 - val_loss: 0.5076 - val_acc: 0.7448\n",
      "Epoch 108/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.4888 - acc: 0.7656 - val_loss: 0.5087 - val_acc: 0.7396\n",
      "Epoch 109/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4850 - acc: 0.7643 - val_loss: 0.5191 - val_acc: 0.7656\n",
      "Epoch 110/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4968 - acc: 0.7526 - val_loss: 0.5205 - val_acc: 0.7708\n",
      "Epoch 111/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4828 - acc: 0.7682 - val_loss: 0.5697 - val_acc: 0.6719\n",
      "Epoch 112/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5181 - acc: 0.7305 - val_loss: 0.5095 - val_acc: 0.7656\n",
      "Epoch 113/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4812 - acc: 0.7760 - val_loss: 0.5094 - val_acc: 0.7083\n",
      "Epoch 114/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4818 - acc: 0.7747 - val_loss: 0.5084 - val_acc: 0.7031\n",
      "Epoch 115/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4857 - acc: 0.7799 - val_loss: 0.5307 - val_acc: 0.7500\n",
      "Epoch 116/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4893 - acc: 0.7448 - val_loss: 0.5047 - val_acc: 0.7344\n",
      "Epoch 117/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4838 - acc: 0.7708 - val_loss: 0.5049 - val_acc: 0.7448\n",
      "Epoch 118/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4797 - acc: 0.7643 - val_loss: 0.5118 - val_acc: 0.7760\n",
      "Epoch 119/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4957 - acc: 0.7695 - val_loss: 0.5067 - val_acc: 0.7135\n",
      "Epoch 120/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4787 - acc: 0.7747 - val_loss: 0.5133 - val_acc: 0.6979\n",
      "Epoch 121/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4895 - acc: 0.7669 - val_loss: 0.5032 - val_acc: 0.7396\n",
      "Epoch 122/200\n",
      "768/768 [==============================] - 0s 79us/step - loss: 0.4775 - acc: 0.7786 - val_loss: 0.5073 - val_acc: 0.7656\n",
      "Epoch 123/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.4797 - acc: 0.7695 - val_loss: 0.5038 - val_acc: 0.7188\n",
      "Epoch 124/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4821 - acc: 0.7604 - val_loss: 0.5139 - val_acc: 0.7656\n",
      "Epoch 125/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4776 - acc: 0.7786 - val_loss: 0.5016 - val_acc: 0.7500\n",
      "Epoch 126/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4808 - acc: 0.7695 - val_loss: 0.5124 - val_acc: 0.7760\n",
      "Epoch 127/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4819 - acc: 0.7812 - val_loss: 0.5068 - val_acc: 0.7656\n",
      "Epoch 128/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4827 - acc: 0.7747 - val_loss: 0.5011 - val_acc: 0.7448\n",
      "Epoch 129/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4775 - acc: 0.7721 - val_loss: 0.5021 - val_acc: 0.7448\n",
      "Epoch 130/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4788 - acc: 0.7617 - val_loss: 0.5041 - val_acc: 0.7604\n",
      "Epoch 131/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4808 - acc: 0.7812 - val_loss: 0.5001 - val_acc: 0.7500\n",
      "Epoch 132/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4748 - acc: 0.7747 - val_loss: 0.5018 - val_acc: 0.7292\n",
      "Epoch 133/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.4763 - acc: 0.7812 - val_loss: 0.5000 - val_acc: 0.7396\n",
      "Epoch 134/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4727 - acc: 0.7656 - val_loss: 0.5007 - val_acc: 0.7396\n",
      "Epoch 135/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4915 - acc: 0.7552 - val_loss: 0.5297 - val_acc: 0.6823\n",
      "Epoch 136/200\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.4740 - acc: 0.7721 - val_loss: 0.5002 - val_acc: 0.7448\n",
      "Epoch 137/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4794 - acc: 0.7656 - val_loss: 0.5100 - val_acc: 0.7812\n",
      "Epoch 138/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4708 - acc: 0.7708 - val_loss: 0.5004 - val_acc: 0.7448\n",
      "Epoch 139/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4630 - acc: 0.7812 - val_loss: 0.5019 - val_acc: 0.7552\n",
      "Epoch 140/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4690 - acc: 0.7786 - val_loss: 0.5012 - val_acc: 0.7552\n",
      "Epoch 141/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4767 - acc: 0.7734 - val_loss: 0.5037 - val_acc: 0.7292\n",
      "Epoch 142/200\n",
      "768/768 [==============================] - 0s 96us/step - loss: 0.4695 - acc: 0.7865 - val_loss: 0.4986 - val_acc: 0.7396\n",
      "Epoch 143/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4689 - acc: 0.7747 - val_loss: 0.5074 - val_acc: 0.7708\n",
      "Epoch 144/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4654 - acc: 0.7773 - val_loss: 0.5013 - val_acc: 0.7552\n",
      "Epoch 145/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.4652 - acc: 0.7773 - val_loss: 0.5127 - val_acc: 0.6927\n",
      "Epoch 146/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.4645 - acc: 0.7943 - val_loss: 0.4974 - val_acc: 0.7708\n",
      "Epoch 147/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4601 - acc: 0.7839 - val_loss: 0.5039 - val_acc: 0.7656\n",
      "Epoch 148/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4667 - acc: 0.7865 - val_loss: 0.4970 - val_acc: 0.7656\n",
      "Epoch 149/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4693 - acc: 0.7799 - val_loss: 0.4981 - val_acc: 0.7448\n",
      "Epoch 150/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4669 - acc: 0.7721 - val_loss: 0.5477 - val_acc: 0.6823\n",
      "Epoch 151/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4704 - acc: 0.7617 - val_loss: 0.5024 - val_acc: 0.7240\n",
      "Epoch 152/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4637 - acc: 0.7760 - val_loss: 0.5043 - val_acc: 0.7708\n",
      "Epoch 153/200\n",
      "768/768 [==============================] - 0s 77us/step - loss: 0.4724 - acc: 0.7760 - val_loss: 0.5225 - val_acc: 0.7552\n",
      "Epoch 154/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4615 - acc: 0.7760 - val_loss: 0.4962 - val_acc: 0.7656\n",
      "Epoch 155/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4616 - acc: 0.7904 - val_loss: 0.4966 - val_acc: 0.7656\n",
      "Epoch 156/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4670 - acc: 0.7865 - val_loss: 0.5009 - val_acc: 0.7240\n",
      "Epoch 157/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4580 - acc: 0.7878 - val_loss: 0.5010 - val_acc: 0.7604\n",
      "Epoch 158/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4623 - acc: 0.7891 - val_loss: 0.4968 - val_acc: 0.7500\n",
      "Epoch 159/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4731 - acc: 0.7669 - val_loss: 0.4972 - val_acc: 0.7656\n",
      "Epoch 160/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.4682 - acc: 0.7852 - val_loss: 0.4955 - val_acc: 0.7656\n",
      "Epoch 161/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.4626 - acc: 0.7878 - val_loss: 0.5097 - val_acc: 0.7604\n",
      "Epoch 162/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.4597 - acc: 0.7799 - val_loss: 0.4956 - val_acc: 0.7656\n",
      "Epoch 163/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.4618 - acc: 0.7865 - val_loss: 0.4963 - val_acc: 0.7604\n",
      "Epoch 164/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4654 - acc: 0.7708 - val_loss: 0.4947 - val_acc: 0.7708\n",
      "Epoch 165/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.4551 - acc: 0.7839 - val_loss: 0.4949 - val_acc: 0.7656\n",
      "Epoch 166/200\n",
      "768/768 [==============================] - 0s 79us/step - loss: 0.4540 - acc: 0.7930 - val_loss: 0.5027 - val_acc: 0.7188\n",
      "Epoch 167/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4718 - acc: 0.7891 - val_loss: 0.4949 - val_acc: 0.7708\n",
      "Epoch 168/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4683 - acc: 0.7812 - val_loss: 0.5093 - val_acc: 0.7604\n",
      "Epoch 169/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4574 - acc: 0.7956 - val_loss: 0.5018 - val_acc: 0.7188\n",
      "Epoch 170/200\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.4634 - acc: 0.7773 - val_loss: 0.5150 - val_acc: 0.7031\n",
      "Epoch 171/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.4585 - acc: 0.7956 - val_loss: 0.4958 - val_acc: 0.7656\n",
      "Epoch 172/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.4664 - acc: 0.767 - 0s 85us/step - loss: 0.4674 - acc: 0.7695 - val_loss: 0.5241 - val_acc: 0.7604\n",
      "Epoch 173/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4687 - acc: 0.7682 - val_loss: 0.4952 - val_acc: 0.7708\n",
      "Epoch 174/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.4534 - acc: 0.7812 - val_loss: 0.5023 - val_acc: 0.7188\n",
      "Epoch 175/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4649 - acc: 0.7734 - val_loss: 0.4946 - val_acc: 0.7604\n",
      "Epoch 176/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4523 - acc: 0.7891 - val_loss: 0.4952 - val_acc: 0.7708\n",
      "Epoch 177/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.4655 - acc: 0.7682 - val_loss: 0.5111 - val_acc: 0.7604\n",
      "Epoch 178/200\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.4663 - acc: 0.7669 - val_loss: 0.4946 - val_acc: 0.7708\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 88us/step - loss: 0.4536 - acc: 0.7839 - val_loss: 0.4944 - val_acc: 0.7604\n",
      "Epoch 180/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4529 - acc: 0.7865 - val_loss: 0.5107 - val_acc: 0.7604\n",
      "Epoch 181/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4562 - acc: 0.7773 - val_loss: 0.5081 - val_acc: 0.7083\n",
      "Epoch 182/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.4691 - acc: 0.7773 - val_loss: 0.5682 - val_acc: 0.6927\n",
      "Epoch 183/200\n",
      "768/768 [==============================] - 0s 80us/step - loss: 0.4866 - acc: 0.7409 - val_loss: 0.4941 - val_acc: 0.7708\n",
      "Epoch 184/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4632 - acc: 0.7852 - val_loss: 0.4986 - val_acc: 0.7760\n",
      "Epoch 185/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4524 - acc: 0.7891 - val_loss: 0.4965 - val_acc: 0.7500\n",
      "Epoch 186/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4621 - acc: 0.7826 - val_loss: 0.4946 - val_acc: 0.7604\n",
      "Epoch 187/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4546 - acc: 0.7786 - val_loss: 0.4932 - val_acc: 0.7708\n",
      "Epoch 188/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4472 - acc: 0.7865 - val_loss: 0.4938 - val_acc: 0.7656\n",
      "Epoch 189/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4566 - acc: 0.7799 - val_loss: 0.4939 - val_acc: 0.7708\n",
      "Epoch 190/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4640 - acc: 0.7826 - val_loss: 0.4946 - val_acc: 0.7604\n",
      "Epoch 191/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.4528 - acc: 0.7943 - val_loss: 0.4932 - val_acc: 0.7708\n",
      "Epoch 192/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4535 - acc: 0.7956 - val_loss: 0.4980 - val_acc: 0.7240\n",
      "Epoch 193/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4566 - acc: 0.7839 - val_loss: 0.4932 - val_acc: 0.7656\n",
      "Epoch 194/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4541 - acc: 0.7956 - val_loss: 0.5007 - val_acc: 0.7708\n",
      "Epoch 195/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4516 - acc: 0.7891 - val_loss: 0.4954 - val_acc: 0.7760\n",
      "Epoch 196/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.4513 - acc: 0.7812 - val_loss: 0.5011 - val_acc: 0.7656\n",
      "Epoch 197/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4580 - acc: 0.7852 - val_loss: 0.4957 - val_acc: 0.7396\n",
      "Epoch 198/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4580 - acc: 0.7865 - val_loss: 0.4936 - val_acc: 0.7656\n",
      "Epoch 199/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4549 - acc: 0.7760 - val_loss: 0.4944 - val_acc: 0.7656\n",
      "Epoch 200/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4466 - acc: 0.7786 - val_loss: 0.4988 - val_acc: 0.7240\n",
      "Predicting...\n",
      "Cross val iteration 5 of 5\n",
      "Fitting...\n",
      "Train on 768 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "768/768 [==============================] - 0s 486us/step - loss: 0.6857 - acc: 0.6315 - val_loss: 0.6838 - val_acc: 0.5833\n",
      "Epoch 2/200\n",
      "768/768 [==============================] - 0s 80us/step - loss: 0.6693 - acc: 0.6354 - val_loss: 0.6780 - val_acc: 0.5833\n",
      "Epoch 3/200\n",
      "768/768 [==============================] - 0s 75us/step - loss: 0.6576 - acc: 0.6354 - val_loss: 0.6820 - val_acc: 0.5833\n",
      "Epoch 4/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.6560 - acc: 0.6354 - val_loss: 0.6824 - val_acc: 0.5833\n",
      "Epoch 5/200\n",
      "768/768 [==============================] - 0s 94us/step - loss: 0.6543 - acc: 0.6354 - val_loss: 0.6817 - val_acc: 0.5833\n",
      "Epoch 6/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.6547 - acc: 0.6354 - val_loss: 0.6823 - val_acc: 0.5833\n",
      "Epoch 7/200\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.6559 - acc: 0.6354 - val_loss: 0.6806 - val_acc: 0.5833\n",
      "Epoch 8/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6507 - acc: 0.6354 - val_loss: 0.6834 - val_acc: 0.5833\n",
      "Epoch 9/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6537 - acc: 0.6354 - val_loss: 0.6811 - val_acc: 0.5833\n",
      "Epoch 10/200\n",
      "768/768 [==============================] - 0s 94us/step - loss: 0.6509 - acc: 0.6354 - val_loss: 0.6810 - val_acc: 0.5833\n",
      "Epoch 11/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6506 - acc: 0.6354 - val_loss: 0.6815 - val_acc: 0.5833\n",
      "Epoch 12/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.6520 - acc: 0.6354 - val_loss: 0.6797 - val_acc: 0.5833\n",
      "Epoch 13/200\n",
      "768/768 [==============================] - 0s 95us/step - loss: 0.6505 - acc: 0.6354 - val_loss: 0.6810 - val_acc: 0.5833\n",
      "Epoch 14/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.6550 - acc: 0.6354 - val_loss: 0.6804 - val_acc: 0.5833\n",
      "Epoch 15/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6518 - acc: 0.6354 - val_loss: 0.6759 - val_acc: 0.5833\n",
      "Epoch 16/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6479 - acc: 0.6354 - val_loss: 0.6786 - val_acc: 0.5833\n",
      "Epoch 17/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6489 - acc: 0.6354 - val_loss: 0.6773 - val_acc: 0.5833\n",
      "Epoch 18/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6477 - acc: 0.6354 - val_loss: 0.6769 - val_acc: 0.5833\n",
      "Epoch 19/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.6488 - acc: 0.6354 - val_loss: 0.6741 - val_acc: 0.5833\n",
      "Epoch 20/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6485 - acc: 0.6354 - val_loss: 0.6735 - val_acc: 0.5833\n",
      "Epoch 21/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6487 - acc: 0.6354 - val_loss: 0.6730 - val_acc: 0.5833\n",
      "Epoch 22/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.6461 - acc: 0.6354 - val_loss: 0.6716 - val_acc: 0.5833\n",
      "Epoch 23/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6439 - acc: 0.6354 - val_loss: 0.6703 - val_acc: 0.5833\n",
      "Epoch 24/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6448 - acc: 0.6354 - val_loss: 0.6687 - val_acc: 0.5833\n",
      "Epoch 25/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.6429 - acc: 0.6354 - val_loss: 0.6632 - val_acc: 0.5833\n",
      "Epoch 26/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.6376 - acc: 0.6354 - val_loss: 0.6621 - val_acc: 0.5833\n",
      "Epoch 27/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.6399 - acc: 0.6354 - val_loss: 0.6588 - val_acc: 0.5833\n",
      "Epoch 28/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6350 - acc: 0.6354 - val_loss: 0.6533 - val_acc: 0.5833\n",
      "Epoch 29/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.6333 - acc: 0.6354 - val_loss: 0.6551 - val_acc: 0.5833\n",
      "Epoch 30/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.6227 - acc: 0.6354 - val_loss: 0.6426 - val_acc: 0.5833\n",
      "Epoch 31/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.6212 - acc: 0.6354 - val_loss: 0.6442 - val_acc: 0.5833\n",
      "Epoch 32/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.6151 - acc: 0.6354 - val_loss: 0.6302 - val_acc: 0.5833\n",
      "Epoch 33/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.6130 - acc: 0.6354 - val_loss: 0.6254 - val_acc: 0.5833\n",
      "Epoch 34/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.6020 - acc: 0.6354 - val_loss: 0.6213 - val_acc: 0.5833\n",
      "Epoch 35/200\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.5995 - acc: 0.6354 - val_loss: 0.6099 - val_acc: 0.5833\n",
      "Epoch 36/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5904 - acc: 0.6393 - val_loss: 0.6012 - val_acc: 0.5833\n",
      "Epoch 37/200\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.5785 - acc: 0.6406 - val_loss: 0.6001 - val_acc: 0.5833\n",
      "Epoch 38/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5712 - acc: 0.6615 - val_loss: 0.5939 - val_acc: 0.5833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5729 - acc: 0.6523 - val_loss: 0.5805 - val_acc: 0.7083\n",
      "Epoch 40/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5642 - acc: 0.6875 - val_loss: 0.5751 - val_acc: 0.7188\n",
      "Epoch 41/200\n",
      "768/768 [==============================] - 0s 94us/step - loss: 0.5581 - acc: 0.6979 - val_loss: 0.5743 - val_acc: 0.5990\n",
      "Epoch 42/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.5509 - acc: 0.6810 - val_loss: 0.5639 - val_acc: 0.7135\n",
      "Epoch 43/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.5520 - acc: 0.7083 - val_loss: 0.5605 - val_acc: 0.7083\n",
      "Epoch 44/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.5490 - acc: 0.7070 - val_loss: 0.5702 - val_acc: 0.6042\n",
      "Epoch 45/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5452 - acc: 0.6966 - val_loss: 0.5598 - val_acc: 0.6562\n",
      "Epoch 46/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.5447 - acc: 0.7057 - val_loss: 0.5554 - val_acc: 0.7031\n",
      "Epoch 47/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5384 - acc: 0.7305 - val_loss: 0.5642 - val_acc: 0.6406\n",
      "Epoch 48/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5426 - acc: 0.7292 - val_loss: 0.5473 - val_acc: 0.7188\n",
      "Epoch 49/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5448 - acc: 0.7018 - val_loss: 0.5654 - val_acc: 0.6875\n",
      "Epoch 50/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5523 - acc: 0.7083 - val_loss: 0.5574 - val_acc: 0.6771\n",
      "Epoch 51/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5452 - acc: 0.7122 - val_loss: 0.5507 - val_acc: 0.7083\n",
      "Epoch 52/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.5417 - acc: 0.6979 - val_loss: 0.5539 - val_acc: 0.6927\n",
      "Epoch 53/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5352 - acc: 0.7005 - val_loss: 0.5435 - val_acc: 0.7188\n",
      "Epoch 54/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5325 - acc: 0.7266 - val_loss: 0.5421 - val_acc: 0.7188\n",
      "Epoch 55/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.5355 - acc: 0.7227 - val_loss: 0.5424 - val_acc: 0.6979\n",
      "Epoch 56/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5254 - acc: 0.7279 - val_loss: 0.5445 - val_acc: 0.7344\n",
      "Epoch 57/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5267 - acc: 0.7266 - val_loss: 0.5503 - val_acc: 0.6823\n",
      "Epoch 58/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5395 - acc: 0.7188 - val_loss: 0.5367 - val_acc: 0.7135\n",
      "Epoch 59/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.5428 - acc: 0.7201 - val_loss: 0.5356 - val_acc: 0.7135\n",
      "Epoch 60/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5284 - acc: 0.7305 - val_loss: 0.5441 - val_acc: 0.6719\n",
      "Epoch 61/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5237 - acc: 0.7552 - val_loss: 0.5344 - val_acc: 0.7344\n",
      "Epoch 62/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5252 - acc: 0.7409 - val_loss: 0.5344 - val_acc: 0.7396\n",
      "Epoch 63/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5337 - acc: 0.7188 - val_loss: 0.5690 - val_acc: 0.6510\n",
      "Epoch 64/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5444 - acc: 0.7240 - val_loss: 0.5334 - val_acc: 0.7083\n",
      "Epoch 65/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5208 - acc: 0.7266 - val_loss: 0.5321 - val_acc: 0.7135\n",
      "Epoch 66/200\n",
      "768/768 [==============================] - 0s 73us/step - loss: 0.5392 - acc: 0.7318 - val_loss: 0.5394 - val_acc: 0.7396\n",
      "Epoch 67/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5238 - acc: 0.7305 - val_loss: 0.5335 - val_acc: 0.7083\n",
      "Epoch 68/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.5151 - acc: 0.7487 - val_loss: 0.5344 - val_acc: 0.7552\n",
      "Epoch 69/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.5280 - acc: 0.7344 - val_loss: 0.5336 - val_acc: 0.7552\n",
      "Epoch 70/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5226 - acc: 0.7214 - val_loss: 0.5289 - val_acc: 0.7344\n",
      "Epoch 71/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5246 - acc: 0.7344 - val_loss: 0.5294 - val_acc: 0.7031\n",
      "Epoch 72/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5195 - acc: 0.7461 - val_loss: 0.5373 - val_acc: 0.6771\n",
      "Epoch 73/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5213 - acc: 0.7292 - val_loss: 0.5298 - val_acc: 0.7448\n",
      "Epoch 74/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5183 - acc: 0.7461 - val_loss: 0.5274 - val_acc: 0.7396\n",
      "Epoch 75/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.5118 - acc: 0.7396 - val_loss: 0.5298 - val_acc: 0.7396\n",
      "Epoch 76/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5222 - acc: 0.7370 - val_loss: 0.5270 - val_acc: 0.6927\n",
      "Epoch 77/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5188 - acc: 0.7279 - val_loss: 0.5279 - val_acc: 0.6771\n",
      "Epoch 78/200\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.5137 - acc: 0.7435 - val_loss: 0.5282 - val_acc: 0.7448\n",
      "Epoch 79/200\n",
      "768/768 [==============================] - 0s 94us/step - loss: 0.5181 - acc: 0.7513 - val_loss: 0.5241 - val_acc: 0.7396\n",
      "Epoch 80/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5239 - acc: 0.7266 - val_loss: 0.5239 - val_acc: 0.7240\n",
      "Epoch 81/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5233 - acc: 0.7240 - val_loss: 0.5267 - val_acc: 0.7396\n",
      "Epoch 82/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5088 - acc: 0.7513 - val_loss: 0.5241 - val_acc: 0.7396\n",
      "Epoch 83/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5133 - acc: 0.7344 - val_loss: 0.5225 - val_acc: 0.7552\n",
      "Epoch 84/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5129 - acc: 0.7409 - val_loss: 0.5223 - val_acc: 0.7083\n",
      "Epoch 85/200\n",
      "768/768 [==============================] - 0s 94us/step - loss: 0.5252 - acc: 0.7357 - val_loss: 0.5241 - val_acc: 0.6771\n",
      "Epoch 86/200\n",
      "768/768 [==============================] - 0s 94us/step - loss: 0.5083 - acc: 0.7552 - val_loss: 0.5265 - val_acc: 0.7448\n",
      "Epoch 87/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5121 - acc: 0.7500 - val_loss: 0.5413 - val_acc: 0.6823\n",
      "Epoch 88/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5161 - acc: 0.7331 - val_loss: 0.5219 - val_acc: 0.6875\n",
      "Epoch 89/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5092 - acc: 0.7487 - val_loss: 0.5224 - val_acc: 0.7500\n",
      "Epoch 90/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.5075 - acc: 0.7357 - val_loss: 0.5208 - val_acc: 0.6875\n",
      "Epoch 91/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5128 - acc: 0.7383 - val_loss: 0.5233 - val_acc: 0.7552\n",
      "Epoch 92/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5119 - acc: 0.7448 - val_loss: 0.5215 - val_acc: 0.7500\n",
      "Epoch 93/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5119 - acc: 0.7526 - val_loss: 0.5189 - val_acc: 0.7552\n",
      "Epoch 94/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.5107 - acc: 0.7526 - val_loss: 0.5182 - val_acc: 0.7448\n",
      "Epoch 95/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.5075 - acc: 0.737 - 0s 87us/step - loss: 0.5173 - acc: 0.7357 - val_loss: 0.5255 - val_acc: 0.7500\n",
      "Epoch 96/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5061 - acc: 0.7422 - val_loss: 0.5191 - val_acc: 0.6875\n",
      "Epoch 97/200\n",
      "768/768 [==============================] - 0s 80us/step - loss: 0.5047 - acc: 0.7526 - val_loss: 0.5174 - val_acc: 0.7448\n",
      "Epoch 98/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5014 - acc: 0.7578 - val_loss: 0.5198 - val_acc: 0.6927\n",
      "Epoch 99/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5049 - acc: 0.7708 - val_loss: 0.5167 - val_acc: 0.7448\n",
      "Epoch 100/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5094 - acc: 0.7357 - val_loss: 0.5300 - val_acc: 0.7604\n",
      "Epoch 101/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.5141 - acc: 0.7318 - val_loss: 0.5164 - val_acc: 0.7344\n",
      "Epoch 102/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.5058 - acc: 0.7591 - val_loss: 0.5207 - val_acc: 0.6875\n",
      "Epoch 103/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4974 - acc: 0.7526 - val_loss: 0.5154 - val_acc: 0.7396\n",
      "Epoch 104/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5043 - acc: 0.7552 - val_loss: 0.5159 - val_acc: 0.7500\n",
      "Epoch 105/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5022 - acc: 0.7643 - val_loss: 0.5162 - val_acc: 0.7500\n",
      "Epoch 106/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.5089 - acc: 0.7526 - val_loss: 0.5159 - val_acc: 0.6979\n",
      "Epoch 107/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5062 - acc: 0.7357 - val_loss: 0.5141 - val_acc: 0.7344\n",
      "Epoch 108/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.5021 - acc: 0.7591 - val_loss: 0.5138 - val_acc: 0.7344\n",
      "Epoch 109/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5099 - acc: 0.7500 - val_loss: 0.5136 - val_acc: 0.7552\n",
      "Epoch 110/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4936 - acc: 0.7591 - val_loss: 0.5252 - val_acc: 0.6875\n",
      "Epoch 111/200\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.5051 - acc: 0.7578 - val_loss: 0.5176 - val_acc: 0.6927\n",
      "Epoch 112/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4971 - acc: 0.7461 - val_loss: 0.5143 - val_acc: 0.7031\n",
      "Epoch 113/200\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.5007 - acc: 0.7656 - val_loss: 0.5121 - val_acc: 0.7448\n",
      "Epoch 114/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4909 - acc: 0.7539 - val_loss: 0.5206 - val_acc: 0.7552\n",
      "Epoch 115/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.5068 - acc: 0.7552 - val_loss: 0.5187 - val_acc: 0.7604\n",
      "Epoch 116/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.4899 - acc: 0.7591 - val_loss: 0.5219 - val_acc: 0.6927\n",
      "Epoch 117/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.5117 - acc: 0.7513 - val_loss: 0.5182 - val_acc: 0.6875\n",
      "Epoch 118/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5100 - acc: 0.7539 - val_loss: 0.5134 - val_acc: 0.7552\n",
      "Epoch 119/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4908 - acc: 0.7630 - val_loss: 0.5106 - val_acc: 0.7448\n",
      "Epoch 120/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4913 - acc: 0.7539 - val_loss: 0.5124 - val_acc: 0.7604\n",
      "Epoch 121/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5064 - acc: 0.7578 - val_loss: 0.5103 - val_acc: 0.7396\n",
      "Epoch 122/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.5001 - acc: 0.7552 - val_loss: 0.5151 - val_acc: 0.7708\n",
      "Epoch 123/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.5007 - acc: 0.7474 - val_loss: 0.5161 - val_acc: 0.7656\n",
      "Epoch 124/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.5014 - acc: 0.7565 - val_loss: 0.5103 - val_acc: 0.7240\n",
      "Epoch 125/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4876 - acc: 0.7643 - val_loss: 0.5100 - val_acc: 0.7396\n",
      "Epoch 126/200\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.4892 - acc: 0.7565 - val_loss: 0.5082 - val_acc: 0.7396\n",
      "Epoch 127/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.4962 - acc: 0.7604 - val_loss: 0.5079 - val_acc: 0.7396\n",
      "Epoch 128/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4936 - acc: 0.7630 - val_loss: 0.5076 - val_acc: 0.7344\n",
      "Epoch 129/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4837 - acc: 0.7786 - val_loss: 0.5074 - val_acc: 0.7448\n",
      "Epoch 130/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4851 - acc: 0.7630 - val_loss: 0.5072 - val_acc: 0.7396\n",
      "Epoch 131/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4875 - acc: 0.7734 - val_loss: 0.5068 - val_acc: 0.7344\n",
      "Epoch 132/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4837 - acc: 0.7682 - val_loss: 0.5103 - val_acc: 0.7656\n",
      "Epoch 133/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4951 - acc: 0.7578 - val_loss: 0.5100 - val_acc: 0.7604\n",
      "Epoch 134/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4863 - acc: 0.7799 - val_loss: 0.5072 - val_acc: 0.7448\n",
      "Epoch 135/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4847 - acc: 0.7695 - val_loss: 0.5075 - val_acc: 0.7552\n",
      "Epoch 136/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4892 - acc: 0.7435 - val_loss: 0.5117 - val_acc: 0.7031\n",
      "Epoch 137/200\n",
      "768/768 [==============================] - 0s 95us/step - loss: 0.4918 - acc: 0.7682 - val_loss: 0.5053 - val_acc: 0.7396\n",
      "Epoch 138/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4987 - acc: 0.7617 - val_loss: 0.5300 - val_acc: 0.7500\n",
      "Epoch 139/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.5208 - acc: 0.7396 - val_loss: 0.5704 - val_acc: 0.6667\n",
      "Epoch 140/200\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.4938 - acc: 0.7461 - val_loss: 0.5051 - val_acc: 0.7396\n",
      "Epoch 141/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4846 - acc: 0.7591 - val_loss: 0.5045 - val_acc: 0.7396\n",
      "Epoch 142/200\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.4794 - acc: 0.7917 - val_loss: 0.5040 - val_acc: 0.7396\n",
      "Epoch 143/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4940 - acc: 0.7682 - val_loss: 0.5078 - val_acc: 0.7344\n",
      "Epoch 144/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4725 - acc: 0.7682 - val_loss: 0.5036 - val_acc: 0.7396\n",
      "Epoch 145/200\n",
      "768/768 [==============================] - 0s 82us/step - loss: 0.4811 - acc: 0.7773 - val_loss: 0.5031 - val_acc: 0.7396\n",
      "Epoch 146/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4822 - acc: 0.7604 - val_loss: 0.5118 - val_acc: 0.7031\n",
      "Epoch 147/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4772 - acc: 0.7747 - val_loss: 0.5048 - val_acc: 0.7344\n",
      "Epoch 148/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4913 - acc: 0.7695 - val_loss: 0.5022 - val_acc: 0.7448\n",
      "Epoch 149/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4811 - acc: 0.7734 - val_loss: 0.5025 - val_acc: 0.7448\n",
      "Epoch 150/200\n",
      "768/768 [==============================] - 0s 78us/step - loss: 0.4779 - acc: 0.7865 - val_loss: 0.5020 - val_acc: 0.7500\n",
      "Epoch 151/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4766 - acc: 0.7617 - val_loss: 0.5014 - val_acc: 0.7500\n",
      "Epoch 152/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4760 - acc: 0.7773 - val_loss: 0.5068 - val_acc: 0.7188\n",
      "Epoch 153/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4810 - acc: 0.7708 - val_loss: 0.5102 - val_acc: 0.7083\n",
      "Epoch 154/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4717 - acc: 0.7669 - val_loss: 0.5031 - val_acc: 0.7500\n",
      "Epoch 155/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4751 - acc: 0.7786 - val_loss: 0.5041 - val_acc: 0.7552\n",
      "Epoch 156/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.4757 - acc: 0.7747 - val_loss: 0.5003 - val_acc: 0.7552\n",
      "Epoch 157/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4858 - acc: 0.7617 - val_loss: 0.5023 - val_acc: 0.7292\n",
      "Epoch 158/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4737 - acc: 0.7630 - val_loss: 0.5274 - val_acc: 0.6927\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 88us/step - loss: 0.4706 - acc: 0.7826 - val_loss: 0.4996 - val_acc: 0.7552\n",
      "Epoch 160/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4849 - acc: 0.7695 - val_loss: 0.5099 - val_acc: 0.7760\n",
      "Epoch 161/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.4758 - acc: 0.7734 - val_loss: 0.5018 - val_acc: 0.7188\n",
      "Epoch 162/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.4753 - acc: 0.7773 - val_loss: 0.5058 - val_acc: 0.7552\n",
      "Epoch 163/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4687 - acc: 0.7721 - val_loss: 0.5047 - val_acc: 0.7500\n",
      "Epoch 164/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4788 - acc: 0.7708 - val_loss: 0.5000 - val_acc: 0.7552\n",
      "Epoch 165/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4711 - acc: 0.7604 - val_loss: 0.5044 - val_acc: 0.7500\n",
      "Epoch 166/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.4838 - acc: 0.7682 - val_loss: 0.5014 - val_acc: 0.7552\n",
      "Epoch 167/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4658 - acc: 0.7839 - val_loss: 0.5017 - val_acc: 0.7552\n",
      "Epoch 168/200\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.4666 - acc: 0.7799 - val_loss: 0.4993 - val_acc: 0.7500\n",
      "Epoch 169/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4763 - acc: 0.7865 - val_loss: 0.4980 - val_acc: 0.7604\n",
      "Epoch 170/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.4803 - acc: 0.7656 - val_loss: 0.5094 - val_acc: 0.7708\n",
      "Epoch 171/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.4807 - acc: 0.7604 - val_loss: 0.5177 - val_acc: 0.6979\n",
      "Epoch 172/200\n",
      "768/768 [==============================] - 0s 79us/step - loss: 0.4822 - acc: 0.7591 - val_loss: 0.5005 - val_acc: 0.7240\n",
      "Epoch 173/200\n",
      "768/768 [==============================] - 0s 81us/step - loss: 0.4705 - acc: 0.7630 - val_loss: 0.5055 - val_acc: 0.7604\n",
      "Epoch 174/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4690 - acc: 0.7695 - val_loss: 0.5024 - val_acc: 0.7240\n",
      "Epoch 175/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4665 - acc: 0.7773 - val_loss: 0.5046 - val_acc: 0.7552\n",
      "Epoch 176/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4757 - acc: 0.7682 - val_loss: 0.4974 - val_acc: 0.7656\n",
      "Epoch 177/200\n",
      "768/768 [==============================] - 0s 89us/step - loss: 0.4702 - acc: 0.7617 - val_loss: 0.4970 - val_acc: 0.7708\n",
      "Epoch 178/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.4652 - acc: 0.7865 - val_loss: 0.4968 - val_acc: 0.7708\n",
      "Epoch 179/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4658 - acc: 0.7682 - val_loss: 0.5015 - val_acc: 0.7240\n",
      "Epoch 180/200\n",
      "768/768 [==============================] - 0s 76us/step - loss: 0.4717 - acc: 0.7695 - val_loss: 0.4979 - val_acc: 0.7604\n",
      "Epoch 181/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.4794 - acc: 0.773 - 0s 91us/step - loss: 0.4727 - acc: 0.7786 - val_loss: 0.4974 - val_acc: 0.7448\n",
      "Epoch 182/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4689 - acc: 0.7708 - val_loss: 0.4983 - val_acc: 0.7604\n",
      "Epoch 183/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4758 - acc: 0.7695 - val_loss: 0.5344 - val_acc: 0.6979\n",
      "Epoch 184/200\n",
      "768/768 [==============================] - 0s 91us/step - loss: 0.4671 - acc: 0.7786 - val_loss: 0.4966 - val_acc: 0.7656\n",
      "Epoch 185/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4825 - acc: 0.7708 - val_loss: 0.4963 - val_acc: 0.7708\n",
      "Epoch 186/200\n",
      "768/768 [==============================] - 0s 92us/step - loss: 0.4579 - acc: 0.7956 - val_loss: 0.5031 - val_acc: 0.7135\n",
      "Epoch 187/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4784 - acc: 0.7630 - val_loss: 0.4960 - val_acc: 0.7708\n",
      "Epoch 188/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4734 - acc: 0.7747 - val_loss: 0.5008 - val_acc: 0.7552\n",
      "Epoch 189/200\n",
      "768/768 [==============================] - 0s 84us/step - loss: 0.4652 - acc: 0.7734 - val_loss: 0.4978 - val_acc: 0.7656\n",
      "Epoch 190/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4690 - acc: 0.7760 - val_loss: 0.5027 - val_acc: 0.7135\n",
      "Epoch 191/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.4630 - acc: 0.7878 - val_loss: 0.5049 - val_acc: 0.7083\n",
      "Epoch 192/200\n",
      "768/768 [==============================] - 0s 86us/step - loss: 0.4650 - acc: 0.7852 - val_loss: 0.5240 - val_acc: 0.7083\n",
      "Epoch 193/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4696 - acc: 0.7773 - val_loss: 0.4952 - val_acc: 0.7708\n",
      "Epoch 194/200\n",
      "768/768 [==============================] - 0s 87us/step - loss: 0.4673 - acc: 0.7839 - val_loss: 0.5058 - val_acc: 0.7656\n",
      "Epoch 195/200\n",
      "768/768 [==============================] - 0s 83us/step - loss: 0.4627 - acc: 0.7799 - val_loss: 0.5044 - val_acc: 0.7604\n",
      "Epoch 196/200\n",
      "768/768 [==============================] - 0s 85us/step - loss: 0.4736 - acc: 0.7734 - val_loss: 0.4950 - val_acc: 0.7760\n",
      "Epoch 197/200\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.4668 - acc: 0.7760 - val_loss: 0.4972 - val_acc: 0.7396\n",
      "Epoch 198/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4695 - acc: 0.7839 - val_loss: 0.4951 - val_acc: 0.7656\n",
      "Epoch 199/200\n",
      "768/768 [==============================] - 0s 90us/step - loss: 0.4668 - acc: 0.7734 - val_loss: 0.4969 - val_acc: 0.7708\n",
      "Epoch 200/200\n",
      "768/768 [==============================] - 0s 88us/step - loss: 0.4604 - acc: 0.7930 - val_loss: 0.4961 - val_acc: 0.7760\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    preds, actual, History = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCAv6hmEYxxEW9A3DMI4jLOgbhmEcR/x/GSaefzlLNyYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(History.history['val_acc'])\n",
    "plt.plot(History.history['acc'])\n",
    "plt.legend(['Val Acc', 'Acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  1.],\n",
       "        [ 0.,  1.],\n",
       "        [ 0.,  1.],\n",
       "        ..., \n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[ 0.,  1.],\n",
       "        [ 0.,  1.],\n",
       "        [ 0.,  1.],\n",
       "        ..., \n",
       "        [ 0.,  1.],\n",
       "        [ 0.,  1.],\n",
       "        [ 0.,  1.]],\n",
       "\n",
       "       [[ 1.,  0.],\n",
       "        [ 0.,  1.],\n",
       "        [ 1.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  1.],\n",
       "        [ 1.,  0.],\n",
       "        [ 0.,  1.]],\n",
       "\n",
       "       [[ 1.,  0.],\n",
       "        [ 0.,  1.],\n",
       "        [ 0.,  1.],\n",
       "        ..., \n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.],\n",
       "        [ 1.,  0.]],\n",
       "\n",
       "       [[ 0.,  1.],\n",
       "        [ 0.,  1.],\n",
       "        [ 0.,  1.],\n",
       "        ..., \n",
       "        [ 0.,  1.],\n",
       "        [ 0.,  1.],\n",
       "        [ 0.,  1.]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.around(preds, decimals = 0)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[158, 122, 175, 180, 180]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totals = []\n",
    "for i in range(0, len(preds)): \n",
    "    current_pred = preds[i]\n",
    "    current_real = actual[i]\n",
    "    total = 0\n",
    "    for j in range(0, len(preds[0])):\n",
    "        if current_pred[j][0] == current_real[j][0]: \n",
    "            total += 1\n",
    "    totals.append(total)\n",
    "    \n",
    "totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6583333333333333, 0.5083333333333333, 0.7291666666666666, 0.75, 0.75]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "accs = []\n",
    "for t in totals: \n",
    "    acc = t/len(preds[i])\n",
    "    i+=1\n",
    "    accs.append(acc)\n",
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.091818056805595474"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67916666666666659"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py34]",
   "language": "python",
   "name": "conda-env-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
