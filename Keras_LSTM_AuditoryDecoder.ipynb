{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/miniconda2/envs/py34/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from lfp_extracters import *\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import initializers \n",
    "from keras.layers import Dense, Activation, SimpleRNN, Conv1D, MaxPooling1D, Flatten, Conv2D, MaxPooling2D, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading \n",
    "#### From samples 1 to 1200, randomly select 80% to be in the train data set. If the number is even, it is in the Stimulus A folder; if it is odd, it is in the Stimulus B folder. 20% of the train data is chosen for validation. The remaining 20% of data comprises the test set. Indices are generated for five fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inds(): \n",
    "    \n",
    "    numIter = 5\n",
    "    rs = ShuffleSplit(1200, n_iter=numIter, test_size=.2, random_state=0)\n",
    "    \n",
    "    A_files = glob.glob('data/data/Stimulus_A/LFP_15_300/*.csv')\n",
    "    B_files = glob.glob('data/data/Stimulus_B/LFP_15_300/*.csv')\n",
    "    files = np.array(A_files + B_files)\n",
    "    \n",
    "    train_inds = []\n",
    "    test_inds = []\n",
    "    for train, test in rs: \n",
    "        train_inds.append(train)\n",
    "        test_inds.append(test)\n",
    "        \n",
    "    train2 = []\n",
    "    val_inds = []\n",
    "    for t in train_inds: \n",
    "        s = ShuffleSplit(len(t), n_iter = 1, test_size = .2, random_state = 0)\n",
    "        for train, val in s: \n",
    "            train2.append(train)\n",
    "            val_inds.append(val)\n",
    "        \n",
    "    train_inds = train2\n",
    "        \n",
    "    return numIter, train_inds, val_inds, test_inds, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_test_val(trial_num, train_inds, test_inds, val_inds, files):\n",
    "    train_files = files[train_inds[trial_num]]\n",
    "    test_files = files[test_inds[trial_num]]\n",
    "    val_files = files[val_inds[trial_num]]\n",
    "    return train_files, test_files, val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_test_df(test_files, with_time, bin_function, threshold, bin_width):\n",
    "    channel_data = ['Ch1', 'Ch2', 'Ch3', 'Ch4', 'Ch5', 'Ch6', 'Ch7', 'Ch8', 'Ch9', 'Ch10', \\\n",
    "                   'Ch11', 'Ch12', 'Ch13', 'Ch14', 'Ch15', 'Ch16']\n",
    "    l = []\n",
    "    y_test = []\n",
    "    bin_width = int(bin_width)\n",
    "    \n",
    "    for i in range(len(test_files)):\n",
    "        n1 = np.genfromtxt(test_files[i], delimiter = ',')\n",
    "        if not with_time: \n",
    "            n1 = n1[0:16]\n",
    "        n_new = bin_function(n1, threshold, bin_width)\n",
    "        l.append(n_new.T) \n",
    "        if 'A' in test_files[i]: \n",
    "            y_test.append([0,1])\n",
    "        else: \n",
    "            y_test.append([1,0])\n",
    "\n",
    "    return np.array(l), y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_train_df(train_files, with_time, bin_function, threshold, bin_width):\n",
    "    channel_data = ['Ch1', 'Ch2', 'Ch3', 'Ch4', 'Ch5', 'Ch6', 'Ch7', 'Ch8', 'Ch9', 'Ch10', \\\n",
    "                   'Ch11', 'Ch12', 'Ch13', 'Ch14', 'Ch15', 'Ch16']\n",
    "    \n",
    "    list_ = []\n",
    "    y_train = []\n",
    "    bin_width = int(bin_width)\n",
    "    \n",
    "    for i in range(len(train_files)):\n",
    "        n1 = np.genfromtxt(train_files[i], delimiter = ',')\n",
    "        if not with_time: \n",
    "            n1 = n1[0:16]\n",
    "        n_new = bin_function(n1, threshold, bin_width)\n",
    "        list_.append(n_new.T)\n",
    "        if 'A' in train_files[i]: \n",
    "            y_train.append([0,1])\n",
    "        else: \n",
    "            y_train.append([1,0])\n",
    "\n",
    "    return np.array(list_), y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_val_df(val_files, with_time, bin_function, threshold, bin_width):\n",
    "    channel_data = ['Ch1', 'Ch2', 'Ch3', 'Ch4', 'Ch5', 'Ch6', 'Ch7', 'Ch8', 'Ch9', 'Ch10', \\\n",
    "                   'Ch11', 'Ch12', 'Ch13', 'Ch14', 'Ch15', 'Ch16']\n",
    "    \n",
    "    list_ = []\n",
    "    y_val = []\n",
    "    bin_width = int(bin_width)\n",
    "    \n",
    "    for i in range(len(val_files)):\n",
    "        n1 = np.genfromtxt(val_files[i], delimiter = ',')\n",
    "        if not with_time: \n",
    "            n1 = n1[0:16]\n",
    "        n_new = bin_function(n1, threshold, bin_width)\n",
    "        list_.append(n_new.T)\n",
    "        if 'A' in val_files[i]: \n",
    "            y_val.append([0,1])\n",
    "        else: \n",
    "            y_val.append([1,0])\n",
    "\n",
    "    return np.array(list_), y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning Metrics\n",
    "#### Threshold Crossing Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def threshold_crossing_rate(x, threshold, bin_width): \n",
    "    inds = range(0, 6001, int(bin_width))\n",
    "    hastime = len(x) == 17\n",
    "    newx = []\n",
    "    if hastime: \n",
    "        t = x[16]\n",
    "    for i in range(0, len(inds) - 1): \n",
    "        sub = x[0:16, inds[i]:inds[i+1]]\n",
    "        count_above = np.sum(abs(np.diff(sub > threshold)), axis = 1)/float(bin_width)\n",
    "        if hastime:\n",
    "            t_seg = np.reshape(t[inds[i]], (1,)) #uses time stamp at beginning of each bin\n",
    "            count_above = np.concatenate((count_above, t_seg))\n",
    "        newx.append(count_above)\n",
    "    return np.array(newx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Root Mean Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def root_mean_square(x, threshold, bin_width): \n",
    "    inds = range(0, 6001, bin_width)\n",
    "    hastime = len(x) == 17\n",
    "    newx = []\n",
    "    if hastime: \n",
    "        t = x[16]\n",
    "    for i in range(0, len(inds) - 1): \n",
    "        sub = x[0:16, inds[i]:inds[i+1]]\n",
    "        rms = np.sqrt(np.mean(sub**2, axis = 1)) \n",
    "        \n",
    "        if hastime: \n",
    "            t_seg = np.reshape(t[inds[i]], (1,))\n",
    "            rms = np.concatenate((rms, t_seg))\n",
    "        newx.append(rms)\n",
    "    return np.array(newx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma Band Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def power_gamma_band(x, threshold, bin_width): \n",
    "    inds = range(0, 6001, bin_width)\n",
    "    hastime = len(x) == 17\n",
    "    newx = [] \n",
    "    #make bandpass filter for 30-100Hz \n",
    "    fs = 6000\n",
    "    nyq = 0.5 * fs\n",
    "    cutOff_L = 30\n",
    "    cutOff_H = 100\n",
    "    low = cutOff_L / nyq\n",
    "    high = cutOff_H / nyq\n",
    "    order = 2\n",
    "    b, a = signal.butter(order, [low, high], btype='band')\n",
    "    \n",
    "    if hastime: \n",
    "        t = x[16]\n",
    "    for i in range(0, len(inds) - 1): \n",
    "        sub = x[0:16, inds[i]:inds[i+1]]\n",
    "        sub_f = signal.lfilter(b, a, sub)\n",
    "        rms = np.sqrt(np.mean(sub_f**2, axis = 1))\n",
    "        if hastime: \n",
    "            t_seg = np.reshape(t[inds[i]], (1,))\n",
    "            rms = np.concatenate((rms, t_seg))\n",
    "        newx.append(rms)\n",
    "    return np.array(newx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Change -- filler function for running on raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def no_change(x, threshold, bin_width): \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_lstm(with_time, bin_width):\n",
    "    model_lstm = Sequential()\n",
    "    \n",
    "    if with_time: \n",
    "        x = 17\n",
    "    else: \n",
    "        x = 16\n",
    "    \n",
    "#     model_lstm.add(Dropout(0.3, input_shape=(x, 6000//bin_width)))\n",
    "    model_lstm.add(LSTM(32, input_shape = (x, 6000//bin_width), return_sequences=False, recurrent_initializer = initializers.Identity(gain=1.0)))\n",
    "#     model_lstm.add(LSTM(32, return_sequences = False, recurrent_initializer = initializers.Identity(gain=1.0)))\n",
    "#     model_lstm.add(Dropout(0.2))\n",
    "    model_lstm.add(Dense(128, activation = 'relu'))\n",
    "#     model_lstm.add(Dropout(0.2))\n",
    "    model_lstm.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "    return model_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit, Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makePredictions(model, train_df, train_l, test_df, test_l, val_df, val_l, with_time):\n",
    "    a = keras.optimizers.Adam(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=a,\n",
    "                  metrics=['accuracy'])\n",
    "    print('Fitting...')\n",
    "    History = model.fit(train_df, train_l, epochs = 200, validation_data = (val_df, val_l))\n",
    "    print('Predicting...')\n",
    "    pred = model.predict(test_df)\n",
    "    return pred, History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(): \n",
    "    \n",
    "    numIter, train_inds, val_inds, test_inds, files = get_inds()    \n",
    "    threshold = 0.010 \n",
    "    bin_width = 265 \n",
    "    bin_function = power_gamma_band\n",
    "    with_time = False\n",
    "    \n",
    "    model = make_lstm(with_time, bin_width)\n",
    "    \n",
    "    preds = []\n",
    "    actual = []\n",
    "    \n",
    "    for i in range(0, numIter): \n",
    "        print('Cross val iteration ' + str(i+1) +  ' of ' + str(numIter))\n",
    "        train_files, test_files, val_files = get_train_test_val(i, train_inds, test_inds, val_inds, files)\n",
    "        \n",
    "        test_df, y_test = make_test_df(test_files, with_time, bin_function, threshold, bin_width)\n",
    "        train_df, y_train = make_train_df(train_files, with_time, bin_function, threshold, bin_width)\n",
    "        val_df, y_val = make_val_df(val_files, with_time, bin_function, threshold, bin_width)\n",
    "\n",
    "        if with_time: \n",
    "            all_times = np.concatenate((train_df[:,-1,:], test_df[:,-1,:], val_df[:,-1,:])) \n",
    "            overall_time_mean = all_times.mean()\n",
    "            overall_time_std = all_times.std() * 100\n",
    "            new_times_train = (train_df[:,-1,:]-overall_time_mean)/overall_time_std\n",
    "            new_times_test = (test_df[:,-1:]-overall_time_mean)/overall_time_std\n",
    "            new_times_val = (val_df[:,-1,:]-overall_time_mean)/overall_time_std\n",
    "            train_df[:,-1,:] = new_times_train\n",
    "            test_df[:,-1,:] = np.reshape(new_times_test, (240, 6000//bin_width))\n",
    "            val_df[:,-1,:] = new_times_val\n",
    "        \n",
    "        actual.append(y_test)\n",
    "        \n",
    "        pred, History = makePredictions(model, train_df, np.array(y_train), test_df, np.array(y_test),\\\n",
    "                                        val_df, np.array(y_val), with_time)\n",
    "\n",
    "        preds.append(pred)\n",
    "        \n",
    "    return preds, actual, History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val iteration 1 of 5\n",
      "Fitting...\n",
      "Train on 768 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "768/768 [==============================] - 6s 8ms/step - loss: 0.6655 - acc: 0.6354 - val_loss: 0.6819 - val_acc: 0.5833\n",
      "Epoch 2/200\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.6617 - acc: 0.6354 - val_loss: 0.6794 - val_acc: 0.5833\n",
      "Epoch 3/200\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.6636 - acc: 0.6354 - val_loss: 0.6801 - val_acc: 0.5833\n",
      "Epoch 4/200\n",
      "768/768 [==============================] - 0s 414us/step - loss: 0.6613 - acc: 0.6354 - val_loss: 0.6831 - val_acc: 0.5833\n",
      "Epoch 5/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.6574 - acc: 0.6354 - val_loss: 0.6829 - val_acc: 0.5833\n",
      "Epoch 6/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.6577 - acc: 0.6354 - val_loss: 0.6810 - val_acc: 0.5833\n",
      "Epoch 7/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.6565 - acc: 0.6354 - val_loss: 0.6885 - val_acc: 0.5833\n",
      "Epoch 8/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.6580 - acc: 0.6354 - val_loss: 0.6793 - val_acc: 0.5833\n",
      "Epoch 9/200\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.6612 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 10/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.6565 - acc: 0.6354 - val_loss: 0.6844 - val_acc: 0.5833\n",
      "Epoch 11/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.6566 - acc: 0.6354 - val_loss: 0.6861 - val_acc: 0.5833\n",
      "Epoch 12/200\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.6578 - acc: 0.6354 - val_loss: 0.6862 - val_acc: 0.5833\n",
      "Epoch 13/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.6572 - acc: 0.6354 - val_loss: 0.6892 - val_acc: 0.5833\n",
      "Epoch 14/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.6563 - acc: 0.6354 - val_loss: 0.6820 - val_acc: 0.5833\n",
      "Epoch 15/200\n",
      "768/768 [==============================] - 0s 415us/step - loss: 0.6574 - acc: 0.6354 - val_loss: 0.6832 - val_acc: 0.5833\n",
      "Epoch 16/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.6578 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 17/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.6571 - acc: 0.6354 - val_loss: 0.6810 - val_acc: 0.5833\n",
      "Epoch 18/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.6586 - acc: 0.6354 - val_loss: 0.6832 - val_acc: 0.5833\n",
      "Epoch 19/200\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.6578 - acc: 0.6354 - val_loss: 0.6845 - val_acc: 0.5833\n",
      "Epoch 20/200\n",
      "768/768 [==============================] - 0s 402us/step - loss: 0.6574 - acc: 0.6354 - val_loss: 0.6840 - val_acc: 0.5833\n",
      "Epoch 21/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.6564 - acc: 0.6354 - val_loss: 0.6832 - val_acc: 0.5833\n",
      "Epoch 22/200\n",
      "768/768 [==============================] - 0s 461us/step - loss: 0.6566 - acc: 0.6354 - val_loss: 0.6865 - val_acc: 0.5833\n",
      "Epoch 23/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.6570 - acc: 0.6354 - val_loss: 0.6829 - val_acc: 0.5833\n",
      "Epoch 24/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.6565 - acc: 0.6354 - val_loss: 0.6842 - val_acc: 0.5833\n",
      "Epoch 25/200\n",
      "768/768 [==============================] - 0s 474us/step - loss: 0.6562 - acc: 0.6354 - val_loss: 0.6841 - val_acc: 0.5833\n",
      "Epoch 26/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.6561 - acc: 0.6354 - val_loss: 0.6847 - val_acc: 0.5833\n",
      "Epoch 27/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.6572 - acc: 0.6354 - val_loss: 0.6837 - val_acc: 0.5833\n",
      "Epoch 28/200\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.6568 - acc: 0.6354 - val_loss: 0.6839 - val_acc: 0.5833\n",
      "Epoch 29/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.6568 - acc: 0.6354 - val_loss: 0.6847 - val_acc: 0.5833\n",
      "Epoch 30/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.6567 - acc: 0.6354 - val_loss: 0.6849 - val_acc: 0.5833\n",
      "Epoch 31/200\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.6561 - acc: 0.6354 - val_loss: 0.6831 - val_acc: 0.5833\n",
      "Epoch 32/200\n",
      "768/768 [==============================] - 0s 409us/step - loss: 0.6562 - acc: 0.6354 - val_loss: 0.6835 - val_acc: 0.5833\n",
      "Epoch 33/200\n",
      "768/768 [==============================] - 0s 414us/step - loss: 0.6564 - acc: 0.6354 - val_loss: 0.6829 - val_acc: 0.5833\n",
      "Epoch 34/200\n",
      "768/768 [==============================] - 0s 416us/step - loss: 0.6564 - acc: 0.6354 - val_loss: 0.6846 - val_acc: 0.5833\n",
      "Epoch 35/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.6572 - acc: 0.6354 - val_loss: 0.6806 - val_acc: 0.5833\n",
      "Epoch 36/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.6561 - acc: 0.6354 - val_loss: 0.6827 - val_acc: 0.5833\n",
      "Epoch 37/200\n",
      "768/768 [==============================] - 0s 413us/step - loss: 0.6560 - acc: 0.6354 - val_loss: 0.6815 - val_acc: 0.5833\n",
      "Epoch 38/200\n",
      "768/768 [==============================] - 0s 418us/step - loss: 0.6555 - acc: 0.6354 - val_loss: 0.6856 - val_acc: 0.5833\n",
      "Epoch 39/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.6550 - acc: 0.6354 - val_loss: 0.6802 - val_acc: 0.5833\n",
      "Epoch 40/200\n",
      "768/768 [==============================] - 0s 419us/step - loss: 0.6588 - acc: 0.6354 - val_loss: 0.6834 - val_acc: 0.5833\n",
      "Epoch 41/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.6561 - acc: 0.6354 - val_loss: 0.6847 - val_acc: 0.5833\n",
      "Epoch 42/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.6557 - acc: 0.6354 - val_loss: 0.6829 - val_acc: 0.5833\n",
      "Epoch 43/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.6546 - acc: 0.6354 - val_loss: 0.6766 - val_acc: 0.5833\n",
      "Epoch 44/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.6555 - acc: 0.6354 - val_loss: 0.6799 - val_acc: 0.5833\n",
      "Epoch 45/200\n",
      "768/768 [==============================] - 0s 407us/step - loss: 0.6686 - acc: 0.6224 - val_loss: 0.6784 - val_acc: 0.5833\n",
      "Epoch 46/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.6602 - acc: 0.6354 - val_loss: 0.6812 - val_acc: 0.5833\n",
      "Epoch 47/200\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.6570 - acc: 0.6354 - val_loss: 0.6832 - val_acc: 0.5833\n",
      "Epoch 48/200\n",
      "768/768 [==============================] - 0s 406us/step - loss: 0.6565 - acc: 0.6354 - val_loss: 0.6874 - val_acc: 0.5833\n",
      "Epoch 49/200\n",
      "768/768 [==============================] - 0s 455us/step - loss: 0.6561 - acc: 0.6354 - val_loss: 0.6835 - val_acc: 0.5833\n",
      "Epoch 50/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.6571 - acc: 0.6354 - val_loss: 0.6843 - val_acc: 0.5833\n",
      "Epoch 51/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.6560 - acc: 0.6354 - val_loss: 0.6878 - val_acc: 0.5833\n",
      "Epoch 52/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.6574 - acc: 0.6354 - val_loss: 0.6822 - val_acc: 0.5833\n",
      "Epoch 53/200\n",
      "768/768 [==============================] - 0s 468us/step - loss: 0.6558 - acc: 0.6354 - val_loss: 0.6841 - val_acc: 0.5833\n",
      "Epoch 54/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.6570 - acc: 0.6354 - val_loss: 0.6854 - val_acc: 0.5833\n",
      "Epoch 55/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.6561 - acc: 0.6354 - val_loss: 0.6850 - val_acc: 0.5833\n",
      "Epoch 56/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.6560 - acc: 0.6354 - val_loss: 0.6825 - val_acc: 0.5833\n",
      "Epoch 57/200\n",
      "768/768 [==============================] - 0s 473us/step - loss: 0.6557 - acc: 0.6354 - val_loss: 0.6843 - val_acc: 0.5833\n",
      "Epoch 58/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.6559 - acc: 0.6354 - val_loss: 0.6858 - val_acc: 0.5833\n",
      "Epoch 59/200\n",
      "768/768 [==============================] - 0s 425us/step - loss: 0.6551 - acc: 0.6354 - val_loss: 0.6814 - val_acc: 0.5833\n",
      "Epoch 60/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.6558 - acc: 0.6354 - val_loss: 0.6846 - val_acc: 0.5833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.6545 - acc: 0.6354 - val_loss: 0.6830 - val_acc: 0.5833\n",
      "Epoch 62/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.6540 - acc: 0.6354 - val_loss: 0.6786 - val_acc: 0.5833\n",
      "Epoch 63/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.6545 - acc: 0.6354 - val_loss: 0.6814 - val_acc: 0.5833\n",
      "Epoch 64/200\n",
      "768/768 [==============================] - 0s 421us/step - loss: 0.6553 - acc: 0.6354 - val_loss: 0.6796 - val_acc: 0.5833\n",
      "Epoch 65/200\n",
      "768/768 [==============================] - 0s 418us/step - loss: 0.6560 - acc: 0.6354 - val_loss: 0.6797 - val_acc: 0.5833\n",
      "Epoch 66/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.6500 - acc: 0.6354 - val_loss: 0.6963 - val_acc: 0.5833\n",
      "Epoch 67/200\n",
      "768/768 [==============================] - 0s 423us/step - loss: 0.6501 - acc: 0.6354 - val_loss: 0.6703 - val_acc: 0.5833\n",
      "Epoch 68/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.6477 - acc: 0.6328 - val_loss: 0.6699 - val_acc: 0.5833\n",
      "Epoch 69/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.6552 - acc: 0.6354 - val_loss: 0.6789 - val_acc: 0.5833\n",
      "Epoch 70/200\n",
      "768/768 [==============================] - 0s 408us/step - loss: 0.6500 - acc: 0.6354 - val_loss: 0.7018 - val_acc: 0.5833\n",
      "Epoch 71/200\n",
      "768/768 [==============================] - 0s 453us/step - loss: 0.6551 - acc: 0.6367 - val_loss: 0.7170 - val_acc: 0.5833\n",
      "Epoch 72/200\n",
      "768/768 [==============================] - 0s 464us/step - loss: 0.6603 - acc: 0.6354 - val_loss: 0.6738 - val_acc: 0.5833\n",
      "Epoch 73/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.6465 - acc: 0.6354 - val_loss: 0.6644 - val_acc: 0.5833\n",
      "Epoch 74/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.6334 - acc: 0.6354 - val_loss: 0.6894 - val_acc: 0.5833\n",
      "Epoch 75/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.6318 - acc: 0.6562 - val_loss: 0.6311 - val_acc: 0.5990\n",
      "Epoch 76/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.6189 - acc: 0.6745 - val_loss: 0.7660 - val_acc: 0.5833\n",
      "Epoch 77/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.6377 - acc: 0.6250 - val_loss: 0.6379 - val_acc: 0.5938\n",
      "Epoch 78/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.5963 - acc: 0.6940 - val_loss: 0.6212 - val_acc: 0.6094\n",
      "Epoch 79/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.6061 - acc: 0.6758 - val_loss: 0.6353 - val_acc: 0.6042\n",
      "Epoch 80/200\n",
      "768/768 [==============================] - 0s 517us/step - loss: 0.5983 - acc: 0.6797 - val_loss: 0.6027 - val_acc: 0.6615\n",
      "Epoch 81/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.5680 - acc: 0.7240 - val_loss: 0.6520 - val_acc: 0.6198\n",
      "Epoch 82/200\n",
      "768/768 [==============================] - 0s 421us/step - loss: 0.6407 - acc: 0.6693 - val_loss: 0.5961 - val_acc: 0.6823\n",
      "Epoch 83/200\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.6004 - acc: 0.6797 - val_loss: 0.5842 - val_acc: 0.7552\n",
      "Epoch 84/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.5794 - acc: 0.7279 - val_loss: 0.6045 - val_acc: 0.6615\n",
      "Epoch 85/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.5433 - acc: 0.7370 - val_loss: 0.5378 - val_acc: 0.7552\n",
      "Epoch 86/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.5542 - acc: 0.7448 - val_loss: 0.5534 - val_acc: 0.7396\n",
      "Epoch 87/200\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.5469 - acc: 0.7448 - val_loss: 0.5624 - val_acc: 0.7240\n",
      "Epoch 88/200\n",
      "768/768 [==============================] - 0s 394us/step - loss: 0.5472 - acc: 0.7513 - val_loss: 0.5518 - val_acc: 0.7448\n",
      "Epoch 89/200\n",
      "768/768 [==============================] - 0s 409us/step - loss: 0.5699 - acc: 0.7161 - val_loss: 0.5375 - val_acc: 0.7656\n",
      "Epoch 90/200\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.5310 - acc: 0.7617 - val_loss: 0.5506 - val_acc: 0.7292\n",
      "Epoch 91/200\n",
      "768/768 [==============================] - 0s 453us/step - loss: 0.5243 - acc: 0.7604 - val_loss: 0.5767 - val_acc: 0.7240\n",
      "Epoch 92/200\n",
      "768/768 [==============================] - 0s 469us/step - loss: 0.5194 - acc: 0.7630 - val_loss: 0.5210 - val_acc: 0.7552\n",
      "Epoch 93/200\n",
      "768/768 [==============================] - 0s 461us/step - loss: 0.5211 - acc: 0.7604 - val_loss: 0.5322 - val_acc: 0.7552\n",
      "Epoch 94/200\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.5105 - acc: 0.7721 - val_loss: 0.5208 - val_acc: 0.7760\n",
      "Epoch 95/200\n",
      "768/768 [==============================] - 0s 420us/step - loss: 0.5140 - acc: 0.7669 - val_loss: 0.6439 - val_acc: 0.7083\n",
      "Epoch 96/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.5330 - acc: 0.7513 - val_loss: 0.6024 - val_acc: 0.7188\n",
      "Epoch 97/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.5222 - acc: 0.7630 - val_loss: 0.5150 - val_acc: 0.7604\n",
      "Epoch 98/200\n",
      "768/768 [==============================] - 0s 472us/step - loss: 0.5276 - acc: 0.7539 - val_loss: 0.5154 - val_acc: 0.7552\n",
      "Epoch 99/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.5511 - acc: 0.7344 - val_loss: 0.5257 - val_acc: 0.7760\n",
      "Epoch 100/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.5528 - acc: 0.7331 - val_loss: 0.5404 - val_acc: 0.7656\n",
      "Epoch 101/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.5384 - acc: 0.7201 - val_loss: 0.5850 - val_acc: 0.7135\n",
      "Epoch 102/200\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.5168 - acc: 0.7682 - val_loss: 0.5138 - val_acc: 0.7760\n",
      "Epoch 103/200\n",
      "768/768 [==============================] - 0s 485us/step - loss: 0.5008 - acc: 0.7812 - val_loss: 0.5129 - val_acc: 0.7708\n",
      "Epoch 104/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.5008 - acc: 0.7799 - val_loss: 0.5355 - val_acc: 0.7552\n",
      "Epoch 105/200\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.5116 - acc: 0.7721 - val_loss: 0.5151 - val_acc: 0.7812\n",
      "Epoch 106/200\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.4969 - acc: 0.7799 - val_loss: 0.5244 - val_acc: 0.7708\n",
      "Epoch 107/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.5104 - acc: 0.7760 - val_loss: 0.5122 - val_acc: 0.7552\n",
      "Epoch 108/200\n",
      "768/768 [==============================] - 0s 414us/step - loss: 0.5096 - acc: 0.7565 - val_loss: 0.5285 - val_acc: 0.7552\n",
      "Epoch 109/200\n",
      "768/768 [==============================] - 0s 425us/step - loss: 0.4973 - acc: 0.7786 - val_loss: 0.5372 - val_acc: 0.7448\n",
      "Epoch 110/200\n",
      "768/768 [==============================] - 0s 467us/step - loss: 0.5045 - acc: 0.7786 - val_loss: 0.5340 - val_acc: 0.7604\n",
      "Epoch 111/200\n",
      "768/768 [==============================] - 0s 484us/step - loss: 0.5037 - acc: 0.7682 - val_loss: 0.6512 - val_acc: 0.6875\n",
      "Epoch 112/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.5284 - acc: 0.7630 - val_loss: 0.5445 - val_acc: 0.7500\n",
      "Epoch 113/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.5247 - acc: 0.7565 - val_loss: 0.5113 - val_acc: 0.7656\n",
      "Epoch 114/200\n",
      "768/768 [==============================] - 0s 478us/step - loss: 0.5044 - acc: 0.7812 - val_loss: 0.5035 - val_acc: 0.7865\n",
      "Epoch 115/200\n",
      "768/768 [==============================] - 0s 472us/step - loss: 0.5168 - acc: 0.7578 - val_loss: 0.5246 - val_acc: 0.7604\n",
      "Epoch 116/200\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.4967 - acc: 0.7878 - val_loss: 0.5012 - val_acc: 0.7812\n",
      "Epoch 117/200\n",
      "768/768 [==============================] - 0s 463us/step - loss: 0.4972 - acc: 0.7839 - val_loss: 0.4998 - val_acc: 0.7812\n",
      "Epoch 118/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.5107 - acc: 0.7799 - val_loss: 0.5120 - val_acc: 0.7708\n",
      "Epoch 119/200\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.4969 - acc: 0.7773 - val_loss: 0.4990 - val_acc: 0.7812\n",
      "Epoch 120/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.4900 - acc: 0.7865 - val_loss: 0.4969 - val_acc: 0.7917\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 445us/step - loss: 0.4893 - acc: 0.7799 - val_loss: 0.5096 - val_acc: 0.7656\n",
      "Epoch 122/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.5049 - acc: 0.7643 - val_loss: 0.5093 - val_acc: 0.7917\n",
      "Epoch 123/200\n",
      "768/768 [==============================] - 0s 469us/step - loss: 0.4912 - acc: 0.7799 - val_loss: 0.5052 - val_acc: 0.7604\n",
      "Epoch 124/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.4947 - acc: 0.7773 - val_loss: 0.4986 - val_acc: 0.7812\n",
      "Epoch 125/200\n",
      "768/768 [==============================] - 0s 453us/step - loss: 0.4855 - acc: 0.7891 - val_loss: 0.5555 - val_acc: 0.7396\n",
      "Epoch 126/200\n",
      "768/768 [==============================] - 0s 485us/step - loss: 0.4914 - acc: 0.7917 - val_loss: 0.4942 - val_acc: 0.7917\n",
      "Epoch 127/200\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.4883 - acc: 0.7786 - val_loss: 0.4937 - val_acc: 0.7708\n",
      "Epoch 128/200\n",
      "768/768 [==============================] - 0s 471us/step - loss: 0.4924 - acc: 0.7734 - val_loss: 0.4993 - val_acc: 0.7812\n",
      "Epoch 129/200\n",
      "768/768 [==============================] - 0s 473us/step - loss: 0.4881 - acc: 0.7839 - val_loss: 0.4982 - val_acc: 0.7969\n",
      "Epoch 130/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.4746 - acc: 0.7865 - val_loss: 0.4901 - val_acc: 0.7865\n",
      "Epoch 131/200\n",
      "768/768 [==============================] - 0s 415us/step - loss: 0.4892 - acc: 0.7799 - val_loss: 0.4965 - val_acc: 0.7812\n",
      "Epoch 132/200\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.4823 - acc: 0.7786 - val_loss: 0.6239 - val_acc: 0.7031\n",
      "Epoch 133/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.4917 - acc: 0.7852 - val_loss: 0.4914 - val_acc: 0.7865\n",
      "Epoch 134/200\n",
      "768/768 [==============================] - 0s 456us/step - loss: 0.4862 - acc: 0.7865 - val_loss: 0.4917 - val_acc: 0.7917\n",
      "Epoch 135/200\n",
      "768/768 [==============================] - 0s 420us/step - loss: 0.4787 - acc: 0.7943 - val_loss: 0.4890 - val_acc: 0.7812\n",
      "Epoch 136/200\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.5011 - acc: 0.7695 - val_loss: 0.4927 - val_acc: 0.7812\n",
      "Epoch 137/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.4823 - acc: 0.7904 - val_loss: 0.5030 - val_acc: 0.7865\n",
      "Epoch 138/200\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.4991 - acc: 0.7760 - val_loss: 0.4978 - val_acc: 0.7917\n",
      "Epoch 139/200\n",
      "768/768 [==============================] - 0s 425us/step - loss: 0.4768 - acc: 0.7799 - val_loss: 0.5146 - val_acc: 0.7812\n",
      "Epoch 140/200\n",
      "768/768 [==============================] - 0s 396us/step - loss: 0.5006 - acc: 0.7799 - val_loss: 0.4917 - val_acc: 0.7812\n",
      "Epoch 141/200\n",
      "768/768 [==============================] - 0s 472us/step - loss: 0.4878 - acc: 0.7760 - val_loss: 0.4920 - val_acc: 0.7865\n",
      "Epoch 142/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.4878 - acc: 0.7891 - val_loss: 0.5096 - val_acc: 0.7760\n",
      "Epoch 143/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.4857 - acc: 0.7852 - val_loss: 0.4879 - val_acc: 0.7917\n",
      "Epoch 144/200\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.4810 - acc: 0.7878 - val_loss: 0.4882 - val_acc: 0.8021\n",
      "Epoch 145/200\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.4773 - acc: 0.7930 - val_loss: 0.4883 - val_acc: 0.8021\n",
      "Epoch 146/200\n",
      "768/768 [==============================] - 0s 411us/step - loss: 0.4803 - acc: 0.7852 - val_loss: 0.4819 - val_acc: 0.7865\n",
      "Epoch 147/200\n",
      "768/768 [==============================] - 0s 456us/step - loss: 0.4727 - acc: 0.7917 - val_loss: 0.4869 - val_acc: 0.7969\n",
      "Epoch 148/200\n",
      "768/768 [==============================] - 0s 421us/step - loss: 0.4726 - acc: 0.7930 - val_loss: 0.4851 - val_acc: 0.7969\n",
      "Epoch 149/200\n",
      "768/768 [==============================] - 0s 465us/step - loss: 0.4756 - acc: 0.7865 - val_loss: 0.4881 - val_acc: 0.7969\n",
      "Epoch 150/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.4749 - acc: 0.7943 - val_loss: 0.4870 - val_acc: 0.7865\n",
      "Epoch 151/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.4761 - acc: 0.7995 - val_loss: 0.4842 - val_acc: 0.7917\n",
      "Epoch 152/200\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.4769 - acc: 0.7799 - val_loss: 0.4949 - val_acc: 0.7865\n",
      "Epoch 153/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.4884 - acc: 0.7799 - val_loss: 0.5143 - val_acc: 0.7708\n",
      "Epoch 154/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.4826 - acc: 0.7839 - val_loss: 0.4978 - val_acc: 0.7865\n",
      "Epoch 155/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.4746 - acc: 0.7982 - val_loss: 0.4983 - val_acc: 0.7708\n",
      "Epoch 156/200\n",
      "768/768 [==============================] - 0s 479us/step - loss: 0.4755 - acc: 0.7878 - val_loss: 0.4999 - val_acc: 0.7865\n",
      "Epoch 157/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.4714 - acc: 0.7904 - val_loss: 0.4888 - val_acc: 0.7865\n",
      "Epoch 158/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.4882 - acc: 0.7773 - val_loss: 0.5678 - val_acc: 0.7396\n",
      "Epoch 159/200\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.4928 - acc: 0.7669 - val_loss: 0.4949 - val_acc: 0.8021\n",
      "Epoch 160/200\n",
      "768/768 [==============================] - 0s 420us/step - loss: 0.4858 - acc: 0.7852 - val_loss: 0.6294 - val_acc: 0.7083\n",
      "Epoch 161/200\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.4768 - acc: 0.7930 - val_loss: 0.5042 - val_acc: 0.7865\n",
      "Epoch 162/200\n",
      "768/768 [==============================] - 0s 421us/step - loss: 0.4781 - acc: 0.7904 - val_loss: 0.4920 - val_acc: 0.7865\n",
      "Epoch 163/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.4679 - acc: 0.7917 - val_loss: 0.4868 - val_acc: 0.7969\n",
      "Epoch 164/200\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.4751 - acc: 0.7904 - val_loss: 0.4967 - val_acc: 0.7865\n",
      "Epoch 165/200\n",
      "768/768 [==============================] - 0s 466us/step - loss: 0.4655 - acc: 0.7969 - val_loss: 0.4908 - val_acc: 0.7969\n",
      "Epoch 166/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.4681 - acc: 0.7852 - val_loss: 0.5143 - val_acc: 0.7865\n",
      "Epoch 167/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.4759 - acc: 0.7878 - val_loss: 0.4833 - val_acc: 0.7865\n",
      "Epoch 168/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.4881 - acc: 0.7852 - val_loss: 0.5922 - val_acc: 0.7240\n",
      "Epoch 169/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.4841 - acc: 0.7839 - val_loss: 0.4820 - val_acc: 0.7865\n",
      "Epoch 170/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.4676 - acc: 0.7826 - val_loss: 0.4843 - val_acc: 0.7812\n",
      "Epoch 171/200\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.4557 - acc: 0.7995 - val_loss: 0.5292 - val_acc: 0.7812\n",
      "Epoch 172/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.4633 - acc: 0.7852 - val_loss: 0.4839 - val_acc: 0.7865\n",
      "Epoch 173/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.4749 - acc: 0.7812 - val_loss: 0.5735 - val_acc: 0.7240\n",
      "Epoch 174/200\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.4756 - acc: 0.7812 - val_loss: 0.4990 - val_acc: 0.7917\n",
      "Epoch 175/200\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.4629 - acc: 0.7891 - val_loss: 0.4897 - val_acc: 0.7969\n",
      "Epoch 176/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.4677 - acc: 0.7956 - val_loss: 0.4998 - val_acc: 0.7917\n",
      "Epoch 177/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.4638 - acc: 0.7917 - val_loss: 0.4946 - val_acc: 0.7812\n",
      "Epoch 178/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.4664 - acc: 0.7904 - val_loss: 0.4929 - val_acc: 0.7917\n",
      "Epoch 179/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.4541 - acc: 0.7917 - val_loss: 0.5828 - val_acc: 0.7344\n",
      "Epoch 180/200\n",
      "768/768 [==============================] - 0s 421us/step - loss: 0.4632 - acc: 0.7943 - val_loss: 0.4835 - val_acc: 0.7812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.4571 - acc: 0.8060 - val_loss: 0.4916 - val_acc: 0.7969\n",
      "Epoch 182/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.4603 - acc: 0.8021 - val_loss: 0.4836 - val_acc: 0.7865\n",
      "Epoch 183/200\n",
      "768/768 [==============================] - 0s 425us/step - loss: 0.4730 - acc: 0.7982 - val_loss: 0.5100 - val_acc: 0.7760\n",
      "Epoch 184/200\n",
      "768/768 [==============================] - 0s 423us/step - loss: 0.4592 - acc: 0.7969 - val_loss: 0.5081 - val_acc: 0.7969\n",
      "Epoch 185/200\n",
      "768/768 [==============================] - 0s 406us/step - loss: 0.4715 - acc: 0.7917 - val_loss: 0.4843 - val_acc: 0.7812\n",
      "Epoch 186/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.4579 - acc: 0.8034 - val_loss: 0.4874 - val_acc: 0.8021\n",
      "Epoch 187/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.4519 - acc: 0.8060 - val_loss: 0.4912 - val_acc: 0.7865\n",
      "Epoch 188/200\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.4476 - acc: 0.8086 - val_loss: 0.5079 - val_acc: 0.7865\n",
      "Epoch 189/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.4459 - acc: 0.8086 - val_loss: 0.5033 - val_acc: 0.7917\n",
      "Epoch 190/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.4458 - acc: 0.8164 - val_loss: 0.4891 - val_acc: 0.7865\n",
      "Epoch 191/200\n",
      "768/768 [==============================] - 0s 474us/step - loss: 0.4485 - acc: 0.7917 - val_loss: 0.4864 - val_acc: 0.7812\n",
      "Epoch 192/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.4479 - acc: 0.8047 - val_loss: 0.4943 - val_acc: 0.7865\n",
      "Epoch 193/200\n",
      "768/768 [==============================] - 0s 471us/step - loss: 0.4575 - acc: 0.7969 - val_loss: 0.4893 - val_acc: 0.7917\n",
      "Epoch 194/200\n",
      "768/768 [==============================] - 0s 421us/step - loss: 0.4693 - acc: 0.7878 - val_loss: 0.4883 - val_acc: 0.7917\n",
      "Epoch 195/200\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.4693 - acc: 0.7969 - val_loss: 0.5005 - val_acc: 0.7708\n",
      "Epoch 196/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.4650 - acc: 0.7812 - val_loss: 0.4810 - val_acc: 0.7917\n",
      "Epoch 197/200\n",
      "768/768 [==============================] - 0s 418us/step - loss: 0.4563 - acc: 0.7995 - val_loss: 0.4990 - val_acc: 0.7812\n",
      "Epoch 198/200\n",
      "768/768 [==============================] - 0s 420us/step - loss: 0.4647 - acc: 0.8008 - val_loss: 0.4962 - val_acc: 0.7917\n",
      "Epoch 199/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.4563 - acc: 0.7982 - val_loss: 0.4838 - val_acc: 0.7917\n",
      "Epoch 200/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.4470 - acc: 0.7995 - val_loss: 0.4992 - val_acc: 0.7969\n",
      "Predicting...\n",
      "Cross val iteration 2 of 5\n",
      "Fitting...\n",
      "Train on 768 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "768/768 [==============================] - 6s 8ms/step - loss: 0.4593 - acc: 0.8034 - val_loss: 0.4991 - val_acc: 0.7969\n",
      "Epoch 2/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.4413 - acc: 0.8073 - val_loss: 0.4855 - val_acc: 0.7865\n",
      "Epoch 3/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.4374 - acc: 0.8190 - val_loss: 0.4955 - val_acc: 0.7604\n",
      "Epoch 4/200\n",
      "768/768 [==============================] - 0s 416us/step - loss: 0.4443 - acc: 0.8138 - val_loss: 0.4981 - val_acc: 0.7917\n",
      "Epoch 5/200\n",
      "768/768 [==============================] - 0s 422us/step - loss: 0.4354 - acc: 0.8086 - val_loss: 0.4877 - val_acc: 0.7656\n",
      "Epoch 6/200\n",
      "768/768 [==============================] - 0s 425us/step - loss: 0.4427 - acc: 0.8021 - val_loss: 0.5073 - val_acc: 0.7917\n",
      "Epoch 7/200\n",
      "768/768 [==============================] - 0s 424us/step - loss: 0.4384 - acc: 0.8047 - val_loss: 0.5041 - val_acc: 0.7917\n",
      "Epoch 8/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.4252 - acc: 0.8203 - val_loss: 0.5374 - val_acc: 0.7500\n",
      "Epoch 9/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.4578 - acc: 0.7956 - val_loss: 0.5217 - val_acc: 0.8021\n",
      "Epoch 10/200\n",
      "768/768 [==============================] - 0s 455us/step - loss: 0.4522 - acc: 0.7982 - val_loss: 0.5013 - val_acc: 0.7812\n",
      "Epoch 11/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.4435 - acc: 0.8151 - val_loss: 0.4979 - val_acc: 0.7969\n",
      "Epoch 12/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.4341 - acc: 0.8138 - val_loss: 0.5038 - val_acc: 0.7917\n",
      "Epoch 13/200\n",
      "768/768 [==============================] - 0s 486us/step - loss: 0.4289 - acc: 0.8125 - val_loss: 0.4938 - val_acc: 0.8021\n",
      "Epoch 14/200\n",
      "768/768 [==============================] - 0s 463us/step - loss: 0.4370 - acc: 0.8164 - val_loss: 0.4955 - val_acc: 0.7917\n",
      "Epoch 15/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.4306 - acc: 0.8112 - val_loss: 0.5287 - val_acc: 0.7865\n",
      "Epoch 16/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.4566 - acc: 0.8125 - val_loss: 0.5020 - val_acc: 0.7969\n",
      "Epoch 17/200\n",
      "768/768 [==============================] - 0s 474us/step - loss: 0.4465 - acc: 0.8034 - val_loss: 0.5116 - val_acc: 0.7865\n",
      "Epoch 18/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.4415 - acc: 0.8177 - val_loss: 0.4891 - val_acc: 0.7760\n",
      "Epoch 19/200\n",
      "768/768 [==============================] - 0s 467us/step - loss: 0.4440 - acc: 0.8034 - val_loss: 0.4891 - val_acc: 0.7760\n",
      "Epoch 20/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.4445 - acc: 0.8047 - val_loss: 0.4882 - val_acc: 0.7865\n",
      "Epoch 21/200\n",
      "768/768 [==============================] - 0s 465us/step - loss: 0.4414 - acc: 0.8125 - val_loss: 0.4787 - val_acc: 0.7812\n",
      "Epoch 22/200\n",
      "768/768 [==============================] - 0s 476us/step - loss: 0.4318 - acc: 0.8177 - val_loss: 0.4929 - val_acc: 0.7656\n",
      "Epoch 23/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.4231 - acc: 0.8177 - val_loss: 0.4958 - val_acc: 0.7604\n",
      "Epoch 24/200\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.4378 - acc: 0.8086 - val_loss: 0.4937 - val_acc: 0.7969\n",
      "Epoch 25/200\n",
      "768/768 [==============================] - 0s 469us/step - loss: 0.4236 - acc: 0.8060 - val_loss: 0.5317 - val_acc: 0.7917\n",
      "Epoch 26/200\n",
      "768/768 [==============================] - 0s 424us/step - loss: 0.4298 - acc: 0.8125 - val_loss: 0.4946 - val_acc: 0.7917\n",
      "Epoch 27/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.4427 - acc: 0.8073 - val_loss: 0.4994 - val_acc: 0.7760\n",
      "Epoch 28/200\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.4433 - acc: 0.7943 - val_loss: 0.5436 - val_acc: 0.7708\n",
      "Epoch 29/200\n",
      "768/768 [==============================] - 0s 405us/step - loss: 0.4376 - acc: 0.7891 - val_loss: 0.5223 - val_acc: 0.7760\n",
      "Epoch 30/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.4333 - acc: 0.8164 - val_loss: 0.4990 - val_acc: 0.7917\n",
      "Epoch 31/200\n",
      "768/768 [==============================] - 0s 463us/step - loss: 0.4327 - acc: 0.8112 - val_loss: 0.4945 - val_acc: 0.7812\n",
      "Epoch 32/200\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.4226 - acc: 0.8229 - val_loss: 0.4917 - val_acc: 0.7917\n",
      "Epoch 33/200\n",
      "768/768 [==============================] - 0s 419us/step - loss: 0.4411 - acc: 0.7943 - val_loss: 0.5175 - val_acc: 0.7865\n",
      "Epoch 34/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.4297 - acc: 0.8242 - val_loss: 0.5244 - val_acc: 0.7917\n",
      "Epoch 35/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.4332 - acc: 0.8177 - val_loss: 0.4928 - val_acc: 0.7708\n",
      "Epoch 36/200\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.4466 - acc: 0.8047 - val_loss: 0.4918 - val_acc: 0.7865\n",
      "Epoch 37/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.4516 - acc: 0.8008 - val_loss: 0.5011 - val_acc: 0.7917\n",
      "Epoch 38/200\n",
      "768/768 [==============================] - 0s 468us/step - loss: 0.4378 - acc: 0.8190 - val_loss: 0.5064 - val_acc: 0.7969\n",
      "Epoch 39/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.4318 - acc: 0.8164 - val_loss: 0.4870 - val_acc: 0.7969\n",
      "Epoch 40/200\n",
      "768/768 [==============================] - 0s 409us/step - loss: 0.4275 - acc: 0.8229 - val_loss: 0.4937 - val_acc: 0.7969\n",
      "Epoch 41/200\n",
      "768/768 [==============================] - 0s 419us/step - loss: 0.4300 - acc: 0.8112 - val_loss: 0.4921 - val_acc: 0.7917\n",
      "Epoch 42/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.4361 - acc: 0.8138 - val_loss: 0.5817 - val_acc: 0.7500\n",
      "Epoch 43/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.4315 - acc: 0.8125 - val_loss: 0.4931 - val_acc: 0.8021\n",
      "Epoch 44/200\n",
      "768/768 [==============================] - 0s 501us/step - loss: 0.4203 - acc: 0.8229 - val_loss: 0.5007 - val_acc: 0.8021\n",
      "Epoch 45/200\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.4238 - acc: 0.8190 - val_loss: 0.5087 - val_acc: 0.7760\n",
      "Epoch 46/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.4274 - acc: 0.8099 - val_loss: 0.4928 - val_acc: 0.7969\n",
      "Epoch 47/200\n",
      "768/768 [==============================] - 0s 468us/step - loss: 0.4265 - acc: 0.8177 - val_loss: 0.4958 - val_acc: 0.7917\n",
      "Epoch 48/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.4265 - acc: 0.8268 - val_loss: 0.4852 - val_acc: 0.7917\n",
      "Epoch 49/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.4183 - acc: 0.8294 - val_loss: 0.5755 - val_acc: 0.7500\n",
      "Epoch 50/200\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.4332 - acc: 0.8099 - val_loss: 0.4976 - val_acc: 0.7969\n",
      "Epoch 51/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.4201 - acc: 0.8229 - val_loss: 0.5205 - val_acc: 0.7812\n",
      "Epoch 52/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.4385 - acc: 0.8047 - val_loss: 0.4969 - val_acc: 0.7865\n",
      "Epoch 53/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.4284 - acc: 0.8268 - val_loss: 0.4968 - val_acc: 0.7760\n",
      "Epoch 54/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.4171 - acc: 0.8255 - val_loss: 0.4933 - val_acc: 0.7865\n",
      "Epoch 55/200\n",
      "768/768 [==============================] - 0s 476us/step - loss: 0.4193 - acc: 0.8255 - val_loss: 0.5141 - val_acc: 0.7969\n",
      "Epoch 56/200\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.4215 - acc: 0.8359 - val_loss: 0.4938 - val_acc: 0.7917\n",
      "Epoch 57/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.4155 - acc: 0.8307 - val_loss: 0.4889 - val_acc: 0.8073\n",
      "Epoch 58/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.4190 - acc: 0.8255 - val_loss: 0.5056 - val_acc: 0.7917\n",
      "Epoch 59/200\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.4312 - acc: 0.8177 - val_loss: 0.4732 - val_acc: 0.8073\n",
      "Epoch 60/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.4332 - acc: 0.8190 - val_loss: 0.4800 - val_acc: 0.8073\n",
      "Epoch 61/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.4123 - acc: 0.8346 - val_loss: 0.4962 - val_acc: 0.7865\n",
      "Epoch 62/200\n",
      "768/768 [==============================] - 0s 419us/step - loss: 0.4179 - acc: 0.8346 - val_loss: 0.5318 - val_acc: 0.7917\n",
      "Epoch 63/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.4240 - acc: 0.8255 - val_loss: 0.5525 - val_acc: 0.7656\n",
      "Epoch 64/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.4376 - acc: 0.8138 - val_loss: 0.4944 - val_acc: 0.7865\n",
      "Epoch 65/200\n",
      "768/768 [==============================] - 0s 410us/step - loss: 0.4154 - acc: 0.8320 - val_loss: 0.5161 - val_acc: 0.7917\n",
      "Epoch 66/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.4428 - acc: 0.8112 - val_loss: 0.5158 - val_acc: 0.7760\n",
      "Epoch 67/200\n",
      "768/768 [==============================] - 0s 473us/step - loss: 0.4533 - acc: 0.8034 - val_loss: 0.5184 - val_acc: 0.7708\n",
      "Epoch 68/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.4180 - acc: 0.8216 - val_loss: 0.4861 - val_acc: 0.7812\n",
      "Epoch 69/200\n",
      "768/768 [==============================] - 0s 424us/step - loss: 0.4174 - acc: 0.8307 - val_loss: 0.4945 - val_acc: 0.8021\n",
      "Epoch 70/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.4119 - acc: 0.8307 - val_loss: 0.4899 - val_acc: 0.8021\n",
      "Epoch 71/200\n",
      "768/768 [==============================] - 0s 419us/step - loss: 0.4173 - acc: 0.8346 - val_loss: 0.4989 - val_acc: 0.8021\n",
      "Epoch 72/200\n",
      "768/768 [==============================] - 0s 424us/step - loss: 0.4142 - acc: 0.8333 - val_loss: 0.4990 - val_acc: 0.7865\n",
      "Epoch 73/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.4231 - acc: 0.8190 - val_loss: 0.4776 - val_acc: 0.8073\n",
      "Epoch 74/200\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.4408 - acc: 0.8216 - val_loss: 0.4853 - val_acc: 0.7865\n",
      "Epoch 75/200\n",
      "768/768 [==============================] - 0s 421us/step - loss: 0.4161 - acc: 0.8203 - val_loss: 0.4896 - val_acc: 0.7865\n",
      "Epoch 76/200\n",
      "768/768 [==============================] - 0s 455us/step - loss: 0.4100 - acc: 0.8255 - val_loss: 0.5177 - val_acc: 0.7760\n",
      "Epoch 77/200\n",
      "768/768 [==============================] - 0s 425us/step - loss: 0.4376 - acc: 0.8112 - val_loss: 0.4993 - val_acc: 0.8021\n",
      "Epoch 78/200\n",
      "768/768 [==============================] - 0s 483us/step - loss: 0.4210 - acc: 0.8255 - val_loss: 0.5057 - val_acc: 0.7656\n",
      "Epoch 79/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.4495 - acc: 0.8164 - val_loss: 0.5029 - val_acc: 0.8073\n",
      "Epoch 80/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.4204 - acc: 0.8242 - val_loss: 0.4861 - val_acc: 0.8125\n",
      "Epoch 81/200\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.4129 - acc: 0.8307 - val_loss: 0.4820 - val_acc: 0.8021\n",
      "Epoch 82/200\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.4126 - acc: 0.8255 - val_loss: 0.5197 - val_acc: 0.7812\n",
      "Epoch 83/200\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.4369 - acc: 0.8164 - val_loss: 0.4929 - val_acc: 0.7969\n",
      "Epoch 84/200\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.4227 - acc: 0.8099 - val_loss: 0.4760 - val_acc: 0.8073\n",
      "Epoch 85/200\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.4191 - acc: 0.8242 - val_loss: 0.5156 - val_acc: 0.7969\n",
      "Epoch 86/200\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.4118 - acc: 0.8372 - val_loss: 0.4859 - val_acc: 0.8073\n",
      "Epoch 87/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.4157 - acc: 0.8372 - val_loss: 0.5463 - val_acc: 0.7396\n",
      "Epoch 88/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.4312 - acc: 0.8255 - val_loss: 0.5155 - val_acc: 0.8021\n",
      "Epoch 89/200\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.4101 - acc: 0.8281 - val_loss: 0.4837 - val_acc: 0.8073\n",
      "Epoch 90/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.4071 - acc: 0.8281 - val_loss: 0.4851 - val_acc: 0.8125\n",
      "Epoch 91/200\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.4084 - acc: 0.8307 - val_loss: 0.5089 - val_acc: 0.7865\n",
      "Epoch 92/200\n",
      "768/768 [==============================] - 0s 486us/step - loss: 0.4180 - acc: 0.8294 - val_loss: 0.4884 - val_acc: 0.7969\n",
      "Epoch 93/200\n",
      "768/768 [==============================] - 0s 456us/step - loss: 0.4089 - acc: 0.8359 - val_loss: 0.4957 - val_acc: 0.7865\n",
      "Epoch 94/200\n",
      "768/768 [==============================] - 0s 471us/step - loss: 0.4077 - acc: 0.8385 - val_loss: 0.4924 - val_acc: 0.8073\n",
      "Epoch 95/200\n",
      "768/768 [==============================] - 0s 481us/step - loss: 0.4131 - acc: 0.8294 - val_loss: 0.5162 - val_acc: 0.7969\n",
      "Epoch 96/200\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.4085 - acc: 0.8372 - val_loss: 0.5338 - val_acc: 0.7812\n",
      "Epoch 97/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.4137 - acc: 0.8268 - val_loss: 0.5045 - val_acc: 0.8021\n",
      "Epoch 98/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.4138 - acc: 0.8294 - val_loss: 0.5148 - val_acc: 0.8021\n",
      "Epoch 99/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.4026 - acc: 0.8320 - val_loss: 0.5532 - val_acc: 0.7500\n",
      "Epoch 100/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 456us/step - loss: 0.4211 - acc: 0.8281 - val_loss: 0.5159 - val_acc: 0.7917\n",
      "Epoch 101/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.4075 - acc: 0.8359 - val_loss: 0.4994 - val_acc: 0.7865\n",
      "Epoch 102/200\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.4020 - acc: 0.8424 - val_loss: 0.4903 - val_acc: 0.8177\n",
      "Epoch 103/200\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.4045 - acc: 0.8346 - val_loss: 0.5407 - val_acc: 0.7760\n",
      "Epoch 104/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.4065 - acc: 0.8385 - val_loss: 0.5085 - val_acc: 0.8073\n",
      "Epoch 105/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.4012 - acc: 0.8333 - val_loss: 0.4907 - val_acc: 0.8073\n",
      "Epoch 106/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.4096 - acc: 0.8320 - val_loss: 0.5032 - val_acc: 0.7865\n",
      "Epoch 107/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.4741 - acc: 0.7865 - val_loss: 0.5569 - val_acc: 0.7604\n",
      "Epoch 108/200\n",
      "768/768 [==============================] - 0s 466us/step - loss: 0.4216 - acc: 0.8216 - val_loss: 0.5235 - val_acc: 0.7552\n",
      "Epoch 109/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.4485 - acc: 0.8125 - val_loss: 0.4896 - val_acc: 0.7969\n",
      "Epoch 110/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.4237 - acc: 0.8164 - val_loss: 0.5170 - val_acc: 0.7812\n",
      "Epoch 111/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.4282 - acc: 0.8112 - val_loss: 0.5023 - val_acc: 0.7917\n",
      "Epoch 112/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.4383 - acc: 0.8255 - val_loss: 0.4873 - val_acc: 0.7917\n",
      "Epoch 113/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.4042 - acc: 0.8333 - val_loss: 0.4887 - val_acc: 0.8177\n",
      "Epoch 114/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.3983 - acc: 0.8503 - val_loss: 0.5123 - val_acc: 0.7917\n",
      "Epoch 115/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.4010 - acc: 0.8438 - val_loss: 0.4897 - val_acc: 0.8177\n",
      "Epoch 116/200\n",
      "768/768 [==============================] - 0s 455us/step - loss: 0.4040 - acc: 0.8464 - val_loss: 0.5039 - val_acc: 0.8021\n",
      "Epoch 117/200\n",
      "768/768 [==============================] - 0s 477us/step - loss: 0.4102 - acc: 0.8372 - val_loss: 0.4830 - val_acc: 0.8177\n",
      "Epoch 118/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.4079 - acc: 0.8333 - val_loss: 0.4920 - val_acc: 0.8125\n",
      "Epoch 119/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.3935 - acc: 0.8438 - val_loss: 0.5078 - val_acc: 0.8073\n",
      "Epoch 120/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.3995 - acc: 0.8516 - val_loss: 0.4974 - val_acc: 0.8073\n",
      "Epoch 121/200\n",
      "768/768 [==============================] - 0s 496us/step - loss: 0.4141 - acc: 0.8255 - val_loss: 0.4791 - val_acc: 0.8021\n",
      "Epoch 122/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.4121 - acc: 0.8307 - val_loss: 0.4916 - val_acc: 0.7969\n",
      "Epoch 123/200\n",
      "768/768 [==============================] - 0s 467us/step - loss: 0.4215 - acc: 0.8333 - val_loss: 0.4687 - val_acc: 0.8177\n",
      "Epoch 124/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.4043 - acc: 0.8359 - val_loss: 0.5148 - val_acc: 0.8073\n",
      "Epoch 125/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.3990 - acc: 0.8438 - val_loss: 0.4880 - val_acc: 0.8177\n",
      "Epoch 126/200\n",
      "768/768 [==============================] - 0s 471us/step - loss: 0.3872 - acc: 0.8607 - val_loss: 0.4777 - val_acc: 0.8073\n",
      "Epoch 127/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.3956 - acc: 0.8464 - val_loss: 0.5031 - val_acc: 0.8021\n",
      "Epoch 128/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.3916 - acc: 0.8464 - val_loss: 0.4942 - val_acc: 0.8021\n",
      "Epoch 129/200\n",
      "768/768 [==============================] - 0s 423us/step - loss: 0.3955 - acc: 0.8438 - val_loss: 0.4784 - val_acc: 0.8125\n",
      "Epoch 130/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.3930 - acc: 0.8451 - val_loss: 0.5071 - val_acc: 0.8021\n",
      "Epoch 131/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.4142 - acc: 0.8346 - val_loss: 0.4821 - val_acc: 0.8177\n",
      "Epoch 132/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.3985 - acc: 0.8398 - val_loss: 0.5105 - val_acc: 0.7812\n",
      "Epoch 133/200\n",
      "768/768 [==============================] - 0s 463us/step - loss: 0.4299 - acc: 0.8242 - val_loss: 0.4827 - val_acc: 0.8229\n",
      "Epoch 134/200\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.4008 - acc: 0.8372 - val_loss: 0.5025 - val_acc: 0.7917\n",
      "Epoch 135/200\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.3973 - acc: 0.8359 - val_loss: 0.5478 - val_acc: 0.7812\n",
      "Epoch 136/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.4183 - acc: 0.8268 - val_loss: 0.5077 - val_acc: 0.7865\n",
      "Epoch 137/200\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.3968 - acc: 0.8398 - val_loss: 0.4798 - val_acc: 0.8229\n",
      "Epoch 138/200\n",
      "768/768 [==============================] - 0s 461us/step - loss: 0.3968 - acc: 0.8398 - val_loss: 0.5066 - val_acc: 0.7969\n",
      "Epoch 139/200\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.4095 - acc: 0.8268 - val_loss: 0.5109 - val_acc: 0.7969\n",
      "Epoch 140/200\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.4134 - acc: 0.8242 - val_loss: 0.4800 - val_acc: 0.8177\n",
      "Epoch 141/200\n",
      "768/768 [==============================] - 0s 412us/step - loss: 0.3980 - acc: 0.8477 - val_loss: 0.5148 - val_acc: 0.8125\n",
      "Epoch 142/200\n",
      "768/768 [==============================] - 0s 423us/step - loss: 0.3896 - acc: 0.8516 - val_loss: 0.4895 - val_acc: 0.8177\n",
      "Epoch 143/200\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.3864 - acc: 0.8529 - val_loss: 0.5082 - val_acc: 0.7917\n",
      "Epoch 144/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.3951 - acc: 0.8464 - val_loss: 0.5054 - val_acc: 0.8073\n",
      "Epoch 145/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.3973 - acc: 0.8411 - val_loss: 0.5156 - val_acc: 0.7917\n",
      "Epoch 146/200\n",
      "768/768 [==============================] - 0s 409us/step - loss: 0.4488 - acc: 0.8203 - val_loss: 0.5722 - val_acc: 0.7760\n",
      "Epoch 147/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.4113 - acc: 0.8229 - val_loss: 0.5475 - val_acc: 0.7604\n",
      "Epoch 148/200\n",
      "768/768 [==============================] - 0s 424us/step - loss: 0.4224 - acc: 0.8268 - val_loss: 0.5198 - val_acc: 0.7656\n",
      "Epoch 149/200\n",
      "768/768 [==============================] - 0s 422us/step - loss: 0.4266 - acc: 0.8281 - val_loss: 0.5324 - val_acc: 0.7969\n",
      "Epoch 150/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.4056 - acc: 0.8307 - val_loss: 0.4820 - val_acc: 0.8073\n",
      "Epoch 151/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.4012 - acc: 0.8451 - val_loss: 0.5017 - val_acc: 0.7917\n",
      "Epoch 152/200\n",
      "768/768 [==============================] - 0s 418us/step - loss: 0.4402 - acc: 0.8151 - val_loss: 0.4925 - val_acc: 0.8021\n",
      "Epoch 153/200\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.4023 - acc: 0.8424 - val_loss: 0.4796 - val_acc: 0.8021\n",
      "Epoch 154/200\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.3980 - acc: 0.8451 - val_loss: 0.4922 - val_acc: 0.7969\n",
      "Epoch 155/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.4015 - acc: 0.8464 - val_loss: 0.5517 - val_acc: 0.7656\n",
      "Epoch 156/200\n",
      "768/768 [==============================] - 0s 478us/step - loss: 0.4395 - acc: 0.8190 - val_loss: 0.4952 - val_acc: 0.7917\n",
      "Epoch 157/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.4322 - acc: 0.8151 - val_loss: 0.4812 - val_acc: 0.7969\n",
      "Epoch 158/200\n",
      "768/768 [==============================] - 0s 413us/step - loss: 0.4052 - acc: 0.8320 - val_loss: 0.4902 - val_acc: 0.7917\n",
      "Epoch 159/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.4002 - acc: 0.8385 - val_loss: 0.4986 - val_acc: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.3904 - acc: 0.8503 - val_loss: 0.4923 - val_acc: 0.7969\n",
      "Epoch 161/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.3896 - acc: 0.8464 - val_loss: 0.5080 - val_acc: 0.7969\n",
      "Epoch 162/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.3901 - acc: 0.8464 - val_loss: 0.4901 - val_acc: 0.7969\n",
      "Epoch 163/200\n",
      "768/768 [==============================] - 0s 456us/step - loss: 0.3946 - acc: 0.8464 - val_loss: 0.5172 - val_acc: 0.7917\n",
      "Epoch 164/200\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.3944 - acc: 0.8568 - val_loss: 0.4912 - val_acc: 0.8125\n",
      "Epoch 165/200\n",
      "768/768 [==============================] - 0s 484us/step - loss: 0.4062 - acc: 0.8438 - val_loss: 0.4738 - val_acc: 0.8125\n",
      "Epoch 166/200\n",
      "768/768 [==============================] - 0s 469us/step - loss: 0.4000 - acc: 0.8477 - val_loss: 0.4898 - val_acc: 0.8177\n",
      "Epoch 167/200\n",
      "768/768 [==============================] - 0s 417us/step - loss: 0.3923 - acc: 0.8424 - val_loss: 0.4778 - val_acc: 0.8177\n",
      "Epoch 168/200\n",
      "768/768 [==============================] - 0s 413us/step - loss: 0.3820 - acc: 0.8568 - val_loss: 0.5023 - val_acc: 0.7969\n",
      "Epoch 169/200\n",
      "768/768 [==============================] - 0s 420us/step - loss: 0.3825 - acc: 0.8529 - val_loss: 0.4846 - val_acc: 0.8229\n",
      "Epoch 170/200\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.3821 - acc: 0.8594 - val_loss: 0.5148 - val_acc: 0.8021\n",
      "Epoch 171/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.3821 - acc: 0.8490 - val_loss: 0.4833 - val_acc: 0.8229\n",
      "Epoch 172/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.3955 - acc: 0.8385 - val_loss: 0.4744 - val_acc: 0.8229\n",
      "Epoch 173/200\n",
      "768/768 [==============================] - 0s 423us/step - loss: 0.3976 - acc: 0.8451 - val_loss: 0.5132 - val_acc: 0.7917\n",
      "Epoch 174/200\n",
      "768/768 [==============================] - 0s 466us/step - loss: 0.4118 - acc: 0.8268 - val_loss: 0.4852 - val_acc: 0.8177\n",
      "Epoch 175/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.3966 - acc: 0.8464 - val_loss: 0.4828 - val_acc: 0.8229\n",
      "Epoch 176/200\n",
      "768/768 [==============================] - 0s 421us/step - loss: 0.3919 - acc: 0.8477 - val_loss: 0.5056 - val_acc: 0.8125\n",
      "Epoch 177/200\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.4077 - acc: 0.8346 - val_loss: 0.4878 - val_acc: 0.7917\n",
      "Epoch 178/200\n",
      "768/768 [==============================] - 0s 409us/step - loss: 0.4229 - acc: 0.8346 - val_loss: 0.5022 - val_acc: 0.8073\n",
      "Epoch 179/200\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.3883 - acc: 0.8568 - val_loss: 0.5121 - val_acc: 0.7917\n",
      "Epoch 180/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.3874 - acc: 0.8555 - val_loss: 0.4975 - val_acc: 0.8177\n",
      "Epoch 181/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.3796 - acc: 0.8581 - val_loss: 0.4827 - val_acc: 0.8073\n",
      "Epoch 182/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.4079 - acc: 0.8359 - val_loss: 0.4846 - val_acc: 0.8177\n",
      "Epoch 183/200\n",
      "768/768 [==============================] - 0s 408us/step - loss: 0.3804 - acc: 0.8620 - val_loss: 0.5101 - val_acc: 0.7917\n",
      "Epoch 184/200\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.3726 - acc: 0.8620 - val_loss: 0.4753 - val_acc: 0.8125\n",
      "Epoch 185/200\n",
      "768/768 [==============================] - 0s 456us/step - loss: 0.3832 - acc: 0.8568 - val_loss: 0.4949 - val_acc: 0.8177\n",
      "Epoch 186/200\n",
      "768/768 [==============================] - 0s 461us/step - loss: 0.4288 - acc: 0.8151 - val_loss: 0.5027 - val_acc: 0.7917\n",
      "Epoch 187/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.4009 - acc: 0.8464 - val_loss: 0.4841 - val_acc: 0.8073\n",
      "Epoch 188/200\n",
      "768/768 [==============================] - 0s 488us/step - loss: 0.3810 - acc: 0.8568 - val_loss: 0.4946 - val_acc: 0.7969\n",
      "Epoch 189/200\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.3952 - acc: 0.8438 - val_loss: 0.5222 - val_acc: 0.8125\n",
      "Epoch 190/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.3842 - acc: 0.8529 - val_loss: 0.5105 - val_acc: 0.8021\n",
      "Epoch 191/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.3759 - acc: 0.8607 - val_loss: 0.4943 - val_acc: 0.8177\n",
      "Epoch 192/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.3721 - acc: 0.8568 - val_loss: 0.4982 - val_acc: 0.8073\n",
      "Epoch 193/200\n",
      "768/768 [==============================] - 0s 455us/step - loss: 0.3846 - acc: 0.8555 - val_loss: 0.4835 - val_acc: 0.8125\n",
      "Epoch 194/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.3797 - acc: 0.8594 - val_loss: 0.5173 - val_acc: 0.7656\n",
      "Epoch 195/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.3734 - acc: 0.8594 - val_loss: 0.4893 - val_acc: 0.7969\n",
      "Epoch 196/200\n",
      "768/768 [==============================] - 0s 468us/step - loss: 0.3710 - acc: 0.8594 - val_loss: 0.5296 - val_acc: 0.8073\n",
      "Epoch 197/200\n",
      "768/768 [==============================] - 0s 456us/step - loss: 0.3948 - acc: 0.8503 - val_loss: 0.4881 - val_acc: 0.8073\n",
      "Epoch 198/200\n",
      "768/768 [==============================] - 0s 463us/step - loss: 0.3832 - acc: 0.8542 - val_loss: 0.5035 - val_acc: 0.7865\n",
      "Epoch 199/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.4055 - acc: 0.8242 - val_loss: 0.5087 - val_acc: 0.7917\n",
      "Epoch 200/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.3862 - acc: 0.8555 - val_loss: 0.4938 - val_acc: 0.7865\n",
      "Predicting...\n",
      "Cross val iteration 3 of 5\n",
      "Fitting...\n",
      "Train on 768 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "768/768 [==============================] - 7s 9ms/step - loss: 0.3875 - acc: 0.8451 - val_loss: 0.5046 - val_acc: 0.7969\n",
      "Epoch 2/200\n",
      "768/768 [==============================] - 0s 456us/step - loss: 0.3838 - acc: 0.8424 - val_loss: 0.4820 - val_acc: 0.8177\n",
      "Epoch 3/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.3852 - acc: 0.8490 - val_loss: 0.5104 - val_acc: 0.8021\n",
      "Epoch 4/200\n",
      "768/768 [==============================] - 0s 477us/step - loss: 0.3727 - acc: 0.8672 - val_loss: 0.4908 - val_acc: 0.8125\n",
      "Epoch 5/200\n",
      "768/768 [==============================] - 0s 485us/step - loss: 0.3655 - acc: 0.8620 - val_loss: 0.5187 - val_acc: 0.7865\n",
      "Epoch 6/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.3682 - acc: 0.8646 - val_loss: 0.5309 - val_acc: 0.8021\n",
      "Epoch 7/200\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.3761 - acc: 0.8490 - val_loss: 0.5201 - val_acc: 0.7865\n",
      "Epoch 8/200\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.3695 - acc: 0.8633 - val_loss: 0.5367 - val_acc: 0.7708\n",
      "Epoch 9/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.3746 - acc: 0.8477 - val_loss: 0.6042 - val_acc: 0.7656\n",
      "Epoch 10/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.4020 - acc: 0.8333 - val_loss: 0.5085 - val_acc: 0.7812\n",
      "Epoch 11/200\n",
      "768/768 [==============================] - 0s 467us/step - loss: 0.3697 - acc: 0.8659 - val_loss: 0.4771 - val_acc: 0.8229\n",
      "Epoch 12/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.3679 - acc: 0.8594 - val_loss: 0.4920 - val_acc: 0.8021\n",
      "Epoch 13/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.3724 - acc: 0.8464 - val_loss: 0.5007 - val_acc: 0.7969\n",
      "Epoch 14/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.3708 - acc: 0.8620 - val_loss: 0.4894 - val_acc: 0.8073\n",
      "Epoch 15/200\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.3557 - acc: 0.8711 - val_loss: 0.5020 - val_acc: 0.7917\n",
      "Epoch 16/200\n",
      "768/768 [==============================] - 0s 468us/step - loss: 0.3590 - acc: 0.8620 - val_loss: 0.5388 - val_acc: 0.7760\n",
      "Epoch 17/200\n",
      "768/768 [==============================] - 0s 493us/step - loss: 0.3657 - acc: 0.8620 - val_loss: 0.4839 - val_acc: 0.8125\n",
      "Epoch 18/200\n",
      "768/768 [==============================] - 0s 461us/step - loss: 0.3549 - acc: 0.8685 - val_loss: 0.5753 - val_acc: 0.7708\n",
      "Epoch 19/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.3735 - acc: 0.8581 - val_loss: 0.5356 - val_acc: 0.7969\n",
      "Epoch 20/200\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.4005 - acc: 0.8359 - val_loss: 0.5438 - val_acc: 0.7656\n",
      "Epoch 21/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.3792 - acc: 0.8542 - val_loss: 0.4856 - val_acc: 0.7969\n",
      "Epoch 22/200\n",
      "768/768 [==============================] - 0s 406us/step - loss: 0.3799 - acc: 0.8529 - val_loss: 0.4756 - val_acc: 0.8073\n",
      "Epoch 23/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.3708 - acc: 0.8659 - val_loss: 0.4797 - val_acc: 0.7969\n",
      "Epoch 24/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.3739 - acc: 0.8594 - val_loss: 0.4941 - val_acc: 0.8229\n",
      "Epoch 25/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.3741 - acc: 0.8516 - val_loss: 0.5252 - val_acc: 0.7760\n",
      "Epoch 26/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.3866 - acc: 0.8542 - val_loss: 0.5447 - val_acc: 0.7760\n",
      "Epoch 27/200\n",
      "768/768 [==============================] - 0s 425us/step - loss: 0.4165 - acc: 0.8190 - val_loss: 0.4868 - val_acc: 0.7917\n",
      "Epoch 28/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.3729 - acc: 0.8555 - val_loss: 0.5247 - val_acc: 0.7760\n",
      "Epoch 29/200\n",
      "768/768 [==============================] - 0s 465us/step - loss: 0.3653 - acc: 0.8659 - val_loss: 0.5147 - val_acc: 0.7865\n",
      "Epoch 30/200\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.3524 - acc: 0.8789 - val_loss: 0.5304 - val_acc: 0.7812\n",
      "Epoch 31/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.3504 - acc: 0.8659 - val_loss: 0.5378 - val_acc: 0.7760\n",
      "Epoch 32/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.3527 - acc: 0.8724 - val_loss: 0.5165 - val_acc: 0.8021\n",
      "Epoch 33/200\n",
      "768/768 [==============================] - 0s 471us/step - loss: 0.3622 - acc: 0.8620 - val_loss: 0.5439 - val_acc: 0.7708\n",
      "Epoch 34/200\n",
      "768/768 [==============================] - 0s 464us/step - loss: 0.3624 - acc: 0.8672 - val_loss: 0.5349 - val_acc: 0.7917\n",
      "Epoch 35/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.3504 - acc: 0.8737 - val_loss: 0.5426 - val_acc: 0.7708\n",
      "Epoch 36/200\n",
      "768/768 [==============================] - 0s 474us/step - loss: 0.3556 - acc: 0.8633 - val_loss: 0.5301 - val_acc: 0.7760\n",
      "Epoch 37/200\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.3500 - acc: 0.8711 - val_loss: 0.5608 - val_acc: 0.7760\n",
      "Epoch 38/200\n",
      "768/768 [==============================] - 0s 479us/step - loss: 0.3564 - acc: 0.8607 - val_loss: 0.4961 - val_acc: 0.8125\n",
      "Epoch 39/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.3553 - acc: 0.8672 - val_loss: 0.4880 - val_acc: 0.8021\n",
      "Epoch 40/200\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.3785 - acc: 0.8633 - val_loss: 0.5433 - val_acc: 0.7917\n",
      "Epoch 41/200\n",
      "768/768 [==============================] - 0s 464us/step - loss: 0.3798 - acc: 0.8424 - val_loss: 0.6017 - val_acc: 0.7552\n",
      "Epoch 42/200\n",
      "768/768 [==============================] - 0s 456us/step - loss: 0.4442 - acc: 0.8164 - val_loss: 0.5888 - val_acc: 0.7240\n",
      "Epoch 43/200\n",
      "768/768 [==============================] - 0s 456us/step - loss: 0.4954 - acc: 0.7799 - val_loss: 0.4868 - val_acc: 0.7917\n",
      "Epoch 44/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.4198 - acc: 0.8255 - val_loss: 0.5013 - val_acc: 0.7812\n",
      "Epoch 45/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.3897 - acc: 0.8464 - val_loss: 0.4990 - val_acc: 0.7917\n",
      "Epoch 46/200\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.3600 - acc: 0.8711 - val_loss: 0.5249 - val_acc: 0.7917\n",
      "Epoch 47/200\n",
      "768/768 [==============================] - 0s 468us/step - loss: 0.3585 - acc: 0.8633 - val_loss: 0.5254 - val_acc: 0.7812\n",
      "Epoch 48/200\n",
      "768/768 [==============================] - 0s 482us/step - loss: 0.3858 - acc: 0.8451 - val_loss: 0.5277 - val_acc: 0.7969\n",
      "Epoch 49/200\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.3539 - acc: 0.8724 - val_loss: 0.5379 - val_acc: 0.7760\n",
      "Epoch 50/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.3660 - acc: 0.8620 - val_loss: 0.5199 - val_acc: 0.7865\n",
      "Epoch 51/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.3746 - acc: 0.8607 - val_loss: 0.5204 - val_acc: 0.7812\n",
      "Epoch 52/200\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.3613 - acc: 0.8633 - val_loss: 0.5454 - val_acc: 0.7656\n",
      "Epoch 53/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.3531 - acc: 0.8659 - val_loss: 0.5298 - val_acc: 0.7917\n",
      "Epoch 54/200\n",
      "768/768 [==============================] - 0s 423us/step - loss: 0.3596 - acc: 0.8633 - val_loss: 0.5900 - val_acc: 0.7812\n",
      "Epoch 55/200\n",
      "768/768 [==============================] - 0s 411us/step - loss: 0.3522 - acc: 0.8659 - val_loss: 0.5116 - val_acc: 0.8073\n",
      "Epoch 56/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.3702 - acc: 0.8542 - val_loss: 0.5835 - val_acc: 0.7500\n",
      "Epoch 57/200\n",
      "768/768 [==============================] - 0s 491us/step - loss: 0.3865 - acc: 0.8516 - val_loss: 0.5799 - val_acc: 0.7604\n",
      "Epoch 58/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.3615 - acc: 0.8633 - val_loss: 0.4721 - val_acc: 0.7969\n",
      "Epoch 59/200\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.3494 - acc: 0.8789 - val_loss: 0.6008 - val_acc: 0.7500\n",
      "Epoch 60/200\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.3661 - acc: 0.8438 - val_loss: 0.5804 - val_acc: 0.7708\n",
      "Epoch 61/200\n",
      "768/768 [==============================] - 0s 469us/step - loss: 0.3555 - acc: 0.8659 - val_loss: 0.5062 - val_acc: 0.7969\n",
      "Epoch 62/200\n",
      "768/768 [==============================] - 0s 423us/step - loss: 0.3551 - acc: 0.8711 - val_loss: 0.5670 - val_acc: 0.7760\n",
      "Epoch 63/200\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.3482 - acc: 0.8672 - val_loss: 0.5510 - val_acc: 0.7708\n",
      "Epoch 64/200\n",
      "768/768 [==============================] - 0s 467us/step - loss: 0.3528 - acc: 0.8672 - val_loss: 0.5067 - val_acc: 0.8281\n",
      "Epoch 65/200\n",
      "768/768 [==============================] - 0s 455us/step - loss: 0.3700 - acc: 0.8594 - val_loss: 0.5054 - val_acc: 0.7969\n",
      "Epoch 66/200\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.3474 - acc: 0.8698 - val_loss: 0.5206 - val_acc: 0.8021\n",
      "Epoch 67/200\n",
      "768/768 [==============================] - 0s 490us/step - loss: 0.3353 - acc: 0.8737 - val_loss: 0.6421 - val_acc: 0.7708\n",
      "Epoch 68/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.3591 - acc: 0.8659 - val_loss: 0.5232 - val_acc: 0.8021\n",
      "Epoch 69/200\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.3431 - acc: 0.8711 - val_loss: 0.5469 - val_acc: 0.7969\n",
      "Epoch 70/200\n",
      "768/768 [==============================] - 0s 477us/step - loss: 0.3432 - acc: 0.8724 - val_loss: 0.5013 - val_acc: 0.7917\n",
      "Epoch 71/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.3539 - acc: 0.8789 - val_loss: 0.5230 - val_acc: 0.8073\n",
      "Epoch 72/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.3365 - acc: 0.8854 - val_loss: 0.5181 - val_acc: 0.8073\n",
      "Epoch 73/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.3380 - acc: 0.8776 - val_loss: 0.4907 - val_acc: 0.8281\n",
      "Epoch 74/200\n",
      "768/768 [==============================] - 0s 463us/step - loss: 0.3415 - acc: 0.8750 - val_loss: 0.5642 - val_acc: 0.7708\n",
      "Epoch 75/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.3542 - acc: 0.8763 - val_loss: 0.5364 - val_acc: 0.8021\n",
      "Epoch 76/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.3776 - acc: 0.8529 - val_loss: 0.5856 - val_acc: 0.7656\n",
      "Epoch 77/200\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.3679 - acc: 0.8555 - val_loss: 0.4957 - val_acc: 0.8125\n",
      "Epoch 78/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.3448 - acc: 0.8698 - val_loss: 0.5400 - val_acc: 0.7917\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 487us/step - loss: 0.3542 - acc: 0.8776 - val_loss: 0.5092 - val_acc: 0.8073\n",
      "Epoch 80/200\n",
      "768/768 [==============================] - 0s 504us/step - loss: 0.3360 - acc: 0.8724 - val_loss: 0.5056 - val_acc: 0.8229\n",
      "Epoch 81/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.3220 - acc: 0.8828 - val_loss: 0.5977 - val_acc: 0.7604\n",
      "Epoch 82/200\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.3428 - acc: 0.8659 - val_loss: 0.5614 - val_acc: 0.7812\n",
      "Epoch 83/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.3685 - acc: 0.8490 - val_loss: 0.5543 - val_acc: 0.7552\n",
      "Epoch 84/200\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.3489 - acc: 0.8763 - val_loss: 0.5278 - val_acc: 0.7917\n",
      "Epoch 85/200\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.3413 - acc: 0.8724 - val_loss: 0.5526 - val_acc: 0.7812\n",
      "Epoch 86/200\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.3484 - acc: 0.8724 - val_loss: 0.5475 - val_acc: 0.7865\n",
      "Epoch 87/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.3403 - acc: 0.8750 - val_loss: 0.5329 - val_acc: 0.8125\n",
      "Epoch 88/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.3248 - acc: 0.8867 - val_loss: 0.5453 - val_acc: 0.7812\n",
      "Epoch 89/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.3327 - acc: 0.8893 - val_loss: 0.5212 - val_acc: 0.8021\n",
      "Epoch 90/200\n",
      "768/768 [==============================] - 0s 474us/step - loss: 0.3380 - acc: 0.8802 - val_loss: 0.5299 - val_acc: 0.8021\n",
      "Epoch 91/200\n",
      "768/768 [==============================] - 0s 471us/step - loss: 0.3288 - acc: 0.8841 - val_loss: 0.6314 - val_acc: 0.7708\n",
      "Epoch 92/200\n",
      "768/768 [==============================] - 0s 491us/step - loss: 0.3391 - acc: 0.8750 - val_loss: 0.5991 - val_acc: 0.7708\n",
      "Epoch 93/200\n",
      "768/768 [==============================] - 0s 461us/step - loss: 0.3530 - acc: 0.8659 - val_loss: 0.4985 - val_acc: 0.7969\n",
      "Epoch 94/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.3399 - acc: 0.8763 - val_loss: 0.5749 - val_acc: 0.7865\n",
      "Epoch 95/200\n",
      "768/768 [==============================] - 0s 422us/step - loss: 0.3358 - acc: 0.8776 - val_loss: 0.5450 - val_acc: 0.8073\n",
      "Epoch 96/200\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.3377 - acc: 0.8789 - val_loss: 0.5312 - val_acc: 0.8021\n",
      "Epoch 97/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.3588 - acc: 0.8581 - val_loss: 0.4866 - val_acc: 0.8125\n",
      "Epoch 98/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.3430 - acc: 0.8724 - val_loss: 0.4989 - val_acc: 0.8073\n",
      "Epoch 99/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.3404 - acc: 0.8763 - val_loss: 0.5528 - val_acc: 0.7969\n",
      "Epoch 100/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.3326 - acc: 0.8750 - val_loss: 0.4980 - val_acc: 0.8281\n",
      "Epoch 101/200\n",
      "768/768 [==============================] - 0s 476us/step - loss: 0.3240 - acc: 0.8854 - val_loss: 0.6988 - val_acc: 0.7500\n",
      "Epoch 102/200\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.3632 - acc: 0.8594 - val_loss: 0.5886 - val_acc: 0.7240\n",
      "Epoch 103/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.3821 - acc: 0.8490 - val_loss: 0.5463 - val_acc: 0.7865\n",
      "Epoch 104/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.3396 - acc: 0.8737 - val_loss: 0.6415 - val_acc: 0.7500\n",
      "Epoch 105/200\n",
      "768/768 [==============================] - 0s 419us/step - loss: 0.3350 - acc: 0.8737 - val_loss: 0.5469 - val_acc: 0.8021\n",
      "Epoch 106/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.3311 - acc: 0.8841 - val_loss: 0.5065 - val_acc: 0.8073\n",
      "Epoch 107/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.3349 - acc: 0.8815 - val_loss: 0.5483 - val_acc: 0.7917\n",
      "Epoch 108/200\n",
      "768/768 [==============================] - 0s 464us/step - loss: 0.3197 - acc: 0.8828 - val_loss: 0.5358 - val_acc: 0.7969\n",
      "Epoch 109/200\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.3318 - acc: 0.8763 - val_loss: 0.5633 - val_acc: 0.7760\n",
      "Epoch 110/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.3360 - acc: 0.8737 - val_loss: 0.5404 - val_acc: 0.8073\n",
      "Epoch 111/200\n",
      "768/768 [==============================] - 0s 424us/step - loss: 0.3121 - acc: 0.8919 - val_loss: 0.6078 - val_acc: 0.7812\n",
      "Epoch 112/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.3178 - acc: 0.8802 - val_loss: 0.5366 - val_acc: 0.7917\n",
      "Epoch 113/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.3198 - acc: 0.8919 - val_loss: 0.6572 - val_acc: 0.7760\n",
      "Epoch 114/200\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.3363 - acc: 0.8763 - val_loss: 0.6668 - val_acc: 0.7552\n",
      "Epoch 115/200\n",
      "768/768 [==============================] - 0s 453us/step - loss: 0.3803 - acc: 0.8464 - val_loss: 0.5548 - val_acc: 0.7865\n",
      "Epoch 116/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.3540 - acc: 0.8672 - val_loss: 0.5123 - val_acc: 0.7917\n",
      "Epoch 117/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.3307 - acc: 0.8828 - val_loss: 0.5096 - val_acc: 0.8021\n",
      "Epoch 118/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.3293 - acc: 0.8737 - val_loss: 0.5806 - val_acc: 0.7760\n",
      "Epoch 119/200\n",
      "768/768 [==============================] - 0s 465us/step - loss: 0.3542 - acc: 0.8555 - val_loss: 0.5037 - val_acc: 0.8281\n",
      "Epoch 120/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.3302 - acc: 0.8815 - val_loss: 0.4764 - val_acc: 0.8281\n",
      "Epoch 121/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.3363 - acc: 0.8789 - val_loss: 0.5445 - val_acc: 0.7760\n",
      "Epoch 122/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.3346 - acc: 0.8815 - val_loss: 0.5790 - val_acc: 0.7812\n",
      "Epoch 123/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.3123 - acc: 0.8867 - val_loss: 0.6710 - val_acc: 0.7656\n",
      "Epoch 124/200\n",
      "768/768 [==============================] - 0s 425us/step - loss: 0.3327 - acc: 0.8750 - val_loss: 0.5626 - val_acc: 0.7708\n",
      "Epoch 125/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.3428 - acc: 0.8646 - val_loss: 0.5097 - val_acc: 0.7969\n",
      "Epoch 126/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.3295 - acc: 0.8789 - val_loss: 0.6059 - val_acc: 0.7760\n",
      "Epoch 127/200\n",
      "768/768 [==============================] - 0s 419us/step - loss: 0.3772 - acc: 0.8464 - val_loss: 0.5446 - val_acc: 0.7969\n",
      "Epoch 128/200\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.3798 - acc: 0.8620 - val_loss: 0.5811 - val_acc: 0.7812\n",
      "Epoch 129/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.3357 - acc: 0.8737 - val_loss: 0.5784 - val_acc: 0.7656\n",
      "Epoch 130/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.3406 - acc: 0.8789 - val_loss: 0.5689 - val_acc: 0.7760\n",
      "Epoch 131/200\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.3266 - acc: 0.8841 - val_loss: 0.5471 - val_acc: 0.7917\n",
      "Epoch 132/200\n",
      "768/768 [==============================] - 0s 471us/step - loss: 0.3241 - acc: 0.8867 - val_loss: 0.5673 - val_acc: 0.7812\n",
      "Epoch 133/200\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.3104 - acc: 0.8854 - val_loss: 0.6141 - val_acc: 0.7917\n",
      "Epoch 134/200\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.3292 - acc: 0.8776 - val_loss: 0.5145 - val_acc: 0.7917\n",
      "Epoch 135/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.3589 - acc: 0.8646 - val_loss: 0.4863 - val_acc: 0.8125\n",
      "Epoch 136/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.3417 - acc: 0.8685 - val_loss: 0.5707 - val_acc: 0.7865\n",
      "Epoch 137/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.3134 - acc: 0.8867 - val_loss: 0.5341 - val_acc: 0.7865\n",
      "Epoch 138/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.3050 - acc: 0.8906 - val_loss: 0.5873 - val_acc: 0.7812\n",
      "Epoch 139/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 462us/step - loss: 0.3168 - acc: 0.8815 - val_loss: 0.5874 - val_acc: 0.7708\n",
      "Epoch 140/200\n",
      "768/768 [==============================] - 0s 464us/step - loss: 0.3232 - acc: 0.8828 - val_loss: 0.6609 - val_acc: 0.7500\n",
      "Epoch 141/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.3461 - acc: 0.8620 - val_loss: 0.6734 - val_acc: 0.7344\n",
      "Epoch 142/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.3486 - acc: 0.8685 - val_loss: 0.4744 - val_acc: 0.8281\n",
      "Epoch 143/200\n",
      "768/768 [==============================] - 0s 476us/step - loss: 0.3228 - acc: 0.8841 - val_loss: 0.5310 - val_acc: 0.7812\n",
      "Epoch 144/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.3250 - acc: 0.8841 - val_loss: 0.5875 - val_acc: 0.7812\n",
      "Epoch 145/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.2991 - acc: 0.8932 - val_loss: 0.6030 - val_acc: 0.7865\n",
      "Epoch 146/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.3230 - acc: 0.8815 - val_loss: 0.5521 - val_acc: 0.7865\n",
      "Epoch 147/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.3134 - acc: 0.8789 - val_loss: 0.5353 - val_acc: 0.8021\n",
      "Epoch 148/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.3109 - acc: 0.8906 - val_loss: 0.6071 - val_acc: 0.7917\n",
      "Epoch 149/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.3265 - acc: 0.8776 - val_loss: 0.5597 - val_acc: 0.7760\n",
      "Epoch 150/200\n",
      "768/768 [==============================] - 0s 466us/step - loss: 0.3144 - acc: 0.8854 - val_loss: 0.6156 - val_acc: 0.7604\n",
      "Epoch 151/200\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.3574 - acc: 0.8672 - val_loss: 0.5960 - val_acc: 0.7812\n",
      "Epoch 152/200\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.3320 - acc: 0.8763 - val_loss: 0.5135 - val_acc: 0.8177\n",
      "Epoch 153/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.3267 - acc: 0.8815 - val_loss: 0.5023 - val_acc: 0.8229\n",
      "Epoch 154/200\n",
      "768/768 [==============================] - 0s 489us/step - loss: 0.3663 - acc: 0.8581 - val_loss: 0.6004 - val_acc: 0.7865\n",
      "Epoch 155/200\n",
      "768/768 [==============================] - 0s 467us/step - loss: 0.3585 - acc: 0.8594 - val_loss: 0.5229 - val_acc: 0.8125\n",
      "Epoch 156/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.3212 - acc: 0.8724 - val_loss: 0.5230 - val_acc: 0.7969\n",
      "Epoch 157/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.3257 - acc: 0.8828 - val_loss: 0.5487 - val_acc: 0.8021\n",
      "Epoch 158/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.3410 - acc: 0.8763 - val_loss: 0.5150 - val_acc: 0.8073\n",
      "Epoch 159/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.3104 - acc: 0.8932 - val_loss: 0.5716 - val_acc: 0.7812\n",
      "Epoch 160/200\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.2918 - acc: 0.9036 - val_loss: 0.6182 - val_acc: 0.7812\n",
      "Epoch 161/200\n",
      "768/768 [==============================] - 0s 464us/step - loss: 0.2957 - acc: 0.8971 - val_loss: 0.5633 - val_acc: 0.7708\n",
      "Epoch 162/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.2987 - acc: 0.8958 - val_loss: 0.6531 - val_acc: 0.7604\n",
      "Epoch 163/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.3073 - acc: 0.8880 - val_loss: 0.5613 - val_acc: 0.7812\n",
      "Epoch 164/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.3019 - acc: 0.8958 - val_loss: 0.5962 - val_acc: 0.7812\n",
      "Epoch 165/200\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.2929 - acc: 0.8958 - val_loss: 0.5715 - val_acc: 0.7917\n",
      "Epoch 166/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.2992 - acc: 0.8919 - val_loss: 0.6355 - val_acc: 0.7604\n",
      "Epoch 167/200\n",
      "768/768 [==============================] - 0s 502us/step - loss: 0.2883 - acc: 0.8984 - val_loss: 0.6211 - val_acc: 0.7708\n",
      "Epoch 168/200\n",
      "768/768 [==============================] - 0s 484us/step - loss: 0.3005 - acc: 0.8880 - val_loss: 0.6708 - val_acc: 0.7500\n",
      "Epoch 169/200\n",
      "768/768 [==============================] - 0s 466us/step - loss: 0.3256 - acc: 0.8737 - val_loss: 0.5739 - val_acc: 0.7708\n",
      "Epoch 170/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.3216 - acc: 0.8789 - val_loss: 0.6109 - val_acc: 0.7708\n",
      "Epoch 171/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.3109 - acc: 0.8906 - val_loss: 0.7083 - val_acc: 0.7500\n",
      "Epoch 172/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.3070 - acc: 0.8841 - val_loss: 0.6554 - val_acc: 0.7604\n",
      "Epoch 173/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.3110 - acc: 0.8893 - val_loss: 0.6236 - val_acc: 0.7708\n",
      "Epoch 174/200\n",
      "768/768 [==============================] - 0s 465us/step - loss: 0.3226 - acc: 0.8789 - val_loss: 0.6041 - val_acc: 0.7708\n",
      "Epoch 175/200\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.4058 - acc: 0.8451 - val_loss: 0.5200 - val_acc: 0.8125\n",
      "Epoch 176/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.3359 - acc: 0.8789 - val_loss: 0.5467 - val_acc: 0.7760\n",
      "Epoch 177/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.3409 - acc: 0.8594 - val_loss: 0.5762 - val_acc: 0.7708\n",
      "Epoch 178/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.3292 - acc: 0.8854 - val_loss: 0.5804 - val_acc: 0.7917\n",
      "Epoch 179/200\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.3328 - acc: 0.8672 - val_loss: 0.5501 - val_acc: 0.7917\n",
      "Epoch 180/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.3007 - acc: 0.8919 - val_loss: 0.6715 - val_acc: 0.7708\n",
      "Epoch 181/200\n",
      "768/768 [==============================] - 0s 453us/step - loss: 0.3002 - acc: 0.8880 - val_loss: 0.6186 - val_acc: 0.7812\n",
      "Epoch 182/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.3303 - acc: 0.8724 - val_loss: 0.5828 - val_acc: 0.7865\n",
      "Epoch 183/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.3040 - acc: 0.8893 - val_loss: 0.6038 - val_acc: 0.7812\n",
      "Epoch 184/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.2875 - acc: 0.8971 - val_loss: 0.6063 - val_acc: 0.7760\n",
      "Epoch 185/200\n",
      "768/768 [==============================] - 0s 463us/step - loss: 0.2762 - acc: 0.8997 - val_loss: 0.6353 - val_acc: 0.7708\n",
      "Epoch 186/200\n",
      "768/768 [==============================] - 0s 464us/step - loss: 0.2999 - acc: 0.8919 - val_loss: 0.5353 - val_acc: 0.8177\n",
      "Epoch 187/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.3158 - acc: 0.8750 - val_loss: 0.5355 - val_acc: 0.8021\n",
      "Epoch 188/200\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.3047 - acc: 0.8893 - val_loss: 0.5262 - val_acc: 0.8073\n",
      "Epoch 189/200\n",
      "768/768 [==============================] - 0s 471us/step - loss: 0.3171 - acc: 0.8880 - val_loss: 0.5260 - val_acc: 0.7969\n",
      "Epoch 190/200\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.2974 - acc: 0.8932 - val_loss: 0.6776 - val_acc: 0.7552\n",
      "Epoch 191/200\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.3152 - acc: 0.8789 - val_loss: 0.6724 - val_acc: 0.7448\n",
      "Epoch 192/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.3138 - acc: 0.8867 - val_loss: 0.5753 - val_acc: 0.7917\n",
      "Epoch 193/200\n",
      "768/768 [==============================] - 0s 425us/step - loss: 0.3281 - acc: 0.8685 - val_loss: 0.5335 - val_acc: 0.7917\n",
      "Epoch 194/200\n",
      "768/768 [==============================] - 0s 463us/step - loss: 0.3422 - acc: 0.8724 - val_loss: 0.5943 - val_acc: 0.7656\n",
      "Epoch 195/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.2974 - acc: 0.8919 - val_loss: 0.5689 - val_acc: 0.7917\n",
      "Epoch 196/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.2799 - acc: 0.9076 - val_loss: 0.5929 - val_acc: 0.7812\n",
      "Epoch 197/200\n",
      "768/768 [==============================] - 0s 467us/step - loss: 0.2765 - acc: 0.9062 - val_loss: 0.6717 - val_acc: 0.7656\n",
      "Epoch 198/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.2967 - acc: 0.8906 - val_loss: 0.9211 - val_acc: 0.7344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.3661 - acc: 0.8633 - val_loss: 0.5764 - val_acc: 0.7969\n",
      "Epoch 200/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.3022 - acc: 0.8880 - val_loss: 0.6018 - val_acc: 0.7760\n",
      "Predicting...\n",
      "Cross val iteration 4 of 5\n",
      "Fitting...\n",
      "Train on 768 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "768/768 [==============================] - 6s 8ms/step - loss: 0.3304 - acc: 0.8802 - val_loss: 0.5413 - val_acc: 0.7969\n",
      "Epoch 2/200\n",
      "768/768 [==============================] - 0s 421us/step - loss: 0.2981 - acc: 0.8893 - val_loss: 0.5780 - val_acc: 0.7917\n",
      "Epoch 3/200\n",
      "768/768 [==============================] - 0s 461us/step - loss: 0.2962 - acc: 0.8893 - val_loss: 0.7688 - val_acc: 0.7448\n",
      "Epoch 4/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.3246 - acc: 0.8750 - val_loss: 0.6323 - val_acc: 0.7760\n",
      "Epoch 5/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.2855 - acc: 0.9115 - val_loss: 0.6622 - val_acc: 0.7812\n",
      "Epoch 6/200\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.3176 - acc: 0.8763 - val_loss: 0.5711 - val_acc: 0.7708\n",
      "Epoch 7/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.3345 - acc: 0.8724 - val_loss: 0.6183 - val_acc: 0.7656\n",
      "Epoch 8/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.3140 - acc: 0.8802 - val_loss: 0.6778 - val_acc: 0.7500\n",
      "Epoch 9/200\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.3284 - acc: 0.8685 - val_loss: 0.5623 - val_acc: 0.7865\n",
      "Epoch 10/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.3011 - acc: 0.8932 - val_loss: 0.5870 - val_acc: 0.7865\n",
      "Epoch 11/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.3263 - acc: 0.8789 - val_loss: 0.6090 - val_acc: 0.7708\n",
      "Epoch 12/200\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.2845 - acc: 0.9010 - val_loss: 0.5830 - val_acc: 0.7865\n",
      "Epoch 13/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.2764 - acc: 0.9036 - val_loss: 0.6489 - val_acc: 0.7656\n",
      "Epoch 14/200\n",
      "768/768 [==============================] - 0s 472us/step - loss: 0.2829 - acc: 0.8958 - val_loss: 0.7421 - val_acc: 0.7448\n",
      "Epoch 15/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.2801 - acc: 0.8919 - val_loss: 0.6485 - val_acc: 0.7760\n",
      "Epoch 16/200\n",
      "768/768 [==============================] - 0s 422us/step - loss: 0.2730 - acc: 0.9049 - val_loss: 0.6770 - val_acc: 0.7552\n",
      "Epoch 17/200\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.2816 - acc: 0.8958 - val_loss: 0.6412 - val_acc: 0.7917\n",
      "Epoch 18/200\n",
      "768/768 [==============================] - 0s 456us/step - loss: 0.2884 - acc: 0.8945 - val_loss: 0.5819 - val_acc: 0.7760\n",
      "Epoch 19/200\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.2791 - acc: 0.8997 - val_loss: 0.6808 - val_acc: 0.7812\n",
      "Epoch 20/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.2677 - acc: 0.9089 - val_loss: 0.6051 - val_acc: 0.7708\n",
      "Epoch 21/200\n",
      "768/768 [==============================] - 0s 473us/step - loss: 0.2919 - acc: 0.8984 - val_loss: 0.6611 - val_acc: 0.7812\n",
      "Epoch 22/200\n",
      "768/768 [==============================] - 0s 478us/step - loss: 0.2854 - acc: 0.8945 - val_loss: 0.7028 - val_acc: 0.7604\n",
      "Epoch 23/200\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.2757 - acc: 0.9089 - val_loss: 0.6556 - val_acc: 0.7760\n",
      "Epoch 24/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.2665 - acc: 0.9062 - val_loss: 0.7535 - val_acc: 0.7552\n",
      "Epoch 25/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.2821 - acc: 0.8958 - val_loss: 0.6012 - val_acc: 0.7969\n",
      "Epoch 26/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.2851 - acc: 0.8984 - val_loss: 0.6715 - val_acc: 0.7708\n",
      "Epoch 27/200\n",
      "768/768 [==============================] - 0s 415us/step - loss: 0.2893 - acc: 0.8880 - val_loss: 0.6514 - val_acc: 0.7604\n",
      "Epoch 28/200\n",
      "768/768 [==============================] - 0s 461us/step - loss: 0.2692 - acc: 0.9076 - val_loss: 0.6800 - val_acc: 0.7812\n",
      "Epoch 29/200\n",
      "768/768 [==============================] - 0s 414us/step - loss: 0.2671 - acc: 0.9089 - val_loss: 0.7173 - val_acc: 0.7656\n",
      "Epoch 30/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.2787 - acc: 0.8984 - val_loss: 0.6616 - val_acc: 0.7656\n",
      "Epoch 31/200\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.2841 - acc: 0.8971 - val_loss: 0.7429 - val_acc: 0.7656\n",
      "Epoch 32/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.3156 - acc: 0.8724 - val_loss: 0.5644 - val_acc: 0.8021\n",
      "Epoch 33/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.3118 - acc: 0.8828 - val_loss: 0.5297 - val_acc: 0.7969\n",
      "Epoch 34/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.3558 - acc: 0.8581 - val_loss: 0.5673 - val_acc: 0.8073\n",
      "Epoch 35/200\n",
      "768/768 [==============================] - 0s 485us/step - loss: 0.3427 - acc: 0.8737 - val_loss: 0.5463 - val_acc: 0.7812\n",
      "Epoch 36/200\n",
      "768/768 [==============================] - 0s 424us/step - loss: 0.3987 - acc: 0.8438 - val_loss: 0.6178 - val_acc: 0.7656\n",
      "Epoch 37/200\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.3006 - acc: 0.8893 - val_loss: 0.6545 - val_acc: 0.7552\n",
      "Epoch 38/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.2917 - acc: 0.8867 - val_loss: 0.5785 - val_acc: 0.7865\n",
      "Epoch 39/200\n",
      "768/768 [==============================] - 0s 423us/step - loss: 0.2771 - acc: 0.9062 - val_loss: 0.6362 - val_acc: 0.7865\n",
      "Epoch 40/200\n",
      "768/768 [==============================] - 0s 420us/step - loss: 0.2676 - acc: 0.9049 - val_loss: 0.6085 - val_acc: 0.7812\n",
      "Epoch 41/200\n",
      "768/768 [==============================] - 0s 421us/step - loss: 0.2831 - acc: 0.8945 - val_loss: 0.6155 - val_acc: 0.7812\n",
      "Epoch 42/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.2770 - acc: 0.8984 - val_loss: 0.6566 - val_acc: 0.7760\n",
      "Epoch 43/200\n",
      "768/768 [==============================] - 0s 476us/step - loss: 0.2838 - acc: 0.8919 - val_loss: 0.6353 - val_acc: 0.7656\n",
      "Epoch 44/200\n",
      "768/768 [==============================] - 0s 463us/step - loss: 0.2721 - acc: 0.9023 - val_loss: 0.7296 - val_acc: 0.7812\n",
      "Epoch 45/200\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.2723 - acc: 0.8997 - val_loss: 0.6439 - val_acc: 0.7917\n",
      "Epoch 46/200\n",
      "768/768 [==============================] - 0s 424us/step - loss: 0.2731 - acc: 0.9049 - val_loss: 0.6558 - val_acc: 0.8021\n",
      "Epoch 47/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.2758 - acc: 0.8984 - val_loss: 0.6796 - val_acc: 0.7708\n",
      "Epoch 48/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.2656 - acc: 0.8971 - val_loss: 0.6501 - val_acc: 0.7917\n",
      "Epoch 49/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.3098 - acc: 0.8763 - val_loss: 0.7420 - val_acc: 0.7604\n",
      "Epoch 50/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.3222 - acc: 0.8724 - val_loss: 0.5393 - val_acc: 0.7917\n",
      "Epoch 51/200\n",
      "768/768 [==============================] - 0s 466us/step - loss: 0.3358 - acc: 0.8698 - val_loss: 0.6952 - val_acc: 0.7656\n",
      "Epoch 52/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.2782 - acc: 0.8945 - val_loss: 0.7096 - val_acc: 0.7708\n",
      "Epoch 53/200\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.2975 - acc: 0.8789 - val_loss: 0.7586 - val_acc: 0.7396\n",
      "Epoch 54/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.2874 - acc: 0.8919 - val_loss: 0.6798 - val_acc: 0.7708\n",
      "Epoch 55/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.2973 - acc: 0.8841 - val_loss: 0.6233 - val_acc: 0.7865\n",
      "Epoch 56/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.2740 - acc: 0.9062 - val_loss: 0.5477 - val_acc: 0.8229\n",
      "Epoch 57/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.2748 - acc: 0.9062 - val_loss: 0.6542 - val_acc: 0.7760\n",
      "Epoch 58/200\n",
      "768/768 [==============================] - 0s 453us/step - loss: 0.2602 - acc: 0.9076 - val_loss: 0.7510 - val_acc: 0.7604\n",
      "Epoch 59/200\n",
      "768/768 [==============================] - 0s 423us/step - loss: 0.2709 - acc: 0.8945 - val_loss: 0.6984 - val_acc: 0.7865\n",
      "Epoch 60/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.2834 - acc: 0.8958 - val_loss: 0.6356 - val_acc: 0.7865\n",
      "Epoch 61/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.2891 - acc: 0.8932 - val_loss: 0.6689 - val_acc: 0.7656\n",
      "Epoch 62/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.2933 - acc: 0.8919 - val_loss: 0.6324 - val_acc: 0.7760\n",
      "Epoch 63/200\n",
      "768/768 [==============================] - 0s 480us/step - loss: 0.2666 - acc: 0.8997 - val_loss: 0.6646 - val_acc: 0.7708\n",
      "Epoch 64/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.2580 - acc: 0.9102 - val_loss: 0.7051 - val_acc: 0.7760\n",
      "Epoch 65/200\n",
      "768/768 [==============================] - 0s 456us/step - loss: 0.2566 - acc: 0.9115 - val_loss: 0.7426 - val_acc: 0.7812\n",
      "Epoch 66/200\n",
      "768/768 [==============================] - 0s 468us/step - loss: 0.2846 - acc: 0.8841 - val_loss: 0.6526 - val_acc: 0.7760\n",
      "Epoch 67/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.3116 - acc: 0.8802 - val_loss: 0.7063 - val_acc: 0.7812\n",
      "Epoch 68/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.2989 - acc: 0.8919 - val_loss: 0.7701 - val_acc: 0.7708\n",
      "Epoch 69/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.2584 - acc: 0.9102 - val_loss: 0.7320 - val_acc: 0.7760\n",
      "Epoch 70/200\n",
      "768/768 [==============================] - 0s 463us/step - loss: 0.2545 - acc: 0.9128 - val_loss: 0.7689 - val_acc: 0.7760\n",
      "Epoch 71/200\n",
      "768/768 [==============================] - 0s 464us/step - loss: 0.3144 - acc: 0.8841 - val_loss: 0.6877 - val_acc: 0.7604\n",
      "Epoch 72/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.3062 - acc: 0.8906 - val_loss: 0.5534 - val_acc: 0.7969\n",
      "Epoch 73/200\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.2790 - acc: 0.8919 - val_loss: 0.6575 - val_acc: 0.7812\n",
      "Epoch 74/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.2828 - acc: 0.9023 - val_loss: 0.8133 - val_acc: 0.7240\n",
      "Epoch 75/200\n",
      "768/768 [==============================] - 0s 428us/step - loss: 0.2842 - acc: 0.8984 - val_loss: 0.7614 - val_acc: 0.7760\n",
      "Epoch 76/200\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.2917 - acc: 0.8854 - val_loss: 0.6592 - val_acc: 0.7812\n",
      "Epoch 77/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.2752 - acc: 0.8932 - val_loss: 0.6780 - val_acc: 0.7760\n",
      "Epoch 78/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.2975 - acc: 0.8789 - val_loss: 0.6413 - val_acc: 0.7604\n",
      "Epoch 79/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.2794 - acc: 0.8932 - val_loss: 0.6608 - val_acc: 0.7969\n",
      "Epoch 80/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.2851 - acc: 0.8971 - val_loss: 0.5849 - val_acc: 0.7812\n",
      "Epoch 81/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.3406 - acc: 0.8633 - val_loss: 0.5954 - val_acc: 0.7917\n",
      "Epoch 82/200\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.2748 - acc: 0.9049 - val_loss: 0.5768 - val_acc: 0.7917\n",
      "Epoch 83/200\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.2607 - acc: 0.9076 - val_loss: 0.6782 - val_acc: 0.7760\n",
      "Epoch 84/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.2982 - acc: 0.8854 - val_loss: 0.6399 - val_acc: 0.7760\n",
      "Epoch 85/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.2629 - acc: 0.9076 - val_loss: 0.6721 - val_acc: 0.8021\n",
      "Epoch 86/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.2528 - acc: 0.9115 - val_loss: 0.6753 - val_acc: 0.7865\n",
      "Epoch 87/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.2422 - acc: 0.9128 - val_loss: 0.7588 - val_acc: 0.7917\n",
      "Epoch 88/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.2821 - acc: 0.8867 - val_loss: 0.6604 - val_acc: 0.7865\n",
      "Epoch 89/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.2859 - acc: 0.8932 - val_loss: 0.7305 - val_acc: 0.7812\n",
      "Epoch 90/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.2953 - acc: 0.8828 - val_loss: 0.7024 - val_acc: 0.7812\n",
      "Epoch 91/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.3036 - acc: 0.8841 - val_loss: 0.7444 - val_acc: 0.7552\n",
      "Epoch 92/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.2661 - acc: 0.8984 - val_loss: 0.7589 - val_acc: 0.7760\n",
      "Epoch 93/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.3058 - acc: 0.8854 - val_loss: 0.5926 - val_acc: 0.7812\n",
      "Epoch 94/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.2682 - acc: 0.9036 - val_loss: 0.6953 - val_acc: 0.7708\n",
      "Epoch 95/200\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.2517 - acc: 0.9115 - val_loss: 0.7423 - val_acc: 0.7708\n",
      "Epoch 96/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.2878 - acc: 0.8932 - val_loss: 0.7525 - val_acc: 0.7656\n",
      "Epoch 97/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.2792 - acc: 0.8919 - val_loss: 0.6652 - val_acc: 0.7865\n",
      "Epoch 98/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.2837 - acc: 0.8958 - val_loss: 0.7050 - val_acc: 0.7760\n",
      "Epoch 99/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.2774 - acc: 0.8984 - val_loss: 0.7367 - val_acc: 0.7708\n",
      "Epoch 100/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.2439 - acc: 0.9167 - val_loss: 0.8069 - val_acc: 0.7760\n",
      "Epoch 101/200\n",
      "768/768 [==============================] - 0s 483us/step - loss: 0.2457 - acc: 0.9128 - val_loss: 0.6897 - val_acc: 0.7865\n",
      "Epoch 102/200\n",
      "768/768 [==============================] - 0s 478us/step - loss: 0.2687 - acc: 0.8971 - val_loss: 0.7439 - val_acc: 0.7760\n",
      "Epoch 103/200\n",
      "768/768 [==============================] - 0s 479us/step - loss: 0.2580 - acc: 0.9023 - val_loss: 0.7206 - val_acc: 0.7812\n",
      "Epoch 104/200\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.2329 - acc: 0.9180 - val_loss: 0.8304 - val_acc: 0.7865\n",
      "Epoch 105/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.2653 - acc: 0.9062 - val_loss: 0.8094 - val_acc: 0.7448\n",
      "Epoch 106/200\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.2584 - acc: 0.9115 - val_loss: 0.6650 - val_acc: 0.7708\n",
      "Epoch 107/200\n",
      "768/768 [==============================] - 0s 468us/step - loss: 0.2560 - acc: 0.9076 - val_loss: 0.6148 - val_acc: 0.7917\n",
      "Epoch 108/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.2692 - acc: 0.9010 - val_loss: 0.7777 - val_acc: 0.7708\n",
      "Epoch 109/200\n",
      "768/768 [==============================] - 0s 455us/step - loss: 0.2476 - acc: 0.9128 - val_loss: 0.7245 - val_acc: 0.7760\n",
      "Epoch 110/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.2521 - acc: 0.9010 - val_loss: 0.7543 - val_acc: 0.7604\n",
      "Epoch 111/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.3287 - acc: 0.8698 - val_loss: 0.6145 - val_acc: 0.7812\n",
      "Epoch 112/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.2708 - acc: 0.8958 - val_loss: 0.7212 - val_acc: 0.7708\n",
      "Epoch 113/200\n",
      "768/768 [==============================] - 0s 483us/step - loss: 0.2896 - acc: 0.8919 - val_loss: 0.6330 - val_acc: 0.7865\n",
      "Epoch 114/200\n",
      "768/768 [==============================] - 0s 480us/step - loss: 0.2833 - acc: 0.8867 - val_loss: 0.7766 - val_acc: 0.7865\n",
      "Epoch 115/200\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.2632 - acc: 0.9010 - val_loss: 0.7521 - val_acc: 0.7812\n",
      "Epoch 116/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.2431 - acc: 0.9193 - val_loss: 0.9160 - val_acc: 0.7708\n",
      "Epoch 117/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.2485 - acc: 0.9102 - val_loss: 0.7996 - val_acc: 0.7760\n",
      "Epoch 118/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 451us/step - loss: 0.2511 - acc: 0.9062 - val_loss: 0.6903 - val_acc: 0.7865\n",
      "Epoch 119/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.2465 - acc: 0.9115 - val_loss: 0.7532 - val_acc: 0.7656\n",
      "Epoch 120/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.2604 - acc: 0.8932 - val_loss: 0.6921 - val_acc: 0.7812\n",
      "Epoch 121/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.2661 - acc: 0.8958 - val_loss: 0.7352 - val_acc: 0.7917\n",
      "Epoch 122/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.2333 - acc: 0.9180 - val_loss: 0.7899 - val_acc: 0.7865\n",
      "Epoch 123/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.2467 - acc: 0.9076 - val_loss: 0.8445 - val_acc: 0.7656\n",
      "Epoch 124/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.2420 - acc: 0.9115 - val_loss: 0.9047 - val_acc: 0.7708\n",
      "Epoch 125/200\n",
      "768/768 [==============================] - 0s 417us/step - loss: 0.2559 - acc: 0.9036 - val_loss: 0.7036 - val_acc: 0.7656\n",
      "Epoch 126/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.2448 - acc: 0.9076 - val_loss: 0.7347 - val_acc: 0.7760\n",
      "Epoch 127/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.2271 - acc: 0.9167 - val_loss: 0.7546 - val_acc: 0.7760\n",
      "Epoch 128/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.2314 - acc: 0.9154 - val_loss: 0.7626 - val_acc: 0.7812\n",
      "Epoch 129/200\n",
      "768/768 [==============================] - 0s 425us/step - loss: 0.2336 - acc: 0.9102 - val_loss: 0.8389 - val_acc: 0.7292\n",
      "Epoch 130/200\n",
      "768/768 [==============================] - 0s 423us/step - loss: 0.2483 - acc: 0.9089 - val_loss: 0.7336 - val_acc: 0.7865\n",
      "Epoch 131/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.2361 - acc: 0.9128 - val_loss: 0.7298 - val_acc: 0.7865\n",
      "Epoch 132/200\n",
      "768/768 [==============================] - 0s 467us/step - loss: 0.2394 - acc: 0.9089 - val_loss: 0.8389 - val_acc: 0.7552\n",
      "Epoch 133/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.2510 - acc: 0.9076 - val_loss: 0.7583 - val_acc: 0.7708\n",
      "Epoch 134/200\n",
      "768/768 [==============================] - 0s 483us/step - loss: 0.2246 - acc: 0.9193 - val_loss: 0.7872 - val_acc: 0.7917\n",
      "Epoch 135/200\n",
      "768/768 [==============================] - 0s 461us/step - loss: 0.2404 - acc: 0.9115 - val_loss: 0.8374 - val_acc: 0.7917\n",
      "Epoch 136/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.2206 - acc: 0.9206 - val_loss: 0.7978 - val_acc: 0.7812\n",
      "Epoch 137/200\n",
      "768/768 [==============================] - 0s 455us/step - loss: 0.3224 - acc: 0.8763 - val_loss: 0.6637 - val_acc: 0.7760\n",
      "Epoch 138/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.3434 - acc: 0.8698 - val_loss: 0.8277 - val_acc: 0.7500\n",
      "Epoch 139/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.3043 - acc: 0.8750 - val_loss: 0.7737 - val_acc: 0.7708\n",
      "Epoch 140/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.2501 - acc: 0.9076 - val_loss: 0.7832 - val_acc: 0.7656\n",
      "Epoch 141/200\n",
      "768/768 [==============================] - 0s 478us/step - loss: 0.2531 - acc: 0.9076 - val_loss: 0.8496 - val_acc: 0.7500\n",
      "Epoch 142/200\n",
      "768/768 [==============================] - 0s 455us/step - loss: 0.2522 - acc: 0.9049 - val_loss: 0.8213 - val_acc: 0.7396\n",
      "Epoch 143/200\n",
      "768/768 [==============================] - 0s 424us/step - loss: 0.2669 - acc: 0.8958 - val_loss: 0.9210 - val_acc: 0.7500\n",
      "Epoch 144/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.3819 - acc: 0.8477 - val_loss: 0.6868 - val_acc: 0.7552\n",
      "Epoch 145/200\n",
      "768/768 [==============================] - 0s 472us/step - loss: 0.3652 - acc: 0.8620 - val_loss: 0.5844 - val_acc: 0.7760\n",
      "Epoch 146/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.2684 - acc: 0.9089 - val_loss: 0.6691 - val_acc: 0.7708\n",
      "Epoch 147/200\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.2312 - acc: 0.9219 - val_loss: 0.7142 - val_acc: 0.7708\n",
      "Epoch 148/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.2538 - acc: 0.9089 - val_loss: 0.7252 - val_acc: 0.7917\n",
      "Epoch 149/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.2443 - acc: 0.9102 - val_loss: 0.6885 - val_acc: 0.8073\n",
      "Epoch 150/200\n",
      "768/768 [==============================] - 0s 479us/step - loss: 0.2704 - acc: 0.9010 - val_loss: 0.6613 - val_acc: 0.7812\n",
      "Epoch 151/200\n",
      "768/768 [==============================] - 0s 487us/step - loss: 0.2806 - acc: 0.8932 - val_loss: 0.8529 - val_acc: 0.7656\n",
      "Epoch 152/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.2703 - acc: 0.8958 - val_loss: 0.6860 - val_acc: 0.7812\n",
      "Epoch 153/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.2525 - acc: 0.9076 - val_loss: 0.7023 - val_acc: 0.7917\n",
      "Epoch 154/200\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.2391 - acc: 0.9115 - val_loss: 0.7332 - val_acc: 0.7865\n",
      "Epoch 155/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.2394 - acc: 0.9049 - val_loss: 0.7458 - val_acc: 0.7760\n",
      "Epoch 156/200\n",
      "768/768 [==============================] - 0s 492us/step - loss: 0.2344 - acc: 0.9128 - val_loss: 0.7971 - val_acc: 0.7969\n",
      "Epoch 157/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.2406 - acc: 0.9141 - val_loss: 0.7304 - val_acc: 0.7865\n",
      "Epoch 158/200\n",
      "768/768 [==============================] - 0s 424us/step - loss: 0.2344 - acc: 0.9036 - val_loss: 0.7637 - val_acc: 0.7812\n",
      "Epoch 159/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.2334 - acc: 0.9115 - val_loss: 0.7764 - val_acc: 0.7708\n",
      "Epoch 160/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.2385 - acc: 0.9154 - val_loss: 0.8292 - val_acc: 0.7552\n",
      "Epoch 161/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.2454 - acc: 0.9089 - val_loss: 0.8774 - val_acc: 0.7552\n",
      "Epoch 162/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.2629 - acc: 0.8997 - val_loss: 0.7668 - val_acc: 0.7552\n",
      "Epoch 163/200\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.2622 - acc: 0.8932 - val_loss: 0.8644 - val_acc: 0.7604\n",
      "Epoch 164/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.2431 - acc: 0.9089 - val_loss: 0.7803 - val_acc: 0.7865\n",
      "Epoch 165/200\n",
      "768/768 [==============================] - 0s 413us/step - loss: 0.2156 - acc: 0.9206 - val_loss: 0.7689 - val_acc: 0.7812\n",
      "Epoch 166/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.2232 - acc: 0.9154 - val_loss: 0.8278 - val_acc: 0.7917\n",
      "Epoch 167/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.2240 - acc: 0.9180 - val_loss: 0.7781 - val_acc: 0.7812\n",
      "Epoch 168/200\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.2580 - acc: 0.9010 - val_loss: 0.7330 - val_acc: 0.7865\n",
      "Epoch 169/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.2185 - acc: 0.9206 - val_loss: 0.7901 - val_acc: 0.7969\n",
      "Epoch 170/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.2068 - acc: 0.9232 - val_loss: 0.8582 - val_acc: 0.7760\n",
      "Epoch 171/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.3386 - acc: 0.8698 - val_loss: 0.6837 - val_acc: 0.7448\n",
      "Epoch 172/200\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.3487 - acc: 0.8594 - val_loss: 0.7376 - val_acc: 0.7604\n",
      "Epoch 173/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.2957 - acc: 0.8841 - val_loss: 0.7303 - val_acc: 0.7708\n",
      "Epoch 174/200\n",
      "768/768 [==============================] - 0s 499us/step - loss: 0.2867 - acc: 0.8906 - val_loss: 0.8407 - val_acc: 0.7500\n",
      "Epoch 175/200\n",
      "768/768 [==============================] - 0s 418us/step - loss: 0.2443 - acc: 0.9102 - val_loss: 0.8225 - val_acc: 0.7812\n",
      "Epoch 176/200\n",
      "768/768 [==============================] - 0s 424us/step - loss: 0.2339 - acc: 0.9167 - val_loss: 0.7449 - val_acc: 0.7812\n",
      "Epoch 177/200\n",
      "768/768 [==============================] - 0s 420us/step - loss: 0.2397 - acc: 0.9102 - val_loss: 0.7468 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.2378 - acc: 0.9089 - val_loss: 0.7788 - val_acc: 0.7604\n",
      "Epoch 179/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.2422 - acc: 0.9193 - val_loss: 0.7308 - val_acc: 0.7917\n",
      "Epoch 180/200\n",
      "768/768 [==============================] - 0s 473us/step - loss: 0.2284 - acc: 0.9206 - val_loss: 0.7905 - val_acc: 0.7760\n",
      "Epoch 181/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.2077 - acc: 0.9271 - val_loss: 0.9147 - val_acc: 0.7760\n",
      "Epoch 182/200\n",
      "768/768 [==============================] - 0s 453us/step - loss: 0.2171 - acc: 0.9219 - val_loss: 0.8044 - val_acc: 0.7708\n",
      "Epoch 183/200\n",
      "768/768 [==============================] - 0s 469us/step - loss: 0.2148 - acc: 0.9232 - val_loss: 0.7834 - val_acc: 0.7760\n",
      "Epoch 184/200\n",
      "768/768 [==============================] - 0s 422us/step - loss: 0.2374 - acc: 0.9102 - val_loss: 0.8204 - val_acc: 0.7604\n",
      "Epoch 185/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.2486 - acc: 0.9062 - val_loss: 0.8372 - val_acc: 0.7604\n",
      "Epoch 186/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.2592 - acc: 0.8906 - val_loss: 0.7258 - val_acc: 0.7812\n",
      "Epoch 187/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.2539 - acc: 0.9023 - val_loss: 0.8150 - val_acc: 0.7812\n",
      "Epoch 188/200\n",
      "768/768 [==============================] - 0s 466us/step - loss: 0.2720 - acc: 0.8932 - val_loss: 0.9658 - val_acc: 0.7500\n",
      "Epoch 189/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.2424 - acc: 0.9062 - val_loss: 0.7543 - val_acc: 0.7708\n",
      "Epoch 190/200\n",
      "768/768 [==============================] - 0s 479us/step - loss: 0.2302 - acc: 0.9128 - val_loss: 0.7321 - val_acc: 0.7812\n",
      "Epoch 191/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.2505 - acc: 0.8958 - val_loss: 0.7343 - val_acc: 0.7812\n",
      "Epoch 192/200\n",
      "768/768 [==============================] - 0s 461us/step - loss: 0.2668 - acc: 0.8932 - val_loss: 0.8109 - val_acc: 0.7604\n",
      "Epoch 193/200\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.2628 - acc: 0.8958 - val_loss: 0.7074 - val_acc: 0.7708\n",
      "Epoch 194/200\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.2507 - acc: 0.9023 - val_loss: 0.7226 - val_acc: 0.7812\n",
      "Epoch 195/200\n",
      "768/768 [==============================] - 0s 479us/step - loss: 0.2483 - acc: 0.9023 - val_loss: 0.8061 - val_acc: 0.7865\n",
      "Epoch 196/200\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.2837 - acc: 0.8815 - val_loss: 0.8023 - val_acc: 0.7760\n",
      "Epoch 197/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.2296 - acc: 0.9076 - val_loss: 0.8041 - val_acc: 0.7812\n",
      "Epoch 198/200\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.2190 - acc: 0.9154 - val_loss: 0.7685 - val_acc: 0.7917\n",
      "Epoch 199/200\n",
      "768/768 [==============================] - 0s 475us/step - loss: 0.2250 - acc: 0.9154 - val_loss: 0.7293 - val_acc: 0.7865\n",
      "Epoch 200/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.2141 - acc: 0.9232 - val_loss: 0.8035 - val_acc: 0.7760\n",
      "Predicting...\n",
      "Cross val iteration 5 of 5\n",
      "Fitting...\n",
      "Train on 768 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "768/768 [==============================] - 7s 9ms/step - loss: 0.2543 - acc: 0.9049 - val_loss: 0.6784 - val_acc: 0.7917\n",
      "Epoch 2/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.2493 - acc: 0.9036 - val_loss: 0.7502 - val_acc: 0.7812\n",
      "Epoch 3/200\n",
      "768/768 [==============================] - 0s 417us/step - loss: 0.2858 - acc: 0.8919 - val_loss: 0.7186 - val_acc: 0.7760\n",
      "Epoch 4/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.2916 - acc: 0.8854 - val_loss: 0.8114 - val_acc: 0.7708\n",
      "Epoch 5/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.2378 - acc: 0.9115 - val_loss: 0.8856 - val_acc: 0.7708\n",
      "Epoch 6/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.2131 - acc: 0.9232 - val_loss: 0.8553 - val_acc: 0.7760\n",
      "Epoch 7/200\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.2051 - acc: 0.9219 - val_loss: 0.8266 - val_acc: 0.7708\n",
      "Epoch 8/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.2102 - acc: 0.9258 - val_loss: 0.8038 - val_acc: 0.7917\n",
      "Epoch 9/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.2074 - acc: 0.9245 - val_loss: 0.9598 - val_acc: 0.7708\n",
      "Epoch 10/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.2059 - acc: 0.9206 - val_loss: 1.0065 - val_acc: 0.7448\n",
      "Epoch 11/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.2277 - acc: 0.9062 - val_loss: 0.8636 - val_acc: 0.7656\n",
      "Epoch 12/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.2263 - acc: 0.9180 - val_loss: 0.7971 - val_acc: 0.7760\n",
      "Epoch 13/200\n",
      "768/768 [==============================] - 0s 418us/step - loss: 0.2026 - acc: 0.9245 - val_loss: 1.0517 - val_acc: 0.7500\n",
      "Epoch 14/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.2585 - acc: 0.9023 - val_loss: 0.9115 - val_acc: 0.7812\n",
      "Epoch 15/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.3439 - acc: 0.8750 - val_loss: 0.9910 - val_acc: 0.7448\n",
      "Epoch 16/200\n",
      "768/768 [==============================] - 0s 467us/step - loss: 0.3010 - acc: 0.8945 - val_loss: 0.7675 - val_acc: 0.7656\n",
      "Epoch 17/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.2378 - acc: 0.9128 - val_loss: 0.7626 - val_acc: 0.7604\n",
      "Epoch 18/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.2140 - acc: 0.9206 - val_loss: 0.9538 - val_acc: 0.7708\n",
      "Epoch 19/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.2548 - acc: 0.8997 - val_loss: 0.7982 - val_acc: 0.7760\n",
      "Epoch 20/200\n",
      "768/768 [==============================] - 0s 453us/step - loss: 0.2618 - acc: 0.9062 - val_loss: 0.7180 - val_acc: 0.7760\n",
      "Epoch 21/200\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.2384 - acc: 0.9089 - val_loss: 0.7676 - val_acc: 0.7604\n",
      "Epoch 22/200\n",
      "768/768 [==============================] - 0s 467us/step - loss: 0.2173 - acc: 0.9193 - val_loss: 0.8605 - val_acc: 0.7865\n",
      "Epoch 23/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.2154 - acc: 0.9154 - val_loss: 0.8892 - val_acc: 0.7656\n",
      "Epoch 24/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.1955 - acc: 0.9297 - val_loss: 0.8447 - val_acc: 0.7760\n",
      "Epoch 25/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.2029 - acc: 0.9245 - val_loss: 0.8184 - val_acc: 0.7812\n",
      "Epoch 26/200\n",
      "768/768 [==============================] - 0s 467us/step - loss: 0.2544 - acc: 0.8997 - val_loss: 0.7522 - val_acc: 0.7812\n",
      "Epoch 27/200\n",
      "768/768 [==============================] - 0s 420us/step - loss: 0.3207 - acc: 0.8776 - val_loss: 0.6421 - val_acc: 0.7969\n",
      "Epoch 28/200\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.2865 - acc: 0.8841 - val_loss: 0.7199 - val_acc: 0.7760\n",
      "Epoch 29/200\n",
      "768/768 [==============================] - 0s 453us/step - loss: 0.2480 - acc: 0.8919 - val_loss: 0.8584 - val_acc: 0.7812\n",
      "Epoch 30/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.2730 - acc: 0.8945 - val_loss: 0.7000 - val_acc: 0.7917\n",
      "Epoch 31/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.2368 - acc: 0.9167 - val_loss: 0.8870 - val_acc: 0.7604\n",
      "Epoch 32/200\n",
      "768/768 [==============================] - 0s 483us/step - loss: 0.2172 - acc: 0.9167 - val_loss: 0.7376 - val_acc: 0.7812\n",
      "Epoch 33/200\n",
      "768/768 [==============================] - 0s 493us/step - loss: 0.2570 - acc: 0.8945 - val_loss: 0.8358 - val_acc: 0.7604\n",
      "Epoch 34/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.2120 - acc: 0.9206 - val_loss: 0.7933 - val_acc: 0.7865\n",
      "Epoch 35/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.1976 - acc: 0.9232 - val_loss: 1.0849 - val_acc: 0.7344\n",
      "Epoch 36/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.2212 - acc: 0.9128 - val_loss: 0.9090 - val_acc: 0.7917\n",
      "Epoch 37/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.2302 - acc: 0.9089 - val_loss: 0.8049 - val_acc: 0.7865\n",
      "Epoch 38/200\n",
      "768/768 [==============================] - 0s 478us/step - loss: 0.1993 - acc: 0.9258 - val_loss: 0.8371 - val_acc: 0.7812\n",
      "Epoch 39/200\n",
      "768/768 [==============================] - 0s 423us/step - loss: 0.1970 - acc: 0.9271 - val_loss: 0.9111 - val_acc: 0.7552\n",
      "Epoch 40/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.1949 - acc: 0.9219 - val_loss: 0.9445 - val_acc: 0.7656\n",
      "Epoch 41/200\n",
      "768/768 [==============================] - 0s 469us/step - loss: 0.1903 - acc: 0.9310 - val_loss: 0.8931 - val_acc: 0.7708\n",
      "Epoch 42/200\n",
      "768/768 [==============================] - 0s 424us/step - loss: 0.1938 - acc: 0.9258 - val_loss: 0.9370 - val_acc: 0.7656\n",
      "Epoch 43/200\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.1830 - acc: 0.9336 - val_loss: 0.9739 - val_acc: 0.7760\n",
      "Epoch 44/200\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.1849 - acc: 0.9271 - val_loss: 0.9427 - val_acc: 0.7812\n",
      "Epoch 45/200\n",
      "768/768 [==============================] - 0s 408us/step - loss: 0.1990 - acc: 0.9271 - val_loss: 0.9373 - val_acc: 0.7760\n",
      "Epoch 46/200\n",
      "768/768 [==============================] - 0s 464us/step - loss: 0.2313 - acc: 0.9128 - val_loss: 1.0646 - val_acc: 0.7708\n",
      "Epoch 47/200\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.2198 - acc: 0.9128 - val_loss: 0.9430 - val_acc: 0.7708\n",
      "Epoch 48/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.2005 - acc: 0.9297 - val_loss: 0.8753 - val_acc: 0.7708\n",
      "Epoch 49/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.2573 - acc: 0.9049 - val_loss: 0.9068 - val_acc: 0.7604\n",
      "Epoch 50/200\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.2139 - acc: 0.9219 - val_loss: 0.9608 - val_acc: 0.7760\n",
      "Epoch 51/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.2154 - acc: 0.9206 - val_loss: 0.9870 - val_acc: 0.7396\n",
      "Epoch 52/200\n",
      "768/768 [==============================] - 0s 490us/step - loss: 0.2568 - acc: 0.8997 - val_loss: 0.9664 - val_acc: 0.7552\n",
      "Epoch 53/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.2486 - acc: 0.9102 - val_loss: 0.6324 - val_acc: 0.7708\n",
      "Epoch 54/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.3116 - acc: 0.8724 - val_loss: 0.8438 - val_acc: 0.7500\n",
      "Epoch 55/200\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.2467 - acc: 0.9023 - val_loss: 0.8197 - val_acc: 0.7760\n",
      "Epoch 56/200\n",
      "768/768 [==============================] - 0s 466us/step - loss: 0.2231 - acc: 0.9167 - val_loss: 0.8233 - val_acc: 0.7760\n",
      "Epoch 57/200\n",
      "768/768 [==============================] - 0s 453us/step - loss: 0.2046 - acc: 0.9232 - val_loss: 0.7710 - val_acc: 0.7865\n",
      "Epoch 58/200\n",
      "768/768 [==============================] - 0s 431us/step - loss: 0.1941 - acc: 0.9310 - val_loss: 0.8619 - val_acc: 0.7812\n",
      "Epoch 59/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.1872 - acc: 0.9349 - val_loss: 1.0406 - val_acc: 0.7656\n",
      "Epoch 60/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.1962 - acc: 0.9245 - val_loss: 1.0266 - val_acc: 0.7708\n",
      "Epoch 61/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.2104 - acc: 0.9154 - val_loss: 1.0449 - val_acc: 0.7552\n",
      "Epoch 62/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.1893 - acc: 0.9258 - val_loss: 0.8650 - val_acc: 0.7760\n",
      "Epoch 63/200\n",
      "768/768 [==============================] - 0s 484us/step - loss: 0.1962 - acc: 0.9219 - val_loss: 0.8860 - val_acc: 0.7812\n",
      "Epoch 64/200\n",
      "768/768 [==============================] - 0s 472us/step - loss: 0.2141 - acc: 0.9193 - val_loss: 1.0217 - val_acc: 0.7396\n",
      "Epoch 65/200\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.2269 - acc: 0.9023 - val_loss: 0.8474 - val_acc: 0.7812\n",
      "Epoch 66/200\n",
      "768/768 [==============================] - 0s 500us/step - loss: 0.2266 - acc: 0.9141 - val_loss: 1.0811 - val_acc: 0.7708\n",
      "Epoch 67/200\n",
      "768/768 [==============================] - 0s 463us/step - loss: 0.2399 - acc: 0.9076 - val_loss: 0.9220 - val_acc: 0.7760\n",
      "Epoch 68/200\n",
      "768/768 [==============================] - 0s 433us/step - loss: 0.2596 - acc: 0.8880 - val_loss: 0.9288 - val_acc: 0.7812\n",
      "Epoch 69/200\n",
      "768/768 [==============================] - 0s 486us/step - loss: 0.2195 - acc: 0.9141 - val_loss: 0.9163 - val_acc: 0.7656\n",
      "Epoch 70/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.2292 - acc: 0.9102 - val_loss: 0.8352 - val_acc: 0.7760\n",
      "Epoch 71/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.2156 - acc: 0.9193 - val_loss: 0.8095 - val_acc: 0.7656\n",
      "Epoch 72/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.2043 - acc: 0.9245 - val_loss: 0.9461 - val_acc: 0.7708\n",
      "Epoch 73/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.2188 - acc: 0.9102 - val_loss: 0.8899 - val_acc: 0.7865\n",
      "Epoch 74/200\n",
      "768/768 [==============================] - 0s 473us/step - loss: 0.2277 - acc: 0.9036 - val_loss: 0.8750 - val_acc: 0.7708\n",
      "Epoch 75/200\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.2412 - acc: 0.9076 - val_loss: 1.1827 - val_acc: 0.7448\n",
      "Epoch 76/200\n",
      "768/768 [==============================] - 0s 423us/step - loss: 0.2046 - acc: 0.9206 - val_loss: 0.9208 - val_acc: 0.7656\n",
      "Epoch 77/200\n",
      "768/768 [==============================] - 0s 424us/step - loss: 0.1795 - acc: 0.9310 - val_loss: 0.8744 - val_acc: 0.7865\n",
      "Epoch 78/200\n",
      "768/768 [==============================] - 0s 455us/step - loss: 0.1802 - acc: 0.9284 - val_loss: 0.8664 - val_acc: 0.7708\n",
      "Epoch 79/200\n",
      "768/768 [==============================] - 0s 480us/step - loss: 0.2092 - acc: 0.9167 - val_loss: 0.9791 - val_acc: 0.7917\n",
      "Epoch 80/200\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.1872 - acc: 0.9258 - val_loss: 1.0231 - val_acc: 0.7760\n",
      "Epoch 81/200\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.1819 - acc: 0.9258 - val_loss: 0.9541 - val_acc: 0.7812\n",
      "Epoch 82/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.1694 - acc: 0.9375 - val_loss: 0.9182 - val_acc: 0.7865\n",
      "Epoch 83/200\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.1705 - acc: 0.9323 - val_loss: 0.9958 - val_acc: 0.7708\n",
      "Epoch 84/200\n",
      "768/768 [==============================] - 0s 479us/step - loss: 0.1726 - acc: 0.9336 - val_loss: 1.0033 - val_acc: 0.7604\n",
      "Epoch 85/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.1715 - acc: 0.9336 - val_loss: 1.0965 - val_acc: 0.7500\n",
      "Epoch 86/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.1770 - acc: 0.9271 - val_loss: 1.0975 - val_acc: 0.7812\n",
      "Epoch 87/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.1982 - acc: 0.9219 - val_loss: 0.9628 - val_acc: 0.7708\n",
      "Epoch 88/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.2682 - acc: 0.8893 - val_loss: 0.9813 - val_acc: 0.7292\n",
      "Epoch 89/200\n",
      "768/768 [==============================] - 0s 441us/step - loss: 0.2727 - acc: 0.8932 - val_loss: 0.8450 - val_acc: 0.7552\n",
      "Epoch 90/200\n",
      "768/768 [==============================] - 0s 427us/step - loss: 0.2088 - acc: 0.9193 - val_loss: 0.9907 - val_acc: 0.7812\n",
      "Epoch 91/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.2012 - acc: 0.9167 - val_loss: 0.9300 - val_acc: 0.7708\n",
      "Epoch 92/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.1815 - acc: 0.9375 - val_loss: 1.1068 - val_acc: 0.7812\n",
      "Epoch 93/200\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.1795 - acc: 0.9310 - val_loss: 1.0150 - val_acc: 0.7708\n",
      "Epoch 94/200\n",
      "768/768 [==============================] - 0s 417us/step - loss: 0.1775 - acc: 0.9336 - val_loss: 1.0129 - val_acc: 0.7760\n",
      "Epoch 95/200\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.2155 - acc: 0.9089 - val_loss: 0.8954 - val_acc: 0.7708\n",
      "Epoch 96/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.1985 - acc: 0.9258 - val_loss: 0.9505 - val_acc: 0.7656\n",
      "Epoch 97/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 459us/step - loss: 0.1953 - acc: 0.9258 - val_loss: 1.0696 - val_acc: 0.7760\n",
      "Epoch 98/200\n",
      "768/768 [==============================] - 0s 483us/step - loss: 0.2177 - acc: 0.9141 - val_loss: 0.8011 - val_acc: 0.7760\n",
      "Epoch 99/200\n",
      "768/768 [==============================] - 0s 440us/step - loss: 0.2624 - acc: 0.8958 - val_loss: 0.8435 - val_acc: 0.7656\n",
      "Epoch 100/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.2331 - acc: 0.9102 - val_loss: 1.2232 - val_acc: 0.7292\n",
      "Epoch 101/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.2657 - acc: 0.8971 - val_loss: 1.2640 - val_acc: 0.7292\n",
      "Epoch 102/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.2343 - acc: 0.9128 - val_loss: 0.8286 - val_acc: 0.7865\n",
      "Epoch 103/200\n",
      "768/768 [==============================] - 0s 503us/step - loss: 0.2278 - acc: 0.9167 - val_loss: 0.9523 - val_acc: 0.7812\n",
      "Epoch 104/200\n",
      "768/768 [==============================] - 0s 475us/step - loss: 0.2049 - acc: 0.9193 - val_loss: 1.2594 - val_acc: 0.7396\n",
      "Epoch 105/200\n",
      "768/768 [==============================] - 0s 429us/step - loss: 0.2031 - acc: 0.9128 - val_loss: 1.0174 - val_acc: 0.7604\n",
      "Epoch 106/200\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.1971 - acc: 0.9232 - val_loss: 0.9195 - val_acc: 0.7656\n",
      "Epoch 107/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.1973 - acc: 0.9245 - val_loss: 0.9149 - val_acc: 0.7708\n",
      "Epoch 108/200\n",
      "768/768 [==============================] - 0s 456us/step - loss: 0.1862 - acc: 0.9271 - val_loss: 1.0615 - val_acc: 0.7812\n",
      "Epoch 109/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.1814 - acc: 0.9323 - val_loss: 0.9753 - val_acc: 0.7812\n",
      "Epoch 110/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.1748 - acc: 0.9349 - val_loss: 1.0256 - val_acc: 0.7760\n",
      "Epoch 111/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.1680 - acc: 0.9349 - val_loss: 0.9392 - val_acc: 0.7760\n",
      "Epoch 112/200\n",
      "768/768 [==============================] - 0s 461us/step - loss: 0.1725 - acc: 0.9297 - val_loss: 1.0722 - val_acc: 0.7604\n",
      "Epoch 113/200\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.1691 - acc: 0.9336 - val_loss: 0.9684 - val_acc: 0.7760\n",
      "Epoch 114/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.1811 - acc: 0.9258 - val_loss: 1.1611 - val_acc: 0.7552\n",
      "Epoch 115/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.2011 - acc: 0.9245 - val_loss: 0.9624 - val_acc: 0.7708\n",
      "Epoch 116/200\n",
      "768/768 [==============================] - 0s 466us/step - loss: 0.1819 - acc: 0.9245 - val_loss: 0.9117 - val_acc: 0.7708\n",
      "Epoch 117/200\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.2157 - acc: 0.9128 - val_loss: 0.7882 - val_acc: 0.7812\n",
      "Epoch 118/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.2701 - acc: 0.8932 - val_loss: 0.8057 - val_acc: 0.7865\n",
      "Epoch 119/200\n",
      "768/768 [==============================] - 0s 466us/step - loss: 0.2692 - acc: 0.8919 - val_loss: 0.8605 - val_acc: 0.7500\n",
      "Epoch 120/200\n",
      "768/768 [==============================] - 0s 453us/step - loss: 0.2764 - acc: 0.8893 - val_loss: 0.7394 - val_acc: 0.7865\n",
      "Epoch 121/200\n",
      "768/768 [==============================] - ETA: 0s - loss: 0.3033 - acc: 0.883 - 0s 438us/step - loss: 0.2919 - acc: 0.8815 - val_loss: 0.8309 - val_acc: 0.7708\n",
      "Epoch 122/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.2388 - acc: 0.9089 - val_loss: 0.7317 - val_acc: 0.7865\n",
      "Epoch 123/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.2317 - acc: 0.9141 - val_loss: 0.7661 - val_acc: 0.7812\n",
      "Epoch 124/200\n",
      "768/768 [==============================] - 0s 434us/step - loss: 0.1930 - acc: 0.9245 - val_loss: 0.9487 - val_acc: 0.7760\n",
      "Epoch 125/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.1842 - acc: 0.9271 - val_loss: 0.9724 - val_acc: 0.7656\n",
      "Epoch 126/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.1686 - acc: 0.9375 - val_loss: 1.0432 - val_acc: 0.7708\n",
      "Epoch 127/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.1647 - acc: 0.9362 - val_loss: 0.9612 - val_acc: 0.7604\n",
      "Epoch 128/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.1732 - acc: 0.9388 - val_loss: 1.0834 - val_acc: 0.7500\n",
      "Epoch 129/200\n",
      "768/768 [==============================] - 0s 459us/step - loss: 0.1663 - acc: 0.9323 - val_loss: 0.9353 - val_acc: 0.7917\n",
      "Epoch 130/200\n",
      "768/768 [==============================] - 0s 463us/step - loss: 0.1893 - acc: 0.9310 - val_loss: 1.0256 - val_acc: 0.7760\n",
      "Epoch 131/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.1956 - acc: 0.9167 - val_loss: 1.0762 - val_acc: 0.7865\n",
      "Epoch 132/200\n",
      "768/768 [==============================] - 0s 474us/step - loss: 0.2140 - acc: 0.9128 - val_loss: 1.0873 - val_acc: 0.7500\n",
      "Epoch 133/200\n",
      "768/768 [==============================] - 0s 467us/step - loss: 0.2285 - acc: 0.9062 - val_loss: 1.0142 - val_acc: 0.7760\n",
      "Epoch 134/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.2237 - acc: 0.9062 - val_loss: 0.9224 - val_acc: 0.7656\n",
      "Epoch 135/200\n",
      "768/768 [==============================] - 0s 418us/step - loss: 0.1993 - acc: 0.9219 - val_loss: 0.9616 - val_acc: 0.7656\n",
      "Epoch 136/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.2133 - acc: 0.9154 - val_loss: 1.2327 - val_acc: 0.7656\n",
      "Epoch 137/200\n",
      "768/768 [==============================] - 0s 478us/step - loss: 0.1999 - acc: 0.9271 - val_loss: 1.0058 - val_acc: 0.7708\n",
      "Epoch 138/200\n",
      "768/768 [==============================] - 0s 449us/step - loss: 0.2045 - acc: 0.9128 - val_loss: 0.9599 - val_acc: 0.7656\n",
      "Epoch 139/200\n",
      "768/768 [==============================] - 0s 475us/step - loss: 0.1800 - acc: 0.9336 - val_loss: 1.0471 - val_acc: 0.7708\n",
      "Epoch 140/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.1713 - acc: 0.9323 - val_loss: 0.9840 - val_acc: 0.7552\n",
      "Epoch 141/200\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.1735 - acc: 0.9349 - val_loss: 1.1220 - val_acc: 0.7500\n",
      "Epoch 142/200\n",
      "768/768 [==============================] - 0s 461us/step - loss: 0.1779 - acc: 0.9375 - val_loss: 1.1123 - val_acc: 0.7708\n",
      "Epoch 143/200\n",
      "768/768 [==============================] - 0s 466us/step - loss: 0.1610 - acc: 0.9362 - val_loss: 1.0038 - val_acc: 0.7604\n",
      "Epoch 144/200\n",
      "768/768 [==============================] - 0s 509us/step - loss: 0.1673 - acc: 0.9310 - val_loss: 1.1466 - val_acc: 0.7760\n",
      "Epoch 145/200\n",
      "768/768 [==============================] - 0s 468us/step - loss: 0.1815 - acc: 0.9271 - val_loss: 1.1006 - val_acc: 0.7760\n",
      "Epoch 146/200\n",
      "768/768 [==============================] - 0s 452us/step - loss: 0.1625 - acc: 0.9362 - val_loss: 1.1227 - val_acc: 0.7760\n",
      "Epoch 147/200\n",
      "768/768 [==============================] - 0s 465us/step - loss: 0.1542 - acc: 0.9375 - val_loss: 1.0815 - val_acc: 0.7708\n",
      "Epoch 148/200\n",
      "768/768 [==============================] - 0s 501us/step - loss: 0.1532 - acc: 0.9362 - val_loss: 1.0449 - val_acc: 0.7708\n",
      "Epoch 149/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.1726 - acc: 0.9323 - val_loss: 0.8403 - val_acc: 0.7865\n",
      "Epoch 150/200\n",
      "768/768 [==============================] - 0s 435us/step - loss: 0.2601 - acc: 0.8958 - val_loss: 0.8919 - val_acc: 0.7656\n",
      "Epoch 151/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.2469 - acc: 0.8984 - val_loss: 0.8189 - val_acc: 0.7760\n",
      "Epoch 152/200\n",
      "768/768 [==============================] - 0s 479us/step - loss: 0.2127 - acc: 0.9128 - val_loss: 0.8904 - val_acc: 0.7708\n",
      "Epoch 153/200\n",
      "768/768 [==============================] - 0s 488us/step - loss: 0.1874 - acc: 0.9258 - val_loss: 0.9223 - val_acc: 0.7656\n",
      "Epoch 154/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.1928 - acc: 0.9180 - val_loss: 1.0180 - val_acc: 0.7760\n",
      "Epoch 155/200\n",
      "768/768 [==============================] - 0s 470us/step - loss: 0.1725 - acc: 0.9284 - val_loss: 0.9546 - val_acc: 0.7708\n",
      "Epoch 156/200\n",
      "768/768 [==============================] - 0s 445us/step - loss: 0.1609 - acc: 0.9362 - val_loss: 0.9326 - val_acc: 0.7708\n",
      "Epoch 157/200\n",
      "768/768 [==============================] - 0s 463us/step - loss: 0.2137 - acc: 0.9089 - val_loss: 0.8148 - val_acc: 0.7812\n",
      "Epoch 158/200\n",
      "768/768 [==============================] - 0s 475us/step - loss: 0.2966 - acc: 0.8867 - val_loss: 1.0138 - val_acc: 0.7552\n",
      "Epoch 159/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.2290 - acc: 0.9036 - val_loss: 1.0637 - val_acc: 0.7656\n",
      "Epoch 160/200\n",
      "768/768 [==============================] - 0s 469us/step - loss: 0.2957 - acc: 0.8776 - val_loss: 0.7494 - val_acc: 0.7812\n",
      "Epoch 161/200\n",
      "768/768 [==============================] - 0s 451us/step - loss: 0.2140 - acc: 0.9167 - val_loss: 0.9826 - val_acc: 0.7604\n",
      "Epoch 162/200\n",
      "768/768 [==============================] - 0s 461us/step - loss: 0.1823 - acc: 0.9232 - val_loss: 0.8500 - val_acc: 0.7604\n",
      "Epoch 163/200\n",
      "768/768 [==============================] - 0s 476us/step - loss: 0.1917 - acc: 0.9206 - val_loss: 0.8135 - val_acc: 0.7604\n",
      "Epoch 164/200\n",
      "768/768 [==============================] - 0s 480us/step - loss: 0.1877 - acc: 0.9219 - val_loss: 0.9940 - val_acc: 0.7760\n",
      "Epoch 165/200\n",
      "768/768 [==============================] - 0s 465us/step - loss: 0.2934 - acc: 0.8763 - val_loss: 0.6862 - val_acc: 0.7917\n",
      "Epoch 166/200\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.2302 - acc: 0.9102 - val_loss: 1.0758 - val_acc: 0.7760\n",
      "Epoch 167/200\n",
      "768/768 [==============================] - 0s 458us/step - loss: 0.1646 - acc: 0.9336 - val_loss: 1.0087 - val_acc: 0.7448\n",
      "Epoch 168/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.1749 - acc: 0.9323 - val_loss: 0.9036 - val_acc: 0.7656\n",
      "Epoch 169/200\n",
      "768/768 [==============================] - 0s 446us/step - loss: 0.2283 - acc: 0.9128 - val_loss: 0.9810 - val_acc: 0.7604\n",
      "Epoch 170/200\n",
      "768/768 [==============================] - 0s 450us/step - loss: 0.2073 - acc: 0.9219 - val_loss: 1.0645 - val_acc: 0.7604\n",
      "Epoch 171/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.1767 - acc: 0.9336 - val_loss: 1.0599 - val_acc: 0.7708\n",
      "Epoch 172/200\n",
      "768/768 [==============================] - 0s 474us/step - loss: 0.1979 - acc: 0.9245 - val_loss: 0.9596 - val_acc: 0.7552\n",
      "Epoch 173/200\n",
      "768/768 [==============================] - 0s 468us/step - loss: 0.1589 - acc: 0.9401 - val_loss: 0.9331 - val_acc: 0.7604\n",
      "Epoch 174/200\n",
      "768/768 [==============================] - 0s 468us/step - loss: 0.1708 - acc: 0.9297 - val_loss: 1.0762 - val_acc: 0.7656\n",
      "Epoch 175/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.1543 - acc: 0.9375 - val_loss: 0.9902 - val_acc: 0.7604\n",
      "Epoch 176/200\n",
      "768/768 [==============================] - 0s 469us/step - loss: 0.1525 - acc: 0.9375 - val_loss: 1.0381 - val_acc: 0.7865\n",
      "Epoch 177/200\n",
      "768/768 [==============================] - 0s 413us/step - loss: 0.1656 - acc: 0.9349 - val_loss: 1.0787 - val_acc: 0.7396\n",
      "Epoch 178/200\n",
      "768/768 [==============================] - 0s 430us/step - loss: 0.1592 - acc: 0.9375 - val_loss: 1.1713 - val_acc: 0.7500\n",
      "Epoch 179/200\n",
      "768/768 [==============================] - 0s 439us/step - loss: 0.1629 - acc: 0.9310 - val_loss: 1.1861 - val_acc: 0.7708\n",
      "Epoch 180/200\n",
      "768/768 [==============================] - 0s 453us/step - loss: 0.1724 - acc: 0.9258 - val_loss: 1.0994 - val_acc: 0.7656\n",
      "Epoch 181/200\n",
      "768/768 [==============================] - 0s 455us/step - loss: 0.1600 - acc: 0.9310 - val_loss: 0.9737 - val_acc: 0.7917\n",
      "Epoch 182/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.2391 - acc: 0.9049 - val_loss: 0.9544 - val_acc: 0.7500\n",
      "Epoch 183/200\n",
      "768/768 [==============================] - 0s 436us/step - loss: 0.2556 - acc: 0.9036 - val_loss: 0.9164 - val_acc: 0.7708\n",
      "Epoch 184/200\n",
      "768/768 [==============================] - 0s 438us/step - loss: 0.4461 - acc: 0.8099 - val_loss: 0.6099 - val_acc: 0.6042\n",
      "Epoch 185/200\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.5624 - acc: 0.6602 - val_loss: 0.7487 - val_acc: 0.7656\n",
      "Epoch 186/200\n",
      "768/768 [==============================] - 0s 466us/step - loss: 0.4701 - acc: 0.8346 - val_loss: 0.8049 - val_acc: 0.7760\n",
      "Epoch 187/200\n",
      "768/768 [==============================] - 0s 423us/step - loss: 0.3362 - acc: 0.8672 - val_loss: 1.0540 - val_acc: 0.7448\n",
      "Epoch 188/200\n",
      "768/768 [==============================] - 0s 474us/step - loss: 0.2817 - acc: 0.8945 - val_loss: 0.9048 - val_acc: 0.7552\n",
      "Epoch 189/200\n",
      "768/768 [==============================] - 0s 442us/step - loss: 0.2681 - acc: 0.8945 - val_loss: 0.8357 - val_acc: 0.7708\n",
      "Epoch 190/200\n",
      "768/768 [==============================] - 0s 443us/step - loss: 0.2073 - acc: 0.9167 - val_loss: 0.8976 - val_acc: 0.7604\n",
      "Epoch 191/200\n",
      "768/768 [==============================] - 0s 447us/step - loss: 0.1913 - acc: 0.9245 - val_loss: 0.9715 - val_acc: 0.7500\n",
      "Epoch 192/200\n",
      "768/768 [==============================] - 0s 454us/step - loss: 0.1714 - acc: 0.9362 - val_loss: 1.0234 - val_acc: 0.7604\n",
      "Epoch 193/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.1613 - acc: 0.9388 - val_loss: 1.0701 - val_acc: 0.7552\n",
      "Epoch 194/200\n",
      "768/768 [==============================] - 0s 448us/step - loss: 0.1752 - acc: 0.9284 - val_loss: 0.9440 - val_acc: 0.7708\n",
      "Epoch 195/200\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.1767 - acc: 0.9414 - val_loss: 0.8744 - val_acc: 0.7760\n",
      "Epoch 196/200\n",
      "768/768 [==============================] - 0s 453us/step - loss: 0.1922 - acc: 0.9232 - val_loss: 0.9191 - val_acc: 0.7760\n",
      "Epoch 197/200\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.2045 - acc: 0.9258 - val_loss: 0.9634 - val_acc: 0.7552\n",
      "Epoch 198/200\n",
      "768/768 [==============================] - 0s 463us/step - loss: 0.1892 - acc: 0.9258 - val_loss: 0.9324 - val_acc: 0.7708\n",
      "Epoch 199/200\n",
      "768/768 [==============================] - 0s 456us/step - loss: 0.1625 - acc: 0.9349 - val_loss: 1.2590 - val_acc: 0.7656\n",
      "Epoch 200/200\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.1543 - acc: 0.9414 - val_loss: 1.0385 - val_acc: 0.7656\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    preds, actual, History = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 240, 2)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.around(preds, decimals = 0)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZR2hoq8oirKOUNFXFEVZR/x/aNkjCaTsDSwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(History.history['val_acc'])\n",
    "plt.plot(History.history['acc'])\n",
    "plt.legend(['Val Acc', 'Acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[178, 188, 191, 201, 191]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totals = []\n",
    "for i in range(0, len(preds)): \n",
    "    current_pred = preds[i]\n",
    "    current_real = actual[i]\n",
    "    total = 0\n",
    "    for j in range(0, len(preds[i])):\n",
    "        if current_pred[j][0] == current_real[j][0]: \n",
    "            total += 1\n",
    "    totals.append(total)\n",
    "    \n",
    "totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7416666666666667,\n",
       " 0.7833333333333333,\n",
       " 0.7958333333333333,\n",
       " 0.8375,\n",
       " 0.7958333333333333]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "accs = []\n",
    "for t in totals: \n",
    "    acc = t/len(preds[i])\n",
    "    i+=1\n",
    "    accs.append(acc)\n",
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030663949154956825"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7908333333333333"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py34]",
   "language": "python",
   "name": "conda-env-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
